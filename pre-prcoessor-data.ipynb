{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/batust/presidio-db-json/blob/main/pre-prcoessor-data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install presidio_analyzer\n",
        "!pip install presidio_anonymizer\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66gqyrXzCVCr",
        "outputId": "5a5f03b5-dd12-42a7-de3d-57bf078bc4ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting presidio_analyzer\n",
            "  Downloading presidio_analyzer-2.2.358-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting phonenumbers<9.0.0,>=8.12 (from presidio_analyzer)\n",
            "  Downloading phonenumbers-8.13.55-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (2024.11.6)\n",
            "Requirement already satisfied: spacy!=3.7.0,<4.0.0,>=3.4.4 in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (3.8.4)\n",
            "Collecting tldextract (from presidio_analyzer)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (3.10)\n",
            "Collecting requests-file>=1.4 (from tldextract->presidio_analyzer)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (3.17.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.1.2)\n",
            "Downloading presidio_analyzer-2.2.358-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading phonenumbers-8.13.55-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: phonenumbers, requests-file, tldextract, presidio_analyzer\n",
            "Successfully installed phonenumbers-8.13.55 presidio_analyzer-2.2.358 requests-file-2.1.0 tldextract-5.1.3\n",
            "Collecting presidio_anonymizer\n",
            "  Downloading presidio_anonymizer-2.2.358-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: cryptography<44.1 in /usr/local/lib/python3.11/dist-packages (from presidio_anonymizer) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<44.1->presidio_anonymizer) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<44.1->presidio_anonymizer) (2.22)\n",
            "Downloading presidio_anonymizer-2.2.358-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: presidio_anonymizer\n",
            "Successfully installed presidio_anonymizer-2.2.358\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying out batch processing\n",
        "from typing import List, Optional, Dict, Union, Iterator, Iterable\n",
        "import collections\n",
        "from dataclasses import dataclass\n",
        "import pprint\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "from presidio_analyzer import AnalyzerEngine, BatchAnalyzerEngine, RecognizerResult, DictAnalyzerResult\n",
        "from presidio_anonymizer import AnonymizerEngine, BatchAnonymizerEngine\n",
        "from presidio_anonymizer.entities import EngineResult"
      ],
      "metadata": {
        "id": "lU-kcaaukxyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zObfcH0H1dqv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Authenicating and accessing Llama\n",
        "from os import environ\n",
        "from huggingface_hub import login\n",
        "\n",
        "HUGGING_FACE_TOKEN = environ.get(\"HUGGING_FACE_TOKEN\")\n",
        "login(token=HUGGING_FACE_TOKEN)"
      ],
      "metadata": {
        "id": "EFHfzJX1HLWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Keeps crashing session\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "llama_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
        "\n",
        "def classify_sensitive_columns(columns):\n",
        "    prompt = f\"Given the column names {columns}, identify which may contain sensitive information (e.g., names, IDs, financial data). Return only a list of sensitive column names.\"\n",
        "\n",
        "    response = llama_pipeline(prompt, max_length=100, do_sample=True)\n",
        "\n",
        "    return response[0]['generated_text']\n",
        "\n",
        "columns = [\"Account\", \"Project ID\", \"Program Name\", \"Employee\", \"Cost Category\",\n",
        "           \"Entity\", \"COA ID\", \"COA\", \"Day\", \"Rate\", \"QUANTITY\", \"Actuals\",\n",
        "           \"Posting Category\", \"Description\"]\n",
        "\n",
        "print(classify_sensitive_columns(columns))"
      ],
      "metadata": {
        "id": "mX1lqOjmGvYc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flair\n",
        "!pip install presidio_analyzer\n",
        "!pip install presidio_anonymizer\n",
        "!python -m spacy download en_core_web_lg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yntJ-G5HqrEl",
        "outputId": "4f14cf0f-42fb-4653-8d9f-4cac6580fd10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flair\n",
            "  Downloading flair-0.15.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting boto3>=1.20.27 (from flair)\n",
            "  Downloading boto3-1.37.16-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting conllu<5.0.0,>=4.0 (from flair)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.11/dist-packages (from flair) (1.2.18)\n",
            "Collecting ftfy>=6.1.0 (from flair)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from flair) (5.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from flair) (0.28.1)\n",
            "Collecting langdetect>=1.0.9 (from flair)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from flair) (5.3.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from flair) (3.10.0)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.11/dist-packages (from flair) (10.6.0)\n",
            "Collecting mpld3>=0.3 (from flair)\n",
            "  Downloading mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting pptree>=3.1 (from flair)\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from flair) (2.8.2)\n",
            "Collecting pytorch-revgrad>=0.2.0 (from flair)\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from flair) (2024.11.6)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from flair) (1.6.1)\n",
            "Collecting segtok>=1.5.11 (from flair)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting sqlitedict>=2.0.0 (from flair)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.11/dist-packages (from flair) (0.9.0)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from flair) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.11/dist-packages (from flair) (4.67.1)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair)\n",
            "  Downloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.25.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (4.48.3)\n",
            "Collecting wikipedia-api>=0.5.7 (from flair)\n",
            "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bioc<3.0.0,>=2.0.0 (from flair)\n",
            "  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting jsonlines>=1.2.0 (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting intervaltree (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botocore<1.38.0,>=1.37.16 (from boto3>=1.20.27->flair)\n",
            "  Downloading botocore-1.37.16-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3>=1.20.27->flair)\n",
            "  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.13->flair) (1.17.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (3.17.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (2.32.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect>=1.0.9->flair) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mpld3>=0.3->flair) (3.1.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->flair) (1.3.0)\n",
            "Collecting numpy>=1.23 (from matplotlib>=2.2.3->flair)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (4.25.6)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.16->boto3>=1.20.27->flair) (2.3.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair) (25.3.0)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (1.3.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.6)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mpld3>=0.3->flair) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (5.9.5)\n",
            "Downloading flair-0.15.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bioc-2.1-py3-none-any.whl (33 kB)\n",
            "Downloading boto3-1.37.16-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m36.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading botocore-1.37.16-py3-none-any.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m72.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.11.4-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect, pptree, sqlitedict, wikipedia-api, docopt, intervaltree\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=c01f04ed0bce1af80cb0f0270c9eb25af0cee672e3e6a8caee8359727e76acc1\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=b8385c99602b7d131326afe6536368f5341746f2f0187cf9d7860b7b4be69bd6\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/8a/eb/d683aa6d09dc68ebfde2f37566ddc8807837c4415b4fd2b04c\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=4b67fdfcbc6a89556c3030e08db53d15af0a6755e1b89e90b999bed77c27f155\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15384 sha256=1694d5d894edb5455636d70fbe264c3f392e199e7c3013c1a9f700d705ecbc9c\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/0f/39/e8214ec038ccd5aeb8c82b957289f2f3ab2251febeae5c2860\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=65bdf316c2bdcd7f1f933fc4a33c4e765db769920fa8824d5531e4e66eeb6bfe\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26097 sha256=c9fe7409d7904b0ec07de943c1fc51c898b5bffdc5a0ee001b870166d5bd1277\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/d7/d9/eec6891f78cac19a693bd40ecb8365d2f4613318c145ec9816\n",
            "Successfully built langdetect pptree sqlitedict wikipedia-api docopt intervaltree\n",
            "Installing collected packages: sqlitedict, pptree, docopt, segtok, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, langdetect, jsonlines, jmespath, intervaltree, ftfy, conllu, wikipedia-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, bioc, s3transfer, nvidia-cusolver-cu12, mpld3, boto3, pytorch-revgrad, transformer-smaller-training-vocab, flair\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bioc-2.1 boto3-1.37.16 botocore-1.37.16 conllu-4.5.3 docopt-0.6.2 flair-0.15.1 ftfy-6.3.1 intervaltree-3.1.0 jmespath-1.0.1 jsonlines-4.0.0 langdetect-1.0.9 mpld3-0.5.10 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.11.4 segtok-1.5.11 sqlitedict-2.1.0 transformer-smaller-training-vocab-0.4.0 wikipedia-api-0.8.1\n",
            "Collecting presidio_analyzer\n",
            "  Downloading presidio_analyzer-2.2.358-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting phonenumbers<9.0.0,>=8.12 (from presidio_analyzer)\n",
            "  Downloading phonenumbers-8.13.55-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (2024.11.6)\n",
            "Requirement already satisfied: spacy!=3.7.0,<4.0.0,>=3.4.4 in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (3.8.4)\n",
            "Collecting tldextract (from presidio_analyzer)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (3.10)\n",
            "Collecting requests-file>=1.4 (from tldextract->presidio_analyzer)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (3.17.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.1.2)\n",
            "Downloading presidio_analyzer-2.2.358-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading phonenumbers-8.13.55-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: phonenumbers, requests-file, tldextract, presidio_analyzer\n",
            "Successfully installed phonenumbers-8.13.55 presidio_analyzer-2.2.358 requests-file-2.1.0 tldextract-5.1.3\n",
            "Collecting presidio_anonymizer\n",
            "  Downloading presidio_anonymizer-2.2.358-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: cryptography<44.1 in /usr/local/lib/python3.11/dist-packages (from presidio_anonymizer) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<44.1->presidio_anonymizer) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<44.1->presidio_anonymizer) (2.22)\n",
            "Downloading presidio_anonymizer-2.2.358-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: presidio_anonymizer\n",
            "Successfully installed presidio_anonymizer-2.2.358\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0].strip().replace(' ', '-')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "0Ry2jFMGtIfT",
        "outputId": "cba9d0af-e699-426a-cd81-9114a4d3afed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d833e594-5ff5-494e-bf02-54f764858cdc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d833e594-5ff5-494e-bf02-54f764858cdc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving testing-csv-ff.csv to testing-csv-ff.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "EXCLUDED_HEADERS = [\"description\", \"comment\", \"comments\", \"query\", \"notes\", \"email\"]\n",
        "\n",
        "# ner_tagger = SequenceTagger.load(\"flair/ner-english\")\n",
        "tagger = SequenceTagger.load('flair/pos-english')\n",
        "if file_name.endswith('.xlsx'):\n",
        "  df = pd.read_excel(file_name)\n",
        "elif file_name.endswith('.csv'):\n",
        "  df = pd.read_csv(file_name)\n",
        "\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "id": "R4ljahrrrhGa",
        "outputId": "9bb980a1-ffa1-43be-f56d-7f2fd6e11726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-19 11:28:24,203 SequenceTagger predicts: Dictionary with 20 tags: <unk>, O, S-ORG, S-MISC, B-PER, E-PER, S-LOC, B-ORG, E-ORG, I-PER, S-PER, B-MISC, I-MISC, E-MISC, I-ORG, B-LOC, E-LOC, I-LOC, <START>, <STOP>\n",
            "2025-03-19 11:28:25,113 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            id                                        name  \\\n",
              "0      5872184                                         ibm   \n",
              "1      4425416                   tata consultancy services   \n",
              "2        21074                                   accenture   \n",
              "3      2309813                                     us army   \n",
              "4      1558607                                          ey   \n",
              "...        ...                                         ...   \n",
              "29995  1201877                                 hans anders   \n",
              "29996  2828041                    real canadian superstore   \n",
              "29997   835865              shumaker, loop & kendrick, llp   \n",
              "29998  1611480                               business wire   \n",
              "29999  1859249  helix technology solutions private limited   \n",
              "\n",
              "                       domain  year founded  \\\n",
              "0                     ibm.com        1911.0   \n",
              "1                     tcs.com        1968.0   \n",
              "2               accenture.com        1989.0   \n",
              "3                  goarmy.com        1800.0   \n",
              "4                      ey.com        1989.0   \n",
              "...                       ...           ...   \n",
              "29995           hansanders.nl        1982.0   \n",
              "29996           superstore.ca           NaN   \n",
              "29997             slk-law.com        1925.0   \n",
              "29998        businesswire.com        1961.0   \n",
              "29999  helixtechsolutions.com           NaN   \n",
              "\n",
              "                                  industry   size range  \\\n",
              "0      information technology and services       10001+   \n",
              "1      information technology and services       10001+   \n",
              "2      information technology and services       10001+   \n",
              "3                                 military       10001+   \n",
              "4                               accounting       10001+   \n",
              "...                                    ...          ...   \n",
              "29995                               retail  1001 - 5000   \n",
              "29996                               retail  1001 - 5000   \n",
              "29997                         law practice   501 - 1000   \n",
              "29998  public relations and communications   501 - 1000   \n",
              "29999                           e-learning  1001 - 5000   \n",
              "\n",
              "                                       locality         country  \\\n",
              "0             new york, new york, united states   united states   \n",
              "1                    bombay, maharashtra, india           india   \n",
              "2                       dublin, dublin, ireland         ireland   \n",
              "3           alexandria, virginia, united states   united states   \n",
              "4        london, greater london, united kingdom  united kingdom   \n",
              "...                                         ...             ...   \n",
              "29995        giethoorn, overijssel, netherlands     netherlands   \n",
              "29996                     madrid, madrid, spain           spain   \n",
              "29997               toledo, ohio, united states   united states   \n",
              "29998  san francisco, california, united states   united states   \n",
              "29999               hyderābād, telangana, india           india   \n",
              "\n",
              "                                            linkedin url  \\\n",
              "0                               linkedin.com/company/ibm   \n",
              "1         linkedin.com/company/tata-consultancy-services   \n",
              "2                         linkedin.com/company/accenture   \n",
              "3                           linkedin.com/company/us-army   \n",
              "4                     linkedin.com/company/ernstandyoung   \n",
              "...                                                  ...   \n",
              "29995                   linkedin.com/company/hans-anders   \n",
              "29996      linkedin.com/company/real-canadian-superstore   \n",
              "29997  linkedin.com/company/shumaker-loop-&-kendrick-llp   \n",
              "29998                 linkedin.com/company/business-wire   \n",
              "29999  linkedin.com/company/helix-technology-solution...   \n",
              "\n",
              "       current employee estimate  total employee estimate  \n",
              "0                         274047                   716906  \n",
              "1                         190771                   341369  \n",
              "2                         190689                   455768  \n",
              "3                         162163                   445958  \n",
              "4                         158363                   428960  \n",
              "...                          ...                      ...  \n",
              "29995                        390                      959  \n",
              "29996                        390                     1320  \n",
              "29997                        390                      688  \n",
              "29998                        390                      998  \n",
              "29999                        390                      501  \n",
              "\n",
              "[30000 rows x 11 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0cadc802-30da-469c-b6c0-85268713b38e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>name</th>\n",
              "      <th>domain</th>\n",
              "      <th>year founded</th>\n",
              "      <th>industry</th>\n",
              "      <th>size range</th>\n",
              "      <th>locality</th>\n",
              "      <th>country</th>\n",
              "      <th>linkedin url</th>\n",
              "      <th>current employee estimate</th>\n",
              "      <th>total employee estimate</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5872184</td>\n",
              "      <td>ibm</td>\n",
              "      <td>ibm.com</td>\n",
              "      <td>1911.0</td>\n",
              "      <td>information technology and services</td>\n",
              "      <td>10001+</td>\n",
              "      <td>new york, new york, united states</td>\n",
              "      <td>united states</td>\n",
              "      <td>linkedin.com/company/ibm</td>\n",
              "      <td>274047</td>\n",
              "      <td>716906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4425416</td>\n",
              "      <td>tata consultancy services</td>\n",
              "      <td>tcs.com</td>\n",
              "      <td>1968.0</td>\n",
              "      <td>information technology and services</td>\n",
              "      <td>10001+</td>\n",
              "      <td>bombay, maharashtra, india</td>\n",
              "      <td>india</td>\n",
              "      <td>linkedin.com/company/tata-consultancy-services</td>\n",
              "      <td>190771</td>\n",
              "      <td>341369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>21074</td>\n",
              "      <td>accenture</td>\n",
              "      <td>accenture.com</td>\n",
              "      <td>1989.0</td>\n",
              "      <td>information technology and services</td>\n",
              "      <td>10001+</td>\n",
              "      <td>dublin, dublin, ireland</td>\n",
              "      <td>ireland</td>\n",
              "      <td>linkedin.com/company/accenture</td>\n",
              "      <td>190689</td>\n",
              "      <td>455768</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2309813</td>\n",
              "      <td>us army</td>\n",
              "      <td>goarmy.com</td>\n",
              "      <td>1800.0</td>\n",
              "      <td>military</td>\n",
              "      <td>10001+</td>\n",
              "      <td>alexandria, virginia, united states</td>\n",
              "      <td>united states</td>\n",
              "      <td>linkedin.com/company/us-army</td>\n",
              "      <td>162163</td>\n",
              "      <td>445958</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1558607</td>\n",
              "      <td>ey</td>\n",
              "      <td>ey.com</td>\n",
              "      <td>1989.0</td>\n",
              "      <td>accounting</td>\n",
              "      <td>10001+</td>\n",
              "      <td>london, greater london, united kingdom</td>\n",
              "      <td>united kingdom</td>\n",
              "      <td>linkedin.com/company/ernstandyoung</td>\n",
              "      <td>158363</td>\n",
              "      <td>428960</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29995</th>\n",
              "      <td>1201877</td>\n",
              "      <td>hans anders</td>\n",
              "      <td>hansanders.nl</td>\n",
              "      <td>1982.0</td>\n",
              "      <td>retail</td>\n",
              "      <td>1001 - 5000</td>\n",
              "      <td>giethoorn, overijssel, netherlands</td>\n",
              "      <td>netherlands</td>\n",
              "      <td>linkedin.com/company/hans-anders</td>\n",
              "      <td>390</td>\n",
              "      <td>959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29996</th>\n",
              "      <td>2828041</td>\n",
              "      <td>real canadian superstore</td>\n",
              "      <td>superstore.ca</td>\n",
              "      <td>NaN</td>\n",
              "      <td>retail</td>\n",
              "      <td>1001 - 5000</td>\n",
              "      <td>madrid, madrid, spain</td>\n",
              "      <td>spain</td>\n",
              "      <td>linkedin.com/company/real-canadian-superstore</td>\n",
              "      <td>390</td>\n",
              "      <td>1320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29997</th>\n",
              "      <td>835865</td>\n",
              "      <td>shumaker, loop &amp; kendrick, llp</td>\n",
              "      <td>slk-law.com</td>\n",
              "      <td>1925.0</td>\n",
              "      <td>law practice</td>\n",
              "      <td>501 - 1000</td>\n",
              "      <td>toledo, ohio, united states</td>\n",
              "      <td>united states</td>\n",
              "      <td>linkedin.com/company/shumaker-loop-&amp;-kendrick-llp</td>\n",
              "      <td>390</td>\n",
              "      <td>688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29998</th>\n",
              "      <td>1611480</td>\n",
              "      <td>business wire</td>\n",
              "      <td>businesswire.com</td>\n",
              "      <td>1961.0</td>\n",
              "      <td>public relations and communications</td>\n",
              "      <td>501 - 1000</td>\n",
              "      <td>san francisco, california, united states</td>\n",
              "      <td>united states</td>\n",
              "      <td>linkedin.com/company/business-wire</td>\n",
              "      <td>390</td>\n",
              "      <td>998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29999</th>\n",
              "      <td>1859249</td>\n",
              "      <td>helix technology solutions private limited</td>\n",
              "      <td>helixtechsolutions.com</td>\n",
              "      <td>NaN</td>\n",
              "      <td>e-learning</td>\n",
              "      <td>1001 - 5000</td>\n",
              "      <td>hyderābād, telangana, india</td>\n",
              "      <td>india</td>\n",
              "      <td>linkedin.com/company/helix-technology-solution...</td>\n",
              "      <td>390</td>\n",
              "      <td>501</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30000 rows × 11 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cadc802-30da-469c-b6c0-85268713b38e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0cadc802-30da-469c-b6c0-85268713b38e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0cadc802-30da-469c-b6c0-85268713b38e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-94c8d719-4b6c-45a9-afae-005f29d29374\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-94c8d719-4b6c-45a9-afae-005f29d29374')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-94c8d719-4b6c-45a9-afae-005f29d29374 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_1d101736-8cd7-4856-ae8a-7fee5cad56b8\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_1d101736-8cd7-4856-ae8a-7fee5cad56b8 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 30000,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2065024,\n        \"min\": 83,\n        \"max\": 7172954,\n        \"num_unique_values\": 30000,\n        \"samples\": [\n          7101437,\n          2451549,\n          4790856\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 29423,\n        \"samples\": [\n          \"positivo inform\\u00e1tica\",\n          \"cbi\",\n          \"orion group\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"domain\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 26710,\n        \"samples\": [\n          \"ehhi.com\",\n          \"rcoe.us\",\n          \"altalink.ca\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"year founded\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 46.819743806326905,\n        \"min\": 1451.0,\n        \"max\": 2018.0,\n        \"num_unique_values\": 222,\n        \"samples\": [\n          2007.0,\n          1814.0,\n          1976.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"industry\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 148,\n        \"samples\": [\n          \"e-learning\",\n          \"food production\",\n          \"alternative medicine\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"size range\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"5001 - 10000\",\n          \"501 - 1000\",\n          \"10001+\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"locality\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 5082,\n        \"samples\": [\n          \"westbury, new york, united states\",\n          \"jumeirah, dubai, united arab emirates\",\n          \"woking, surrey, united kingdom\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"country\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 148,\n        \"samples\": [\n          \"gibraltar\",\n          \"bermuda\",\n          \"bolivia\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"linkedin url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 30000,\n        \"samples\": [\n          \"linkedin.com/company/defense-logistics-agency\",\n          \"linkedin.com/company/asd-healthcare\",\n          \"linkedin.com/company/bvg-india-ltd-\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"current employee estimate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5177,\n        \"min\": 390,\n        \"max\": 274047,\n        \"num_unique_values\": 4989,\n        \"samples\": [\n          4196,\n          4886,\n          4599\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"total employee estimate\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 12864,\n        \"min\": 396,\n        \"max\": 716906,\n        \"num_unique_values\": 8063,\n        \"samples\": [\n          16946,\n          7462,\n          6228\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Not batch processing\n",
        "def classify_cell_as_noun(texts):\n",
        "    sentences = [Sentence(text) for text in texts]\n",
        "    tagger.predict(sentences)\n",
        "\n",
        "    results = []\n",
        "    noun_count = 0\n",
        "\n",
        "    for sentence, original_text in zip(sentences, texts):\n",
        "        all_nouns = any(\"NN\" in token.tag for token in sentence)\n",
        "        classification = \"Noun\" if all_nouns else \"Not a noun\"\n",
        "\n",
        "        results.append({\"Original Text\": original_text, \"Classification\": classification})\n",
        "        noun_count += all_nouns\n",
        "\n",
        "    is_noun_column = noun_count > (len(results) // 1.5)\n",
        "    return results, is_noun_column\n",
        "\n",
        "sensitive = set()\n",
        "\n",
        "for column in df.columns:\n",
        "    values = df[column].astype(str).tolist()\n",
        "    results, is_noun = classify_cell_as_noun(values)\n",
        "\n",
        "    if is_noun: sensitive.add(column)\n",
        "\n",
        "print(sensitive)\n"
      ],
      "metadata": {
        "id": "g1IS53-78T_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch processing\n",
        "import pandas as pd\n",
        "import concurrent.futures\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "\n",
        "tagger = SequenceTagger.load(\"flair/pos-english\")\n",
        "\n",
        "EXCLUDED_HEADERS = [\"description\", \"comment\", \"comments\", \"query\", \"notes\", \"email\"]\n",
        "FILE_NAME = \"companies_sorted.csv\"\n",
        "BATCH_SIZE = 10000\n",
        "FLAIR_BATCH_SIZE = 100\n",
        "MAX_THREADS = 4\n",
        "\n",
        "sensitive = set()\n",
        "\n",
        "def classify_cells_as_nouns(texts):\n",
        "    \"\"\"Batch processes text to check if columns are noun-heavy.\"\"\"\n",
        "    texts = ['' if pd.isna(text) else text for text in texts]\n",
        "\n",
        "    results = []\n",
        "    noun_count = 0\n",
        "\n",
        "    for i in range(0, len(texts), FLAIR_BATCH_SIZE):\n",
        "        batch_text = texts[i : i + FLAIR_BATCH_SIZE]\n",
        "        sentences = [Sentence(text) for text in batch_text]\n",
        "\n",
        "        tagger.predict(sentences)\n",
        "\n",
        "        for sentence, original_text in zip(sentences, batch_text):\n",
        "            all_nouns = any(\"NN\" in token.tag for token in sentence)\n",
        "            classification = \"Noun\" if all_nouns else \"Not a noun\"\n",
        "\n",
        "            results.append({\"Original Text\": original_text, \"Classification\": classification})\n",
        "\n",
        "            noun_count += all_nouns\n",
        "\n",
        "    is_noun_column = noun_count > (len(results) * 0.66)\n",
        "\n",
        "    return is_noun_column\n",
        "\n",
        "def process_column(col, chunk):\n",
        "    \"\"\"Runs noun classification on a single column.\"\"\"\n",
        "    values = chunk[col].astype(str).fillna(\"\").tolist()\n",
        "    return col if classify_cells_as_nouns(values) else None\n",
        "\n",
        "FILE_NAME = 'smaller_companies.csv'\n",
        "if FILE_NAME.endswith(\".xlsx\"):\n",
        "    df_iterator = pd.read_excel(FILE_NAME, chunksize=BATCH_SIZE)\n",
        "elif FILE_NAME.endswith(\".csv\"):\n",
        "    df_iterator = pd.read_csv(FILE_NAME, chunksize=BATCH_SIZE)\n",
        "\n",
        "for chunk in df_iterator:\n",
        "    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
        "        futures = {executor.submit(process_column, col, chunk): col for col in chunk.columns}\n",
        "\n",
        "        for future in concurrent.futures.as_completed(futures):\n",
        "            result = future.result()\n",
        "            if result:\n",
        "                sensitive.add(result)\n",
        "\n",
        "print(\"Sensitive Columns:\", sensitive)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHVCsND0MvVZ",
        "outputId": "c544cb31-f56f-4615-e4d9-cb16fb83bf4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-19 11:51:26,893 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
            "  result = _VF.lstm(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:1124: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at /pytorch/aten/src/ATen/native/cudnn/RNN.cpp:1412.)\n",
            "  result = _VF.lstm(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitive Columns: {'linkedin url', 'name', 'country', 'industry', 'locality'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_df = df_iterator.copy()\n",
        "\n",
        "for col in masked_df:\n",
        "  if col in sensitive:\n",
        "    masked_df[col] = [f'{col}_{i}' for i in range(len(masked_df[col]))]\n",
        "\n",
        "masked_df"
      ],
      "metadata": {
        "id": "TXaqMb3adrPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Trying spacy and flair on large dataset using Threading\n",
        "import re\n",
        "import json\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from threading import Lock\n",
        "\n",
        "# Load models\n",
        "flair_tagger = SequenceTagger.load(\"flair/pos-english\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Constants\n",
        "BATCH_SIZE = 10000  # Process in chunks\n",
        "FLAIR_SAMPLE_SIZE = 10  # Sample size for noun detection\n",
        "THREADS = 4  # Number of parallel threads\n",
        "PHONE_REGEX = r\"\\b(\\+\\d{1,4}\\s?)?(\\(?\\d{1,4}\\)?[-.]?\\s?)?\\d{3,4}[-.]?\\d{3,4}[-.]?\\d{3,4}\\b\"\n",
        "\n",
        "DESCRIPTIVE_COLUMNS = {\"description\", \"comment\", \"comments\", \"query\", \"notes\"}\n",
        "SENSITIVE_COLUMNS = set()\n",
        "MAPPINGS = {}\n",
        "VALUE_TO_MASK = {}\n",
        "ENTITY_COUNTER = 1\n",
        "mask_lock = Lock()  # Thread-safe lock for dictionary updates\n",
        "\n",
        "def is_noun_column(texts):\n",
        "    \"\"\"Identify if a column contains mostly nouns based on a sample.\"\"\"\n",
        "    texts = [\"\" if pd.isna(text) else str(text) for text in texts[:FLAIR_SAMPLE_SIZE]]\n",
        "    sentences = [Sentence(text) for text in texts]\n",
        "    flair_tagger.predict(sentences)\n",
        "    noun_count = sum(any(\"NN\" in token.tag for token in sent) for sent in sentences)\n",
        "    return noun_count > (len(texts) * 0.6)\n",
        "\n",
        "def mask_text(text, col_name, row_idx):\n",
        "    \"\"\"Mask the input text and store the mapping, ensuring thread-safe access.\"\"\"\n",
        "    global ENTITY_COUNTER\n",
        "    if pd.isna(text) or str(text).strip() == \"\":\n",
        "        return text\n",
        "    with mask_lock:  # Thread-safe updates\n",
        "        if text in VALUE_TO_MASK:\n",
        "            return VALUE_TO_MASK[text]\n",
        "        masked_value = f\"{col_name}_{row_idx}\"\n",
        "        VALUE_TO_MASK[text] = masked_value\n",
        "        MAPPINGS[masked_value] = text\n",
        "        ENTITY_COUNTER += 1\n",
        "    return masked_value\n",
        "\n",
        "def process_chunk(chunk):\n",
        "    \"\"\"Process a chunk of the CSV, masking sensitive data, and return a new masked DataFrame.\"\"\"\n",
        "    new_chunk = chunk.copy()  # Ensure original DataFrame remains unchanged\n",
        "\n",
        "    for row_idx in new_chunk.index:\n",
        "        # Mask sensitive columns\n",
        "        for col in SENSITIVE_COLUMNS:\n",
        "            new_chunk.at[row_idx, col] = mask_text(new_chunk.at[row_idx, col], col, row_idx)\n",
        "\n",
        "        # Mask phone numbers across all columns\n",
        "        for col in new_chunk.columns:\n",
        "            new_chunk.at[row_idx, col] = re.sub(PHONE_REGEX, lambda m: mask_text(m.group(0), \"phone\", row_idx), str(new_chunk.at[row_idx, col]))\n",
        "\n",
        "        # Mask descriptive text columns\n",
        "        descriptive_cols = [col for col in new_chunk.columns if col.lower() in DESCRIPTIVE_COLUMNS]\n",
        "        for col in descriptive_cols:\n",
        "            doc = nlp(str(new_chunk.at[row_idx, col]))\n",
        "            for ent in doc.ents:\n",
        "                new_chunk.at[row_idx, col] = new_chunk.at[row_idx, col].replace(ent.text, mask_text(ent.text, col, row_idx))\n",
        "\n",
        "    return new_chunk  # Return masked DataFrame\n",
        "\n",
        "def main(file_path):\n",
        "    \"\"\"Process the entire CSV file in chunks and write masked data to a new file.\"\"\"\n",
        "    global SENSITIVE_COLUMNS\n",
        "\n",
        "    # Identify sensitive columns from a sample\n",
        "    df_sample = pd.read_csv(file_path, nrows=FLAIR_SAMPLE_SIZE)\n",
        "    SENSITIVE_COLUMNS = {col for col in df_sample.columns if is_noun_column(df_sample[col].astype(str).tolist())}\n",
        "\n",
        "    # Process CSV in chunks with threading\n",
        "    reader = pd.read_csv(file_path, chunksize=BATCH_SIZE)\n",
        "    first_chunk = True  # Ensure header is written only once\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=THREADS) as executor:\n",
        "        for masked_chunk in executor.map(process_chunk, reader):\n",
        "            # Append masked chunk to file (efficient memory usage)\n",
        "            masked_chunk.to_csv(\"masked_output.csv\", index=False, mode=\"a\", header=first_chunk)\n",
        "            first_chunk = False  # Disable header for subsequent chunks\n",
        "\n",
        "    # Save mappings separately for possible de-anonymization\n",
        "    with open(\"mappings.json\", \"w\") as f:\n",
        "        json.dump(MAPPINGS, f)\n",
        "\n",
        "    print(\"Processing complete. Masked data saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main('smaller_companies.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyvUoB4fDZxJ",
        "outputId": "71ea32ef-b36c-4079-c3b9-8a8b36f5332c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-03-20 08:47:18,085 SequenceTagger predicts: Dictionary with 53 tags: <unk>, O, UH, ,, VBD, PRP, VB, PRP$, NN, RB, ., DT, JJ, VBP, VBG, IN, CD, NNS, NNP, WRB, VBZ, WDT, CC, TO, MD, VBN, WP, :, RP, EX, JJR, FW, XX, HYPH, POS, RBR, JJS, PDT, NNPS, RBS, AFX, WP$, -LRB-, -RRB-, ``, '', LS, $, SYM, ADD\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-72a88127c458>:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '5872184' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  new_chunk.at[row_idx, col] = re.sub(PHONE_REGEX, lambda m: mask_text(m.group(0), \"phone\", row_idx), str(new_chunk.at[row_idx, col]))\n",
            "<ipython-input-24-72a88127c458>:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1911.0' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  new_chunk.at[row_idx, col] = re.sub(PHONE_REGEX, lambda m: mask_text(m.group(0), \"phone\", row_idx), str(new_chunk.at[row_idx, col]))\n",
            "<ipython-input-24-72a88127c458>:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '274047' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  new_chunk.at[row_idx, col] = re.sub(PHONE_REGEX, lambda m: mask_text(m.group(0), \"phone\", row_idx), str(new_chunk.at[row_idx, col]))\n",
            "<ipython-input-24-72a88127c458>:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '716906' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  new_chunk.at[row_idx, col] = re.sub(PHONE_REGEX, lambda m: mask_text(m.group(0), \"phone\", row_idx), str(new_chunk.at[row_idx, col]))\n",
            "<ipython-input-24-72a88127c458>:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '3875354' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  new_chunk.at[row_idx, col] = re.sub(PHONE_REGEX, lambda m: mask_text(m.group(0), \"phone\", row_idx), str(new_chunk.at[row_idx, col]))\n",
            "<ipython-input-24-72a88127c458>:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1946.0' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  new_chunk.at[row_idx, col] = re.sub(PHONE_REGEX, lambda m: mask_text(m.group(0), \"phone\", row_idx), str(new_chunk.at[row_idx, col]))\n",
            "<ipython-input-24-72a88127c458>:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '1117' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  new_chunk.at[row_idx, col] = re.sub(PHONE_REGEX, lambda m: mask_text(m.group(0), \"phone\", row_idx), str(new_chunk.at[row_idx, col]))\n",
            "<ipython-input-24-72a88127c458>:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '4262' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  new_chunk.at[row_idx, col] = re.sub(PHONE_REGEX, lambda m: mask_text(m.group(0), \"phone\", row_idx), str(new_chunk.at[row_idx, col]))\n",
            "<ipython-input-24-72a88127c458>:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '4982906' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  new_chunk.at[row_idx, col] = re.sub(PHONE_REGEX, lambda m: mask_text(m.group(0), \"phone\", row_idx), str(new_chunk.at[row_idx, col]))\n",
            "<ipython-input-24-72a88127c458>:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value 'nan' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
            "  new_chunk.at[row_idx, col] = re.sub(PHONE_REGEX, lambda m: mask_text(m.group(0), \"phone\", row_idx), str(new_chunk.at[row_idx, col]))\n",
            "<ipython-input-24-72a88127c458>:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '583' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  new_chunk.at[row_idx, col] = re.sub(PHONE_REGEX, lambda m: mask_text(m.group(0), \"phone\", row_idx), str(new_chunk.at[row_idx, col]))\n",
            "<ipython-input-24-72a88127c458>:61: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '787' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  new_chunk.at[row_idx, col] = re.sub(PHONE_REGEX, lambda m: mask_text(m.group(0), \"phone\", row_idx), str(new_chunk.at[row_idx, col]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing complete. Masked data saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flair\n",
        "!pip install presidio_analyzer\n",
        "!pip install presidio_anonymizer\n",
        "# !python -m spacy download en_core_web_lg\n",
        "!pip install --upgrade --force-reinstall \"numpy==1.26.4\"\n",
        "!pip install --upgrade --force-reinstall \"Cython\" \"spacy\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qJ96uMjuFBlK",
        "outputId": "55d047e9-c613-4d4b-ea5f-83d42eee0adc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flair\n",
            "  Downloading flair-0.15.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting boto3>=1.20.27 (from flair)\n",
            "  Downloading boto3-1.37.18-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting conllu<5.0.0,>=4.0 (from flair)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.11/dist-packages (from flair) (1.2.18)\n",
            "Collecting ftfy>=6.1.0 (from flair)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from flair) (5.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from flair) (0.29.3)\n",
            "Collecting langdetect>=1.0.9 (from flair)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from flair) (5.3.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from flair) (3.10.0)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.11/dist-packages (from flair) (10.6.0)\n",
            "Collecting mpld3>=0.3 (from flair)\n",
            "  Downloading mpld3-0.5.10-py3-none-any.whl.metadata (5.1 kB)\n",
            "Collecting pptree>=3.1 (from flair)\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from flair) (2.8.2)\n",
            "Collecting pytorch-revgrad>=0.2.0 (from flair)\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from flair) (2024.11.6)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from flair) (1.6.1)\n",
            "Collecting segtok>=1.5.11 (from flair)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting sqlitedict>=2.0.0 (from flair)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.11/dist-packages (from flair) (0.9.0)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from flair) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.11/dist-packages (from flair) (4.67.1)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair)\n",
            "  Downloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.25.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (4.49.0)\n",
            "Collecting wikipedia-api>=0.5.7 (from flair)\n",
            "  Downloading wikipedia_api-0.8.1.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bioc<3.0.0,>=2.0.0 (from flair)\n",
            "  Downloading bioc-2.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting jsonlines>=1.2.0 (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting intervaltree (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting docopt (from bioc<3.0.0,>=2.0.0->flair)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botocore<1.38.0,>=1.37.18 (from boto3>=1.20.27->flair)\n",
            "  Downloading botocore-1.37.18-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.12.0,>=0.11.0 (from boto3>=1.20.27->flair)\n",
            "  Downloading s3transfer-0.11.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.13->flair) (1.17.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (2.32.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect>=1.0.9->flair) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (2.0.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mpld3>=0.3->flair) (3.1.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.1->flair)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->flair) (1.3.0)\n",
            "Collecting numpy>=1.23 (from matplotlib>=2.2.3->flair)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (5.29.3)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.18->boto3>=1.20.27->flair) (2.3.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair) (25.3.0)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (1.5.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.6)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mpld3>=0.3->flair) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (5.9.5)\n",
            "Downloading flair-0.15.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bioc-2.1-py3-none-any.whl (33 kB)\n",
            "Downloading boto3-1.37.18-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.6/139.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mpld3-0.5.10-py3-none-any.whl (202 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.6/202.6 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m855.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformer_smaller_training_vocab-0.4.0-py3-none-any.whl (14 kB)\n",
            "Downloading botocore-1.37.18-py3-none-any.whl (13.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.11.4-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.4/84.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: langdetect, pptree, sqlitedict, wikipedia-api, docopt, intervaltree\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=4a4a2ecaad8ec6e4d23ff7a4682a67c1b72a41bed0694b07161583e3db656bef\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=2d0ebd74a0f687946cbbdcb4969ce5a9f271399ab634baa7b78e92fc4a424542\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/8a/eb/d683aa6d09dc68ebfde2f37566ddc8807837c4415b4fd2b04c\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16864 sha256=a3fed7b591e1f3663bce6223e18ee18181489e32a984d749377f79fcbc706618\n",
            "  Stored in directory: /root/.cache/pip/wheels/73/63/89/7210274f9b7fb033b8f22671f64c0e0b55083d30c3c046a3ff\n",
            "  Building wheel for wikipedia-api (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia-api: filename=Wikipedia_API-0.8.1-py3-none-any.whl size=15384 sha256=6b718902f5a77685bad08d774005f55c82fa634a042d72f3feffea5d2ebeb99f\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/0f/39/e8214ec038ccd5aeb8c82b957289f2f3ab2251febeae5c2860\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=0d46f787addfd65478a985d173d917ce60c0aab9d34651f065f32645566a2cfb\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26097 sha256=4fa4d68d68f21ad9f8192851eb40d6903957cd97329237174a70d55f78931f3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/d7/d9/eec6891f78cac19a693bd40ecb8365d2f4613318c145ec9816\n",
            "Successfully built langdetect pptree sqlitedict wikipedia-api docopt intervaltree\n",
            "Installing collected packages: sqlitedict, pptree, docopt, segtok, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, langdetect, jsonlines, jmespath, intervaltree, ftfy, conllu, wikipedia-api, nvidia-cusparse-cu12, nvidia-cudnn-cu12, botocore, bioc, s3transfer, nvidia-cusolver-cu12, mpld3, boto3, pytorch-revgrad, transformer-smaller-training-vocab, flair\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed bioc-2.1 boto3-1.37.18 botocore-1.37.18 conllu-4.5.3 docopt-0.6.2 flair-0.15.1 ftfy-6.3.1 intervaltree-3.1.0 jmespath-1.0.1 jsonlines-4.0.0 langdetect-1.0.9 mpld3-0.5.10 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pptree-3.1 pytorch-revgrad-0.2.0 s3transfer-0.11.4 segtok-1.5.11 sqlitedict-2.1.0 transformer-smaller-training-vocab-0.4.0 wikipedia-api-0.8.1\n",
            "Collecting presidio_analyzer\n",
            "  Downloading presidio_analyzer-2.2.358-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting phonenumbers<9.0.0,>=8.12 (from presidio_analyzer)\n",
            "  Downloading phonenumbers-8.13.55-py2.py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (2024.11.6)\n",
            "Requirement already satisfied: spacy!=3.7.0,<4.0.0,>=3.4.4 in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (3.8.4)\n",
            "Collecting tldextract (from presidio_analyzer)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (3.10)\n",
            "Collecting requests-file>=1.4 (from tldextract->presidio_analyzer)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (3.18.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.1.2)\n",
            "Downloading presidio_analyzer-2.2.358-py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading phonenumbers-8.13.55-py2.py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.9/104.9 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Installing collected packages: phonenumbers, requests-file, tldextract, presidio_analyzer\n",
            "Successfully installed phonenumbers-8.13.55 presidio_analyzer-2.2.358 requests-file-2.1.0 tldextract-5.1.3\n",
            "Collecting presidio_anonymizer\n",
            "  Downloading presidio_anonymizer-2.2.358-py3-none-any.whl.metadata (8.1 kB)\n",
            "Requirement already satisfied: cryptography<44.1 in /usr/local/lib/python3.11/dist-packages (from presidio_anonymizer) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<44.1->presidio_anonymizer) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<44.1->presidio_anonymizer) (2.22)\n",
            "Downloading presidio_anonymizer-2.2.358-py3-none-any.whl (31 kB)\n",
            "Installing collected packages: presidio_anonymizer\n",
            "Successfully installed presidio_anonymizer-2.2.358\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "Successfully installed numpy-1.26.4\n",
            "Collecting Cython\n",
            "  Downloading Cython-3.0.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.7/57.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests<3.0.0,>=2.13.0 (from spacy)\n",
            "  Downloading requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
            "  Downloading pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting jinja2 (from spacy)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting setuptools (from spacy)\n",
            "  Downloading setuptools-77.0.3-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting packaging>=20.0 (from spacy)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting typing-extensions>=4.12.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
            "  Downloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading marisa_trie-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading Cython-3.0.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (218 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m218.9/218.9 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.0/183.0 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading murmurhash-1.0.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.2/157.2 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m431.7/431.7 kB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m66.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.9/64.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typer-0.15.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading setuptools-77.0.3-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading blis-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.7/11.7 MB\u001b[0m \u001b[31m88.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.9/143.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading idna-3.10-py3-none-any.whl (70 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.4/70.4 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.4/242.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.4/128.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marisa_trie-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: cymem, wrapt, wasabi, urllib3, typing-extensions, tqdm, spacy-loggers, spacy-legacy, shellingham, setuptools, pygments, packaging, numpy, murmurhash, mdurl, MarkupSafe, idna, Cython, cloudpathlib, click, charset-normalizer, certifi, catalogue, annotated-types, srsly, smart-open, requests, pydantic-core, preshed, markdown-it-py, marisa-trie, jinja2, blis, rich, pydantic, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
            "  Attempting uninstall: cymem\n",
            "    Found existing installation: cymem 2.0.11\n",
            "    Uninstalling cymem-2.0.11:\n",
            "      Successfully uninstalled cymem-2.0.11\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.3\n",
            "    Uninstalling wasabi-1.1.3:\n",
            "      Successfully uninstalled wasabi-1.1.3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: spacy-loggers\n",
            "    Found existing installation: spacy-loggers 1.0.5\n",
            "    Uninstalling spacy-loggers-1.0.5:\n",
            "      Successfully uninstalled spacy-loggers-1.0.5\n",
            "  Attempting uninstall: spacy-legacy\n",
            "    Found existing installation: spacy-legacy 3.0.12\n",
            "    Uninstalling spacy-legacy-3.0.12:\n",
            "      Successfully uninstalled spacy-legacy-3.0.12\n",
            "  Attempting uninstall: shellingham\n",
            "    Found existing installation: shellingham 1.5.4\n",
            "    Uninstalling shellingham-1.5.4:\n",
            "      Successfully uninstalled shellingham-1.5.4\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 75.1.0\n",
            "    Uninstalling setuptools-75.1.0:\n",
            "      Successfully uninstalled setuptools-75.1.0\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.18.0\n",
            "    Uninstalling Pygments-2.18.0:\n",
            "      Successfully uninstalled Pygments-2.18.0\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: murmurhash\n",
            "    Found existing installation: murmurhash 1.0.12\n",
            "    Uninstalling murmurhash-1.0.12:\n",
            "      Successfully uninstalled murmurhash-1.0.12\n",
            "  Attempting uninstall: mdurl\n",
            "    Found existing installation: mdurl 0.1.2\n",
            "    Uninstalling mdurl-0.1.2:\n",
            "      Successfully uninstalled mdurl-0.1.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 3.0.12\n",
            "    Uninstalling Cython-3.0.12:\n",
            "      Successfully uninstalled Cython-3.0.12\n",
            "  Attempting uninstall: cloudpathlib\n",
            "    Found existing installation: cloudpathlib 0.21.0\n",
            "    Uninstalling cloudpathlib-0.21.0:\n",
            "      Successfully uninstalled cloudpathlib-0.21.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.8\n",
            "    Uninstalling click-8.1.8:\n",
            "      Successfully uninstalled click-8.1.8\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.1\n",
            "    Uninstalling charset-normalizer-3.4.1:\n",
            "      Successfully uninstalled charset-normalizer-3.4.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.10\n",
            "    Uninstalling catalogue-2.0.10:\n",
            "      Successfully uninstalled catalogue-2.0.10\n",
            "  Attempting uninstall: annotated-types\n",
            "    Found existing installation: annotated-types 0.7.0\n",
            "    Uninstalling annotated-types-0.7.0:\n",
            "      Successfully uninstalled annotated-types-0.7.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.5.1\n",
            "    Uninstalling srsly-2.5.1:\n",
            "      Successfully uninstalled srsly-2.5.1\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.27.2\n",
            "    Uninstalling pydantic_core-2.27.2:\n",
            "      Successfully uninstalled pydantic_core-2.27.2\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.9\n",
            "    Uninstalling preshed-3.0.9:\n",
            "      Successfully uninstalled preshed-3.0.9\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: marisa-trie\n",
            "    Found existing installation: marisa-trie 1.2.1\n",
            "    Uninstalling marisa-trie-1.2.1:\n",
            "      Successfully uninstalled marisa-trie-1.2.1\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.2.0\n",
            "    Uninstalling blis-1.2.0:\n",
            "      Successfully uninstalled blis-1.2.0\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.9.4\n",
            "    Uninstalling rich-13.9.4:\n",
            "      Successfully uninstalled rich-13.9.4\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.10.6\n",
            "    Uninstalling pydantic-2.10.6:\n",
            "      Successfully uninstalled pydantic-2.10.6\n",
            "  Attempting uninstall: language-data\n",
            "    Found existing installation: language_data 1.3.0\n",
            "    Uninstalling language_data-1.3.0:\n",
            "      Successfully uninstalled language_data-1.3.0\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.2\n",
            "    Uninstalling typer-0.15.2:\n",
            "      Successfully uninstalled typer-0.15.2\n",
            "  Attempting uninstall: langcodes\n",
            "    Found existing installation: langcodes 3.5.0\n",
            "    Uninstalling langcodes-3.5.0:\n",
            "      Successfully uninstalled langcodes-3.5.0\n",
            "  Attempting uninstall: confection\n",
            "    Found existing installation: confection 0.1.5\n",
            "    Uninstalling confection-0.1.5:\n",
            "      Successfully uninstalled confection-0.1.5\n",
            "  Attempting uninstall: weasel\n",
            "    Found existing installation: weasel 0.4.1\n",
            "    Uninstalling weasel-0.4.1:\n",
            "      Successfully uninstalled weasel-0.4.1\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.4\n",
            "    Uninstalling thinc-8.3.4:\n",
            "      Successfully uninstalled thinc-8.3.4\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.4\n",
            "    Uninstalling spacy-3.8.4:\n",
            "      Successfully uninstalled spacy-3.8.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "transformer-smaller-training-vocab 0.4.0 requires numpy<2.0.0,>=1.21.0; python_version >= \"3.9\", but you have numpy 2.2.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Cython-3.0.12 MarkupSafe-3.0.2 annotated-types-0.7.0 blis-1.2.0 catalogue-2.0.10 certifi-2025.1.31 charset-normalizer-3.4.1 click-8.1.8 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 idna-3.10 jinja2-3.1.6 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 numpy-2.2.4 packaging-24.2 preshed-3.0.9 pydantic-2.10.6 pydantic-core-2.27.2 pygments-2.19.1 requests-2.32.3 rich-13.9.4 setuptools-77.0.3 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 tqdm-4.67.1 typer-0.15.2 typing-extensions-4.12.2 urllib3-2.3.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "certifi"
                ]
              },
              "id": "f2d68674a30945eabc457eb62fccf1ba"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing on smaller dataset\n",
        "import re\n",
        "import json\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from flair.data import Sentence\n",
        "from flair.models import SequenceTagger\n",
        "from threading import Lock\n",
        "\n",
        "# Load models\n",
        "flair_tagger = SequenceTagger.load(\"flair/pos-english\")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Constants\n",
        "PHONE_REGEX = r\"\\b(\\+\\d{1,4}\\s?)?(\\(?\\d{1,4}\\)?[-.]?\\s?)?\\d{3,4}[-.]?\\d{3,4}[-.]?\\d{3,4}\\b\"\n",
        "DESCRIPTIVE_COLUMNS = {\"description\", \"comment\", \"comments\", \"query\", \"notes\"}\n",
        "SENSITIVE_COLUMNS = set()\n",
        "MAPPINGS = {}\n",
        "VALUE_TO_MASK = {}\n",
        "mask_lock = Lock()\n",
        "\n",
        "def is_noun_column(texts):\n",
        "    \"\"\"Identify if a column contains mostly nouns based on a sample.\"\"\"\n",
        "    texts = [\"\" if pd.isna(text) else str(text) for text in texts]\n",
        "    sentences = [Sentence(text) for text in texts]\n",
        "    flair_tagger.predict(sentences)\n",
        "    noun_count = sum(any(\"NN\" in token.tag for token in sent) for sent in sentences)\n",
        "    return noun_count > (len(texts) * 0.6)\n",
        "\n",
        "def mask_text(text, col_name, row_idx):\n",
        "    \"\"\"Mask the input text and store the mapping.\"\"\"\n",
        "    if pd.isna(text) or str(text).strip() == \"\":\n",
        "        return text\n",
        "    with mask_lock:\n",
        "        if text in VALUE_TO_MASK:\n",
        "            return VALUE_TO_MASK[text]\n",
        "        masked_value = f\"{col_name}_{row_idx}\"\n",
        "        VALUE_TO_MASK[text] = masked_value\n",
        "        MAPPINGS[masked_value] = text\n",
        "    return masked_value\n",
        "\n",
        "def process_chunk(chunk):\n",
        "    \"\"\"Process a dataframe chunk (30 rows) for masking and return a new masked DataFrame.\"\"\"\n",
        "    # Create a copy of the DataFrame to avoid modifying the original\n",
        "    new_chunk = chunk.copy()\n",
        "\n",
        "    for row_idx in new_chunk.index:\n",
        "        # Mask sensitive columns\n",
        "        for col in SENSITIVE_COLUMNS:\n",
        "            new_chunk.loc[row_idx, col] = mask_text(new_chunk.loc[row_idx, col], col, row_idx)\n",
        "\n",
        "        # Mask phone numbers\n",
        "        for col in new_chunk.columns:\n",
        "            new_chunk.loc[row_idx, col] = re.sub(PHONE_REGEX, lambda m: mask_text(m.group(0), \"phone\", row_idx), str(new_chunk.loc[row_idx, col]))\n",
        "\n",
        "        # Mask descriptive text columns\n",
        "        for col in set(DESCRIPTIVE_COLUMNS) & set(new_chunk.columns):\n",
        "            doc = nlp(str(new_chunk.loc[row_idx, col]))\n",
        "            for ent in doc.ents:\n",
        "                new_chunk.loc[row_idx, col] = new_chunk.loc[row_idx, col].replace(ent.text, mask_text(ent.text, col, row_idx))\n",
        "\n",
        "    return new_chunk  # Returning the new modified DataFrame\n",
        "\n",
        "\n",
        "def main(file_path):\n",
        "    \"\"\"Process only 30 rows of the CSV file.\"\"\"\n",
        "    global SENSITIVE_COLUMNS\n",
        "    # Read first 30 rows\n",
        "    df = pd.read_csv(file_path, nrows=30)\n",
        "\n",
        "    # Identify sensitive columns (case-insensitive)\n",
        "    SENSITIVE_COLUMNS = {col for col in df.columns if is_noun_column(df[col].astype(str).tolist())}\n",
        "\n",
        "    # Process and save output\n",
        "    processed_df = process_chunk(df)\n",
        "    print(\"\\nProcessed Data:\\n\", processed_df.head(10))  # Display first 10 rows for verification\n",
        "    processed_df.to_csv(\"masked_output.csv\", index=False)\n",
        "\n",
        "    with open(\"mappings.json\", \"w\") as f:\n",
        "        json.dump(MAPPINGS, f)\n",
        "\n",
        "    print(\"Processing complete. Masked data saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main('smaller_companies.csv')\n"
      ],
      "metadata": {
        "id": "CS9jIsokfzzB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TESTING CODE!**"
      ],
      "metadata": {
        "id": "wpOzics_xYe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Regex\n",
        "EMAIL_REGEX = r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n",
        "PHONE_REGEX = r\"\\b(\\+\\d{1,4}\\s?)?(\\(?\\d{1,4}\\)?[-.]?\\s?)?\\d{3,4}[-.]?\\d{3,4}[-.]?\\d{3,4}\\b\"\n",
        "\n",
        "# def detect_noun_desc(text):\n",
        "#     doc = nlp(text)\n",
        "#     modified_text = []\n",
        "\n",
        "#     for token in doc:\n",
        "#         token_text = token.text\n",
        "\n",
        "#         if re.fullmatch(EMAIL_REGEX, token_text):\n",
        "#             modified_text.append(\"<email>\")\n",
        "#         elif re.fullmatch(PHONE_REGEX, token_text):\n",
        "#             modified_text.append(\"<phone>\")\n",
        "#         elif token.pos_ == \"PROPN\":\n",
        "#             modified_text.append(\"<sensitive>\")\n",
        "#         else:\n",
        "#             modified_text.append(token_text)\n",
        "\n",
        "#     return \" \".join(modified_text)\n",
        "\n",
        "def detect_noun(file_path):\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        df = pd.read_csv(file_path, engine=\"python\")\n",
        "    elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "        df = pd.read_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
        "\n",
        "    sensitive = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['int64', 'float64']:\n",
        "            continue  # Skip purely numeric columns\n",
        "\n",
        "        text_samples = df[col].astype(str).head(5)  # Take first 5 values\n",
        "        noun_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for value in text_samples:\n",
        "            if \"%\" in value or value.replace(\".\", \"\").isdigit():\n",
        "                continue  # Skip percentage or number-like values\n",
        "\n",
        "            doc = nlp(value)\n",
        "            for token in doc:\n",
        "                if token.pos_ in ['NOUN', 'PROPN']:\n",
        "                    noun_count += 1\n",
        "                total_count += 1\n",
        "\n",
        "        # Mark column as sensitive only if a significant portion are nouns\n",
        "        if total_count > 0 and (noun_count / total_count) > 0.2:\n",
        "            sensitive.append(col)\n",
        "\n",
        "    return sensitive"
      ],
      "metadata": {
        "id": "R4UOPh72x2mt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def descriptive_cols(file_path):\n",
        "    \"\"\"\n",
        "    Reads a CSV or Excel file, removes column names similar to 'description', 'remarks', etc.,\n",
        "    and returns a filtered list of sensitive columns.\n",
        "    \"\"\"\n",
        "    # Define keywords to filter out\n",
        "    keywords = [\"description\", \"remarks\", \"notes\", \"comments\", \"observations\", \"details\", \"summary\", \"explanation\",\n",
        "    \"reviews\", \"feedback\", \"testimonials\", \"opinions\", \"assessment\", \"suggestions\", \"experience\",\n",
        "    \"incident_report\", \"case_notes\", \"audit_notes\", \"findings\", \"status_update\", \"history\", \"progress_report\",\n",
        "    \"additional_info\", \"clarifications\", \"justification\", \"annotations\", \"excerpts\", \"statement\", \"explanation_text\"]\n",
        "\n",
        "    # Ensure columns are properly loaded from CSV/Excel\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        df = pd.read_csv(file_path, nrows=1)  # Read only header\n",
        "    elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "        df = pd.read_excel(file_path, nrows=1, engine=\"openpyxl\")  # Read only header\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
        "\n",
        "    # Get actual column names\n",
        "    all_columns = df.columns.tolist()\n",
        "\n",
        "    # Filter out columns that match the keywords\n",
        "    filtered_columns = [col for col in all_columns if any(re.search(keyword, col, re.IGNORECASE) for keyword in keywords)]\n",
        "\n",
        "    return filtered_columns\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R6F-UtGRxt9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def detect_noun(file_path):\n",
        "    # Detect file type and read accordingly\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        df = pd.read_csv(file_path, engine=\"python\")\n",
        "    elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "        df = pd.read_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
        "\n",
        "    sensitive = []\n",
        "    column=[]\n",
        "    for col in df.columns:\n",
        "        column.append(col)\n",
        "        # **Skip numeric columns**\n",
        "        if pd.api.types.is_numeric_dtype(df[col]): continue\n",
        "\n",
        "        # **Process only non-empty, valid string values**\n",
        "        valid_values = df[col].dropna().astype(str)  # Remove NaN\n",
        "        valid_values = [val for val in valid_values.head(3) if val.lower() not in [\"nan\", \"none\", \"\",\"na\",\"null\",\"n/a\"]]\n",
        "\n",
        "        for value in valid_values:\n",
        "            doc = nlp(value)\n",
        "            noun_count = sum(1 for token in doc if token.pos_ in ['NOUN', 'PROPN'])\n",
        "            word_count = len(doc)\n",
        "\n",
        "            # **Mark column as sensitive only if a high % of words are nouns**\n",
        "            if word_count > 0 and (noun_count / word_count) > 0.2:\n",
        "                if col not in sensitive:\n",
        "                    sensitive.append(col)\n",
        "    print(column)\n",
        "    print(sensitive)\n",
        "    return sensitive\n",
        "\n"
      ],
      "metadata": {
        "id": "hIImmYAbxsnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing code!\n",
        "# Main code\n",
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "mapping = {}\n",
        "\n",
        "def csv_an(input_file, output_file):\n",
        "    df = pd.read_csv(input_file, engine=\"python\")\n",
        "\n",
        "    sensitive_old = detect_noun(input_file)\n",
        "    descrp = descriptive_cols(input_file)\n",
        "    sensitive = list(set(sensitive_old) - set(descrp))\n",
        "    column_counters = {col: 1 for col in df.columns if col in sensitive_old}\n",
        "    column_mappings = {col: {} for col in sensitive}  # Store mappings for each column\n",
        "\n",
        "    for col in sensitive:\n",
        "        new_values = []\n",
        "        for val in df[col].astype(str):\n",
        "            if pd.notna(val):\n",
        "                if val in column_mappings[col]:\n",
        "                    anonymized_value = column_mappings[col][val]  # Use existing mapping\n",
        "                else:\n",
        "                    anonymized_value = f\"{col}{column_counters[col]}\"\n",
        "                    column_mappings[col][val] = anonymized_value  # Store new mapping\n",
        "                    column_counters[col] += 1  # Increment counter\n",
        "\n",
        "                mapping[anonymized_value] = val\n",
        "                new_values.append(anonymized_value)\n",
        "            else:\n",
        "                new_values.append(val)\n",
        "        df[col] = new_values\n",
        "\n",
        "    for col in descrp:\n",
        "        for idx, val in enumerate(df[col].astype(str)):\n",
        "            if pd.notna(val):\n",
        "                df.at[idx, col] = detect_noun_desc(val)\n",
        "\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Anonymized file saved as {output_file}\")\n",
        "\n",
        "    # Save mapping as JSON\n",
        "    with open(\"mappings.json\", \"w\") as f:\n",
        "        json.dump(mapping, f)\n",
        "\n",
        "\n",
        "csv_an('testing-csv-ff.csv', 'masked_csv_output.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJhoM_Lrw3Gt",
        "outputId": "360b7443-725b-4fcc-b2a5-5876878391b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Reporting Month', 'Cluster Name', 'Input Account Name', 'Account Name', 'New Assets Issuance', 'Allocations', 'Welcome Email', 'Onboarding Trainings', 'BGV', 'NDA', 'Granting Access', 'Client Onboarding', 'Deallocation', 'Assets Return', 'Access Removal', 'Compliance % on mandatory trainings', 'Compliance % on mandatory audits', 'Timesheet Compliance %']\n",
            "['Reporting Month', 'Cluster Name', 'Input Account Name', 'Account Name', 'Client Onboarding', 'Compliance % on mandatory trainings', 'Compliance % on mandatory audits', 'Timesheet Compliance %']\n",
            "Anonymized file saved as masked_csv_output.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CONSOLITATING CODE**"
      ],
      "metadata": {
        "id": "8_D1dww-ehSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flair\n",
        "!pip install faker\n",
        "!pip install presidio_analyzer\n",
        "!pip install presidio_anonymizer\n",
        "# !python -m spacy download en_core_web_lg\n",
        "!pip install --upgrade --force-reinstall \"numpy==1.26.4\"\n",
        "!pip install --upgrade --force-reinstall \"Cython\" \"spacy\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FU2nNoKeesf-",
        "outputId": "ba078415-05fe-4b4a-ba31-584d5f60d5f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flair in /usr/local/lib/python3.11/dist-packages (0.15.1)\n",
            "Requirement already satisfied: boto3>=1.20.27 in /usr/local/lib/python3.11/dist-packages (from flair) (1.37.19)\n",
            "Requirement already satisfied: conllu<5.0.0,>=4.0 in /usr/local/lib/python3.11/dist-packages (from flair) (4.5.3)\n",
            "Requirement already satisfied: deprecated>=1.2.13 in /usr/local/lib/python3.11/dist-packages (from flair) (1.2.18)\n",
            "Requirement already satisfied: ftfy>=6.1.0 in /usr/local/lib/python3.11/dist-packages (from flair) (6.3.1)\n",
            "Requirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from flair) (5.2.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from flair) (0.29.3)\n",
            "Requirement already satisfied: langdetect>=1.0.9 in /usr/local/lib/python3.11/dist-packages (from flair) (1.0.9)\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from flair) (5.3.1)\n",
            "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from flair) (3.10.0)\n",
            "Requirement already satisfied: more-itertools>=8.13.0 in /usr/local/lib/python3.11/dist-packages (from flair) (10.6.0)\n",
            "Requirement already satisfied: mpld3>=0.3 in /usr/local/lib/python3.11/dist-packages (from flair) (0.5.10)\n",
            "Requirement already satisfied: pptree>=3.1 in /usr/local/lib/python3.11/dist-packages (from flair) (3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from flair) (2.8.2)\n",
            "Requirement already satisfied: pytorch-revgrad>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from flair) (0.2.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from flair) (2024.11.6)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.11/dist-packages (from flair) (1.6.1)\n",
            "Requirement already satisfied: segtok>=1.5.11 in /usr/local/lib/python3.11/dist-packages (from flair) (1.5.11)\n",
            "Requirement already satisfied: sqlitedict>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from flair) (2.1.0)\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.11/dist-packages (from flair) (0.9.0)\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.11/dist-packages (from flair) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.63.0 in /usr/local/lib/python3.11/dist-packages (from flair) (4.67.1)\n",
            "Requirement already satisfied: transformer-smaller-training-vocab>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from flair) (0.4.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.25.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (4.49.0)\n",
            "Requirement already satisfied: wikipedia-api>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from flair) (0.8.1)\n",
            "Requirement already satisfied: bioc<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from flair) (2.1)\n",
            "Requirement already satisfied: jsonlines>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from bioc<3.0.0,>=2.0.0->flair) (4.0.0)\n",
            "Requirement already satisfied: intervaltree in /usr/local/lib/python3.11/dist-packages (from bioc<3.0.0,>=2.0.0->flair) (3.1.0)\n",
            "Requirement already satisfied: docopt in /usr/local/lib/python3.11/dist-packages (from bioc<3.0.0,>=2.0.0->flair) (0.6.2)\n",
            "Requirement already satisfied: botocore<1.38.0,>=1.37.19 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.20.27->flair) (1.37.19)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.20.27->flair) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3>=1.20.27->flair) (0.11.4)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.13->flair) (1.17.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy>=6.1.0->flair) (0.2.13)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown>=4.4.0->flair) (2.32.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.10.0->flair) (4.12.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect>=1.0.9->flair) (1.17.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (2.2.4)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.2.3->flair) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mpld3>=0.3->flair) (3.1.6)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.2->flair) (3.6.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.1->flair) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.1->flair) (1.3.0)\n",
            "Collecting numpy>=1.23 (from matplotlib>=2.2.3->flair)\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.25.0->transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.5.3)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<5.0.0,>=4.25.0->flair) (5.29.4)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.38.0,>=1.37.19->boto3>=1.20.27->flair) (2.3.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonlines>=1.2.0->bioc<3.0.0,>=2.0.0->flair) (25.3.0)\n",
            "Requirement already satisfied: accelerate>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (1.5.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair) (2.6)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from intervaltree->bioc<3.0.0,>=2.0.0->flair) (2.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mpld3>=0.3->flair) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.4.0->flair) (1.7.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.26.0->transformers[sentencepiece,torch]<5.0,>=4.1->transformer-smaller-training-vocab>=0.2.3->flair) (5.9.5)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.2.4\n",
            "    Uninstalling numpy-2.2.4:\n",
            "      Successfully uninstalled numpy-2.2.4\n",
            "Successfully installed numpy-1.26.4\n",
            "Requirement already satisfied: faker in /usr/local/lib/python3.11/dist-packages (37.1.0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n",
            "Requirement already satisfied: presidio_analyzer in /usr/local/lib/python3.11/dist-packages (2.2.358)\n",
            "Requirement already satisfied: phonenumbers<9.0.0,>=8.12 in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (8.13.55)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (6.0.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (2024.11.6)\n",
            "Requirement already satisfied: spacy!=3.7.0,<4.0.0,>=3.4.4 in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (3.8.4)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.11/dist-packages (from presidio_analyzer) (5.1.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (78.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (3.10)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (2.1.0)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.11/dist-packages (from tldextract->presidio_analyzer) (3.18.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (2.19.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy!=3.7.0,<4.0.0,>=3.4.4->presidio_analyzer) (0.1.2)\n",
            "Requirement already satisfied: presidio_anonymizer in /usr/local/lib/python3.11/dist-packages (2.2.358)\n",
            "Requirement already satisfied: cryptography<44.1 in /usr/local/lib/python3.11/dist-packages (from presidio_anonymizer) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography<44.1->presidio_anonymizer) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography<44.1->presidio_anonymizer) (2.22)\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Installing collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "Successfully installed numpy-1.26.4\n",
            "Collecting Cython\n",
            "  Using cached Cython-3.0.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting spacy\n",
            "  Using cached spacy-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (27 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Using cached murmurhash-1.0.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Using cached cymem-2.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.5 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Using cached preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Using cached thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Using cached srsly-2.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
            "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
            "  Using cached typer-0.15.2-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting tqdm<5.0.0,>=4.38.0 (from spacy)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
            "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
            "  Using cached pydantic-2.10.6-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting jinja2 (from spacy)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting setuptools (from spacy)\n",
            "  Using cached setuptools-78.0.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting packaging>=20.0 (from spacy)\n",
            "  Using cached packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Using cached langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Using cached language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.27.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Using cached pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting typing-extensions>=4.12.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Using cached charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting blis<1.3.0,>=1.2.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Using cached blis-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting click>=8.0.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Using cached cloudpathlib-0.21.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Using cached smart_open-7.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
            "  Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Using cached marisa_trie-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached pygments-2.19.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Using cached Cython-3.0.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "Using cached spacy-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.6 MB)\n",
            "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Using cached cymem-2.0.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (218 kB)\n",
            "Using cached langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "Using cached murmurhash-1.0.12-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (134 kB)\n",
            "Using cached numpy-2.2.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.4 MB)\n",
            "Using cached packaging-24.2-py3-none-any.whl (65 kB)\n",
            "Using cached preshed-3.0.9-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (157 kB)\n",
            "Using cached pydantic-2.10.6-py3-none-any.whl (431 kB)\n",
            "Using cached pydantic_core-2.27.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
            "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Using cached srsly-2.5.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "Using cached thinc-8.3.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.9 MB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Using cached typer-0.15.2-py3-none-any.whl (45 kB)\n",
            "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached setuptools-78.0.2-py3-none-any.whl (1.3 MB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Using cached blis-1.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.7 MB)\n",
            "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
            "Using cached charset_normalizer-3.4.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (143 kB)\n",
            "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Using cached cloudpathlib-0.21.0-py3-none-any.whl (52 kB)\n",
            "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Using cached language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
            "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Using cached smart_open-7.1.0-py3-none-any.whl (61 kB)\n",
            "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
            "Using cached marisa_trie-1.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.4 MB)\n",
            "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
            "Using cached pygments-2.19.1-py3-none-any.whl (1.2 MB)\n",
            "Using cached wrapt-1.17.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (83 kB)\n",
            "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Installing collected packages: cymem, wrapt, wasabi, urllib3, typing-extensions, tqdm, spacy-loggers, spacy-legacy, shellingham, setuptools, pygments, packaging, numpy, murmurhash, mdurl, MarkupSafe, idna, Cython, cloudpathlib, click, charset-normalizer, certifi, catalogue, annotated-types, srsly, smart-open, requests, pydantic-core, preshed, markdown-it-py, marisa-trie, jinja2, blis, rich, pydantic, language-data, typer, langcodes, confection, weasel, thinc, spacy\n",
            "  Attempting uninstall: cymem\n",
            "    Found existing installation: cymem 2.0.11\n",
            "    Uninstalling cymem-2.0.11:\n",
            "      Successfully uninstalled cymem-2.0.11\n",
            "  Attempting uninstall: wrapt\n",
            "    Found existing installation: wrapt 1.17.2\n",
            "    Uninstalling wrapt-1.17.2:\n",
            "      Successfully uninstalled wrapt-1.17.2\n",
            "  Attempting uninstall: wasabi\n",
            "    Found existing installation: wasabi 1.1.3\n",
            "    Uninstalling wasabi-1.1.3:\n",
            "      Successfully uninstalled wasabi-1.1.3\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.3.0\n",
            "    Uninstalling urllib3-2.3.0:\n",
            "      Successfully uninstalled urllib3-2.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.12.2\n",
            "    Uninstalling typing_extensions-4.12.2:\n",
            "      Successfully uninstalled typing_extensions-4.12.2\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.67.1\n",
            "    Uninstalling tqdm-4.67.1:\n",
            "      Successfully uninstalled tqdm-4.67.1\n",
            "  Attempting uninstall: spacy-loggers\n",
            "    Found existing installation: spacy-loggers 1.0.5\n",
            "    Uninstalling spacy-loggers-1.0.5:\n",
            "      Successfully uninstalled spacy-loggers-1.0.5\n",
            "  Attempting uninstall: spacy-legacy\n",
            "    Found existing installation: spacy-legacy 3.0.12\n",
            "    Uninstalling spacy-legacy-3.0.12:\n",
            "      Successfully uninstalled spacy-legacy-3.0.12\n",
            "  Attempting uninstall: shellingham\n",
            "    Found existing installation: shellingham 1.5.4\n",
            "    Uninstalling shellingham-1.5.4:\n",
            "      Successfully uninstalled shellingham-1.5.4\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 78.0.2\n",
            "    Uninstalling setuptools-78.0.2:\n",
            "      Successfully uninstalled setuptools-78.0.2\n",
            "  Attempting uninstall: pygments\n",
            "    Found existing installation: Pygments 2.19.1\n",
            "    Uninstalling Pygments-2.19.1:\n",
            "      Successfully uninstalled Pygments-2.19.1\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 24.2\n",
            "    Uninstalling packaging-24.2:\n",
            "      Successfully uninstalled packaging-24.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.26.4\n",
            "    Uninstalling numpy-1.26.4:\n",
            "      Successfully uninstalled numpy-1.26.4\n",
            "  Attempting uninstall: murmurhash\n",
            "    Found existing installation: murmurhash 1.0.12\n",
            "    Uninstalling murmurhash-1.0.12:\n",
            "      Successfully uninstalled murmurhash-1.0.12\n",
            "  Attempting uninstall: mdurl\n",
            "    Found existing installation: mdurl 0.1.2\n",
            "    Uninstalling mdurl-0.1.2:\n",
            "      Successfully uninstalled mdurl-0.1.2\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 3.0.12\n",
            "    Uninstalling Cython-3.0.12:\n",
            "      Successfully uninstalled Cython-3.0.12\n",
            "  Attempting uninstall: cloudpathlib\n",
            "    Found existing installation: cloudpathlib 0.21.0\n",
            "    Uninstalling cloudpathlib-0.21.0:\n",
            "      Successfully uninstalled cloudpathlib-0.21.0\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.8\n",
            "    Uninstalling click-8.1.8:\n",
            "      Successfully uninstalled click-8.1.8\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.4.1\n",
            "    Uninstalling charset-normalizer-3.4.1:\n",
            "      Successfully uninstalled charset-normalizer-3.4.1\n",
            "  Attempting uninstall: certifi\n",
            "    Found existing installation: certifi 2025.1.31\n",
            "    Uninstalling certifi-2025.1.31:\n",
            "      Successfully uninstalled certifi-2025.1.31\n",
            "  Attempting uninstall: catalogue\n",
            "    Found existing installation: catalogue 2.0.10\n",
            "    Uninstalling catalogue-2.0.10:\n",
            "      Successfully uninstalled catalogue-2.0.10\n",
            "  Attempting uninstall: annotated-types\n",
            "    Found existing installation: annotated-types 0.7.0\n",
            "    Uninstalling annotated-types-0.7.0:\n",
            "      Successfully uninstalled annotated-types-0.7.0\n",
            "  Attempting uninstall: srsly\n",
            "    Found existing installation: srsly 2.5.1\n",
            "    Uninstalling srsly-2.5.1:\n",
            "      Successfully uninstalled srsly-2.5.1\n",
            "  Attempting uninstall: smart-open\n",
            "    Found existing installation: smart-open 7.1.0\n",
            "    Uninstalling smart-open-7.1.0:\n",
            "      Successfully uninstalled smart-open-7.1.0\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.32.3\n",
            "    Uninstalling requests-2.32.3:\n",
            "      Successfully uninstalled requests-2.32.3\n",
            "  Attempting uninstall: pydantic-core\n",
            "    Found existing installation: pydantic_core 2.27.2\n",
            "    Uninstalling pydantic_core-2.27.2:\n",
            "      Successfully uninstalled pydantic_core-2.27.2\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.9\n",
            "    Uninstalling preshed-3.0.9:\n",
            "      Successfully uninstalled preshed-3.0.9\n",
            "  Attempting uninstall: markdown-it-py\n",
            "    Found existing installation: markdown-it-py 3.0.0\n",
            "    Uninstalling markdown-it-py-3.0.0:\n",
            "      Successfully uninstalled markdown-it-py-3.0.0\n",
            "  Attempting uninstall: marisa-trie\n",
            "    Found existing installation: marisa-trie 1.2.1\n",
            "    Uninstalling marisa-trie-1.2.1:\n",
            "      Successfully uninstalled marisa-trie-1.2.1\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: blis\n",
            "    Found existing installation: blis 1.2.0\n",
            "    Uninstalling blis-1.2.0:\n",
            "      Successfully uninstalled blis-1.2.0\n",
            "  Attempting uninstall: rich\n",
            "    Found existing installation: rich 13.9.4\n",
            "    Uninstalling rich-13.9.4:\n",
            "      Successfully uninstalled rich-13.9.4\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.10.6\n",
            "    Uninstalling pydantic-2.10.6:\n",
            "      Successfully uninstalled pydantic-2.10.6\n",
            "  Attempting uninstall: language-data\n",
            "    Found existing installation: language_data 1.3.0\n",
            "    Uninstalling language_data-1.3.0:\n",
            "      Successfully uninstalled language_data-1.3.0\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.15.2\n",
            "    Uninstalling typer-0.15.2:\n",
            "      Successfully uninstalled typer-0.15.2\n",
            "  Attempting uninstall: langcodes\n",
            "    Found existing installation: langcodes 3.5.0\n",
            "    Uninstalling langcodes-3.5.0:\n",
            "      Successfully uninstalled langcodes-3.5.0\n",
            "  Attempting uninstall: confection\n",
            "    Found existing installation: confection 0.1.5\n",
            "    Uninstalling confection-0.1.5:\n",
            "      Successfully uninstalled confection-0.1.5\n",
            "  Attempting uninstall: weasel\n",
            "    Found existing installation: weasel 0.4.1\n",
            "    Uninstalling weasel-0.4.1:\n",
            "      Successfully uninstalled weasel-0.4.1\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 8.3.4\n",
            "    Uninstalling thinc-8.3.4:\n",
            "      Successfully uninstalled thinc-8.3.4\n",
            "  Attempting uninstall: spacy\n",
            "    Found existing installation: spacy 3.8.4\n",
            "    Uninstalling spacy-3.8.4:\n",
            "      Successfully uninstalled spacy-3.8.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "transformer-smaller-training-vocab 0.4.0 requires numpy<2.0.0,>=1.21.0; python_version >= \"3.9\", but you have numpy 2.2.4 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.4 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Cython-3.0.12 MarkupSafe-3.0.2 annotated-types-0.7.0 blis-1.2.0 catalogue-2.0.10 certifi-2025.1.31 charset-normalizer-3.4.1 click-8.1.8 cloudpathlib-0.21.0 confection-0.1.5 cymem-2.0.11 idna-3.10 jinja2-3.1.6 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.12 numpy-2.2.4 packaging-24.2 preshed-3.0.9 pydantic-2.10.6 pydantic-core-2.27.2 pygments-2.19.1 requests-2.32.3 rich-13.9.4 setuptools-78.0.2 shellingham-1.5.4 smart-open-7.1.0 spacy-3.8.4 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.4 tqdm-4.67.1 typer-0.15.2 typing-extensions-4.12.2 urllib3-2.3.0 wasabi-1.1.3 weasel-0.4.1 wrapt-1.17.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "Cython",
                  "_distutils_hack",
                  "blis",
                  "catalogue",
                  "certifi",
                  "charset_normalizer",
                  "confection",
                  "cymem",
                  "cython",
                  "langcodes",
                  "markupsafe",
                  "murmurhash",
                  "preshed",
                  "requests",
                  "shellingham",
                  "spacy",
                  "srsly",
                  "thinc",
                  "tqdm",
                  "wasabi",
                  "weasel",
                  "wrapt"
                ]
              },
              "id": "b8281dfbdaac404cb2b4b80c2836c068"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "import time\n",
        "import spacy\n",
        "import openpyxl\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "mapping={}\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def time_it(func):\n",
        "  def wrapper(*args, **kwargs):\n",
        "    start = time.time()\n",
        "    result = func(*args, **kwargs)\n",
        "    end = time.time()\n",
        "    print(f'\\n\\nExecution time {func.__name__}: {end-start:.6f} seconds')\n",
        "    return result\n",
        "  return wrapper\n",
        "\n",
        "#-------------------------------------------------------------------------------------------------------------------------------\n",
        "#function to find out the noun values dominating columns considering it will be a sensitive data if there is so many nouns\n",
        "#------------------------------------------------------------------------------------------------------------------------------\n",
        "def detect_noun(file_path):\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        df = pd.read_csv(file_path, engine=\"python\")\n",
        "    elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "        df = pd.read_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
        "\n",
        "    sensitive = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['int64', 'float64']:\n",
        "            continue  # Skip purely numeric columns\n",
        "\n",
        "        text_samples = df[col].astype(str).head(5)  # Take first 5 values\n",
        "        noun_count = 0\n",
        "        total_count = 0\n",
        "\n",
        "        for value in text_samples:\n",
        "            if \"%\" in value or value.replace(\".\", \"\").isdigit():\n",
        "                continue  # Skip percentage or number-like values\n",
        "\n",
        "            doc = nlp(value)\n",
        "            for token in doc:\n",
        "                if token.pos_ in ['NOUN', 'PROPN']:\n",
        "                    noun_count += 1\n",
        "                total_count += 1\n",
        "\n",
        "        # Mark column as sensitive only if a significant portion are nouns\n",
        "        if total_count > 0 and (noun_count / total_count) > 0.2:\n",
        "            sensitive.append(col)\n",
        "\n",
        "    return sensitive\n",
        "\n",
        "\n",
        "#------------------------------------------------------------------------------------------\n",
        "#finding out the descriptive data columns which may have sensitive data in the form of text\n",
        "#------------------------------------------------------------------------------------------\n",
        "def descriptive_columns(file_path):\n",
        "    # Define keywords to filter out\n",
        "    keywords = [\"description\", \"remarks\", \"notes\", \"comments\", \"observations\", \"details\", \"summary\", \"explanation\",\n",
        "    \"reviews\", \"feedback\", \"testimonials\", \"opinions\", \"assessment\", \"suggestions\", \"experience\",\n",
        "    \"incident_report\", \"case_notes\", \"audit_notes\", \"findings\", \"status_update\", \"history\", \"progress_report\",\n",
        "    \"additional_info\", \"clarifications\", \"justification\", \"annotations\", \"excerpts\", \"statement\", \"explanation_text\"]\n",
        "\n",
        "    # Ensure columns are properly loaded from CSV/Excel\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        df = pd.read_csv(file_path, nrows=1)  # Read only header\n",
        "    elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "        df = pd.read_excel(file_path, nrows=1, engine=\"openpyxl\")  # Read only header\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
        "\n",
        "    # Get actual column names\n",
        "    all_columns = df.columns.tolist()\n",
        "\n",
        "\n",
        "    des=[col for col in all_columns if any(re.search(keyword, col, re.IGNORECASE) for keyword in keywords)]\n",
        "    return des\n",
        "\n",
        "#-----------------------------------------------------------------\n",
        "#anonymizing descriptive values\n",
        "#-----------------------------------------------------------------\n",
        "def detect_noun_desc(text):\n",
        "    doc = nlp(text)\n",
        "    modified_text=[]\n",
        "    for token in doc:\n",
        "        if token.pos_ in ['PROPN']:\n",
        "            modified_text.append(\"<sensitive>\")\n",
        "        else:\n",
        "            modified_text.append(token.text)\n",
        "    text=\" \".join(modified_text)\n",
        "    return text\n",
        "\n",
        "\n",
        "#-------------------------------------------------------------------\n",
        "#function to anonymize a excel file\n",
        "#-------------------------------------------------------------------\n",
        "def excel_an(input_file, output_file):\n",
        "    df = pd.read_excel(input_file)  # Read as string for safety\n",
        "\n",
        "    sensitive_old = detect_noun(input_file)\n",
        "    desc = descriptive_columns(input_file)\n",
        "    sensitive = list(set(sensitive_old) - set(desc))\n",
        "\n",
        "    column_counters = {col: 1 for col in df.columns if col in sensitive_old}\n",
        "    column_mappings = {col: {} for col in sensitive}  # Store mappings for each column\n",
        "    mapping = {}\n",
        "\n",
        "    # Anonymize sensitive columns while maintaining consistency\n",
        "    for col in sensitive:\n",
        "        new_values = []\n",
        "        for val in df[col].astype(str):\n",
        "            if pd.notna(val):\n",
        "                if val in column_mappings[col]:\n",
        "                    anonymized_value = column_mappings[col][val]  # Use existing mapping\n",
        "                else:\n",
        "                    anonymized_value = f\"{col}{column_counters[col]}\"\n",
        "                    column_mappings[col][val] = anonymized_value  # Store new mapping\n",
        "                    column_counters[col] += 1  # Increment counter\n",
        "\n",
        "                mapping[anonymized_value] = val\n",
        "                new_values.append(anonymized_value)\n",
        "            else:\n",
        "                new_values.append(val)\n",
        "\n",
        "        df[col] = new_values\n",
        "\n",
        "    # Anonymize descriptive columns\n",
        "    for col in desc:\n",
        "        for idx, val in enumerate(df[col].astype(str)):\n",
        "            if pd.notna(val):\n",
        "                an_values = detect_noun_desc(val)\n",
        "                df.at[idx, col] = an_values\n",
        "                mapping[an_values] = val\n",
        "\n",
        "    df.to_excel(output_file, index=False, sheet_name=\"Anonymized Data\")\n",
        "    print(f\"✅ Anonymized file saved as {output_file}\")\n",
        "\n",
        "    # Save mapping as JSON\n",
        "    with open(\"mappings.json\", \"w\") as f:\n",
        "        json.dump(mapping, f)\n",
        "\n",
        "#------------------------------------------------------\n",
        "#function for de-anonymizing excel data\n",
        "#------------------------------------------------------\n",
        "\n",
        "def excel_dean(input_file, output_file, mapping_file):\n",
        "    print(\"🔄 Loading data...\")\n",
        "\n",
        "    with open(mapping_file, \"r\") as f:\n",
        "        mapping = json.load(f)\n",
        "\n",
        "    df = pd.read_excel(input_file)  # Read as string for safety\n",
        "    mapping_keys = set(mapping.keys())\n",
        "    df = df.applymap(lambda x: mapping[x] if x in mapping_keys else x)\n",
        "    df.to_excel(output_file, index=False, sheet_name=\"De-anonymized Data\")\n",
        "\n",
        "    print(f\"✅ De-anonymized file saved as {output_file}\")\n",
        "\n",
        "#----------------------------------------------------------\n",
        "#function for anonymizing csv data\n",
        "#----------------------------------------------------------\n",
        "def csv_an(input_file, output_file):\n",
        "    df = pd.read_csv(input_file, engine=\"python\")\n",
        "\n",
        "    sensitive_old = detect_noun(input_file)\n",
        "    desc = descriptive_columns(input_file)\n",
        "    sensitive = list(set(sensitive_old) - set(desc))\n",
        "\n",
        "    column_counters = {col: 1 for col in df.columns if col in sensitive_old}\n",
        "    column_mappings = {col: {} for col in sensitive}  # Store mappings for each column\n",
        "\n",
        "    # Anonymize sensitive columns while maintaining consistency\n",
        "    for col in sensitive:\n",
        "        new_values = []\n",
        "        for val in df[col].astype(str):\n",
        "            if pd.notna(val):\n",
        "                if val in column_mappings[col]:\n",
        "                    anonymized_value = column_mappings[col][val]  # Use existing mapping\n",
        "                else:\n",
        "                    anonymized_value = f\"{col}{column_counters[col]}\"\n",
        "                    column_mappings[col][val] = anonymized_value  # Store new mapping\n",
        "                    column_counters[col] += 1  # Increment counter\n",
        "\n",
        "                mapping[anonymized_value] = val\n",
        "                new_values.append(anonymized_value)\n",
        "            else:\n",
        "                new_values.append(val)\n",
        "\n",
        "        df[col] = new_values\n",
        "\n",
        "    # Anonymize descriptive columns\n",
        "    for col in desc:\n",
        "        for idx, val in enumerate(df[col].astype(str)):\n",
        "            if pd.notna(val):\n",
        "                an_values = detect_noun_desc(val)\n",
        "                df.at[idx, col] = an_values\n",
        "                mapping[an_values] = val\n",
        "\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"✅ Anonymized file saved as {output_file}\")\n",
        "\n",
        "    # Save mapping as JSON\n",
        "    with open(\"mappings.json\", \"w\") as f:\n",
        "        json.dump(mapping, f)\n",
        "\n",
        "#------------------------------------------------------------\n",
        "#function for de anonymizing csv data\n",
        "#------------------------------------------------------------\n",
        "def csv_dean(input_file, output_file, mapping_file):\n",
        "    print(\"🔄 Loading data...\")\n",
        "\n",
        "    with open(mapping_file, \"r\") as f:\n",
        "        mapping = json.load(f)\n",
        "    df = pd.read_csv(input_file, engine=\"python\", dtype=str)  # Read as string for safety\n",
        "    mapping_keys = set(mapping.keys())\n",
        "    df = df.applymap(lambda x: mapping[x] if x in mapping_keys else x)\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "    print(f\"✅ De-anonymized file saved as {output_file}\")\n",
        "\n",
        "\n",
        "#--------------------------------------------------------------\n",
        "#function to determine whcih type of data is need to perform\n",
        "#--------------------------------------------------------------\n",
        "@time_it\n",
        "def anonymization(input_file):\n",
        "    if input_file.endswith(\".csv\"):\n",
        "        csv_an(input_file,\"intermediate.csv\")\n",
        "        csv_dean(\"intermediate.csv\",\"deanonymized.csv\",\"mappings.json\")\n",
        "    elif input_file.endswith(\".xlsx\"):\n",
        "        excel_an(input_file,\"intermediate.xlsx\")\n",
        "        excel_dean(\"intermediate.xlsx\",\"deanonymized.xlsx\",\"mappings.json\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0].strip().replace(' ', '-')\n",
        "anonymization(file_name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "LyYn8GSqeobK",
        "outputId": "70617204-e87a-4923-abe3-bb5acb6e9657"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-994c98d3-552c-4c0d-839a-034a26198764\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-994c98d3-552c-4c0d-839a-034a26198764\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving testing-csv-ff.csv to testing-csv-ff.csv\n",
            "✅ Anonymized file saved as intermediate.csv\n",
            "🔄 Loading data...\n",
            "✅ De-anonymized file saved as deanonymized.csv\n",
            "\n",
            "\n",
            "Execution time anonymization: 0.533583 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-c92c682ea52b>:220: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: mapping[x] if x in mapping_keys else x)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing simplified consolidated code\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "\n",
        "mapping = {}\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "analyzer = AnalyzerEngine()\n",
        "anonymizer = AnonymizerEngine()\n",
        "\n",
        "def time_it(func):\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(f'\\n\\nExecution time {func.__name__}: {end-start:.6f} seconds')\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "def read_file(file_path):\n",
        "    \"\"\"Read CSV or Excel file and return DataFrame.\"\"\"\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        return pd.read_csv(file_path, engine=\"python\"), \"csv\"\n",
        "    elif file_path.endswith(\".xlsx\"):\n",
        "        return pd.read_excel(file_path, engine=\"openpyxl\"), \"excel\"\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
        "\n",
        "def detect_noun(df):\n",
        "    sensitive = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['int64', 'float64'] or re.search(r'\\b(id|rate|amount|price|cost|value|score|percentage)\\b', col, re.IGNORECASE):\n",
        "            continue  # Skip numeric and financial data\n",
        "\n",
        "        text_samples = df[col].astype(str).head(10)\n",
        "        noun_count, total_count = 0, 0\n",
        "\n",
        "        for value in text_samples:\n",
        "            if value.replace(\".\", \"\").isdigit() or '%' in value:\n",
        "                continue\n",
        "\n",
        "            doc = nlp(value)\n",
        "            for token in doc:\n",
        "                if token.pos_ in ['PROPN', 'NOUN']:\n",
        "                    noun_count += 1\n",
        "                total_count += 1\n",
        "\n",
        "        if total_count > 0 and (noun_count / total_count) > 0.5:\n",
        "            sensitive.append(col)\n",
        "\n",
        "    print('\\n\\nSensitive cols:', sensitive)\n",
        "    return sensitive\n",
        "\n",
        "def detect_id_columns(df):\n",
        "    return [col for col in df.columns if re.search(r'\\b(id)$\\b', col, re.IGNORECASE)]\n",
        "\n",
        "def descriptive_columns(df):\n",
        "    keywords = [\"description\", \"remarks\", \"notes\", \"comments\", \"observations\", \"details\", \"summary\", \"explanation\", \"reviews\", \"feedback\", \"history\"]\n",
        "    return [col for col in df.columns if any(re.search(keyword, col, re.IGNORECASE) for keyword in keywords)]\n",
        "\n",
        "def detect_pii(text):\n",
        "    results = analyzer.analyze(text=text, entities=[\"PHONE_NUMBER\", \"CREDIT_CARD\"], language=\"en\")\n",
        "    return anonymizer.anonymize(text=text, analyzer_results=results).text if results else text\n",
        "\n",
        "def detect_pii_columns(df):\n",
        "    \"\"\"Detect columns that contain PII data (phone, credit card, ID).\"\"\"\n",
        "    pii_columns = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        sample_texts = df[col].astype(str).head(10).dropna().tolist()\n",
        "        for text in sample_texts:\n",
        "            if analyzer.analyze(text=text, entities=[\"PHONE_NUMBER\", \"CREDIT_CARD\"], language=\"en\"):\n",
        "                pii_columns.append(col)\n",
        "                break\n",
        "\n",
        "    print(\"\\n\\nPII Columns:\", pii_columns)\n",
        "    return pii_columns\n",
        "\n",
        "def anonymize_data(df):\n",
        "    sensitive = detect_noun(df)\n",
        "    id_cols = detect_id_columns(df)\n",
        "    desc = descriptive_columns(df)\n",
        "    pii_cols = detect_pii_columns(df)\n",
        "    all_sensitive_cols = set(sensitive + id_cols + pii_cols)  # Combine all detected sensitive columns\n",
        "\n",
        "    column_counters = {col: 1 for col in all_sensitive_cols}\n",
        "    column_mappings = {col: {} for col in all_sensitive_cols}\n",
        "\n",
        "    for col in all_sensitive_cols:\n",
        "        new_values = []\n",
        "        for val in df[col].astype(str):\n",
        "            if pd.notna(val):\n",
        "                if val in column_mappings[col]:\n",
        "                    anonymized_value = column_mappings[col][val]\n",
        "                else:\n",
        "                    anonymized_value = f\"{col}{column_counters[col]}\"\n",
        "                    column_mappings[col][val] = anonymized_value\n",
        "                    column_counters[col] += 1\n",
        "                mapping[anonymized_value] = val\n",
        "                new_values.append(anonymized_value)\n",
        "            else:\n",
        "                new_values.append(val)\n",
        "        df[col] = new_values\n",
        "\n",
        "    for col in desc:\n",
        "        df[col] = df[col].astype(str).apply(detect_pii)  # PII masking for descriptive columns\n",
        "\n",
        "    return df\n",
        "\n",
        "def save_file(df, file_type, output_file):\n",
        "    if file_type == \"csv\":\n",
        "        df.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        df.to_excel(output_file, index=False, sheet_name=\"Anonymized Data\")\n",
        "\n",
        "@time_it\n",
        "def anonymization(input_file):\n",
        "    df, file_type = read_file(input_file)\n",
        "    df = anonymize_data(df)\n",
        "    output_file = f\"anonymized.{file_type}\"\n",
        "    save_file(df, file_type, output_file)\n",
        "    with open(\"mappings.json\", \"w\") as f:\n",
        "        json.dump(mapping, f)\n",
        "    print(f\"✅ Anonymized file saved as {output_file}\")\n",
        "\n",
        "@time_it\n",
        "def de_anonymization(input_file, mapping_file):\n",
        "    df, file_type = read_file(input_file)\n",
        "    with open(mapping_file, \"r\") as f:\n",
        "        mapping = json.load(f)\n",
        "    df = df.applymap(lambda x: mapping.get(x, x))\n",
        "    output_file = f\"deanonymized.{file_type}\"\n",
        "    save_file(df, file_type, output_file)\n",
        "    print(f\"✅ De-anonymized file saved as {output_file}\")\n",
        "\n",
        "# uploaded = files.upload()\n",
        "# file_name = list(uploaded.keys())[0].strip().replace(' ', '-')\n",
        "# anonymization(file_name)\n",
        "anonymization(input_file = 'testing-csv-ff.csv')\n",
        "de_anonymization(file_name, \"mappings.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0kvk1-tmiMM",
        "outputId": "99c21a61-8cf3-4cb6-f9a0-d000b9bc8ef2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Sensitive cols: ['Reporting Month', 'Cluster Name', 'Input Account Name', 'Account Name']\n",
            "\n",
            "\n",
            "PII Columns: ['Mobile Number', 'Credit Card Number']\n",
            "✅ Anonymized file saved as anonymized.csv\n",
            "\n",
            "\n",
            "Execution time anonymization: 4.649121 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-38-7c67c9f901aa>:138: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: mapping.get(x, x))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ De-anonymized file saved as deanonymized.csv\n",
            "\n",
            "\n",
            "Execution time de_anonymization: 0.777546 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Masking prompt from values in mappings.json\n",
        "import json\n",
        "\n",
        "def mask_prompt(prompt, mapping_file=\"mappings.json\"):\n",
        "    \"\"\"Replaces original values in the prompt with masked values from mappings.json.\"\"\"\n",
        "    with open(mapping_file, \"r\") as f:\n",
        "        mapping = json.load(f)\n",
        "\n",
        "    reversed_mapping = {v: k for k, v in mapping.items()}  # Reverse mapping for lookup\n",
        "    for original, masked in reversed_mapping.items():\n",
        "        if original in prompt:\n",
        "            prompt = prompt.replace(original, masked)\n",
        "\n",
        "    return prompt\n",
        "\n",
        "def unmask_response(response, mapping_file=\"mappings.json\"):\n",
        "    \"\"\"Replaces masked values in the response with original values from mappings.json.\"\"\"\n",
        "    with open(mapping_file, \"r\") as f:\n",
        "        mapping = json.load(f)\n",
        "\n",
        "    for masked, original in mapping.items():\n",
        "        if masked in response:\n",
        "            response = response.replace(masked, original)\n",
        "\n",
        "    return response\n",
        "\n",
        "prompt_1 = \"Please analyze the sales performance for Gamma Inc in Mar-24. The key account manager is Alex Brown, and their contact number is 7654321098. Employee ID 35678 was responsible for managing this account. Identify any trends or anomalies.\"\n",
        "prompt_2 = \"For the North cluster, examine the transactions linked to Liam Johnson in Apr-24. His registered mobile number is 5432109876, and the associated credit card number is 4539876543210987. Provide insights on spending patterns and any unusual activities.\"\n",
        "prompt_3 = \"Check the financial records for Theta Corp in Jul-24. The primary contact is William Young, and his phone number is 7654321096. Employee ID 55667 handled this account. Assess if there were any fraudulent transactions.\"\n",
        "\n",
        "response_1 = mask_prompt(prompt_1)\n",
        "\n",
        "print(response_1)\n",
        "print(unmask_response(response_1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N01tnBJwrfjE",
        "outputId": "80779337-dd2f-49e3-c652-c1d8bfaa9194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please analyze the sales performance for Input Account Name3 in Reporting Month3. The key account manager is Account Name3, and their contact number is Mobile Number3. Employee ID Employee ID3 was responsible for managing this account. Identify any trends or anomalies.\n",
            "Please analyze the sales performance for Gamma Inc in Mar-24. The key account manager is Alex Brown, and their contact number is 7654321098. Employee ID 35678 was responsible for managing this account. Identify any trends or anomalies.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "01gv8UuN-LsJ",
        "outputId": "d0cc5a8b-6104-4bc8-e5e1-001459ee56fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading faker-37.0.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from faker) (2025.1)\n",
            "Downloading faker-37.0.2-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-37.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Implementing using Faker\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "from google.colab import files\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "\n",
        "# Initialize dependencies\n",
        "fake = Faker()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "analyzer = AnalyzerEngine()\n",
        "anonymizer = AnonymizerEngine()\n",
        "\n",
        "mapping = {}  # Stores original-to-anonymized values for de-anonymization\n",
        "\n",
        "def time_it(func):\n",
        "    \"\"\"Decorator to measure execution time.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(f'\\n\\nExecution time {func.__name__}: {end-start:.6f} seconds')\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "def read_file(file_path):\n",
        "    \"\"\"Read CSV or Excel file and return DataFrame.\"\"\"\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        return pd.read_csv(file_path, engine=\"python\"), \"csv\"\n",
        "    elif file_path.endswith(\".xlsx\"):\n",
        "        return pd.read_excel(file_path, engine=\"openpyxl\"), \"excel\"\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
        "\n",
        "def detect_noun(df):\n",
        "    \"\"\"Detect columns that contain mostly noun-based text data.\"\"\"\n",
        "    sensitive = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype in ['int64', 'float64'] or re.search(r'\\b(id|rate|amount|price|cost|value|score|percentage)\\b', col, re.IGNORECASE):\n",
        "            continue  # Skip numeric and financial data\n",
        "\n",
        "        text_samples = df[col].astype(str).head(10)\n",
        "        noun_count, total_count = 0, 0\n",
        "\n",
        "        for value in text_samples:\n",
        "            if value.replace(\".\", \"\").isdigit() or '%' in value:\n",
        "                continue\n",
        "\n",
        "            doc = nlp(value)\n",
        "            for token in doc:\n",
        "                if token.pos_ in ['PROPN', 'NOUN']:\n",
        "                    noun_count += 1\n",
        "                total_count += 1\n",
        "\n",
        "        if total_count > 0 and (noun_count / total_count) > 0.5:\n",
        "            sensitive.append(col)\n",
        "\n",
        "    print('\\n\\nSensitive cols:', sensitive)\n",
        "    return sensitive\n",
        "\n",
        "def detect_id_columns(df):\n",
        "    \"\"\"Detect columns that contain ID values.\"\"\"\n",
        "    return [col for col in df.columns if re.search(r'\\b(id)$\\b', col, re.IGNORECASE)]\n",
        "\n",
        "def descriptive_columns(df):\n",
        "    \"\"\"Detect columns that contain descriptive or free-text data.\"\"\"\n",
        "    keywords = [\"description\", \"remarks\", \"notes\", \"comments\", \"observations\", \"details\", \"summary\", \"explanation\", \"reviews\", \"feedback\", \"history\"]\n",
        "    return [col for col in df.columns if any(re.search(keyword, col, re.IGNORECASE) for keyword in keywords)]\n",
        "\n",
        "def detect_pii(text):\n",
        "    \"\"\"Detect and anonymize PII data in text fields.\"\"\"\n",
        "    results = analyzer.analyze(text=text, entities=[\"PHONE_NUMBER\", \"CREDIT_CARD\"], language=\"en\")\n",
        "    return anonymizer.anonymize(text=text, analyzer_results=results).text if results else text\n",
        "\n",
        "def detect_pii_columns(df):\n",
        "    \"\"\"Detect columns that contain PII data (phone, credit card, etc.).\"\"\"\n",
        "    pii_columns = []\n",
        "\n",
        "    for col in df.columns:\n",
        "        sample_texts = df[col].astype(str).head(10).dropna().tolist()\n",
        "        for text in sample_texts:\n",
        "            if analyzer.analyze(text=text, entities=[\"PHONE_NUMBER\", \"CREDIT_CARD\"], language=\"en\"):\n",
        "                pii_columns.append(col)\n",
        "                break\n",
        "\n",
        "    print(\"\\n\\nPII Columns:\", pii_columns)\n",
        "    return pii_columns\n",
        "\n",
        "# Mapping Faker functions to relevant column types\n",
        "faker_mapping = {\n",
        "    \"name\": fake.name,\n",
        "    \"email\": fake.email,\n",
        "    \"phone\": fake.phone_number,\n",
        "    \"address\": fake.address,\n",
        "    \"uuid\": fake.uuid4,\n",
        "    \"credit_card\": fake.credit_card_number,\n",
        "}\n",
        "\n",
        "def get_faker_value(column_name):\n",
        "    \"\"\"Return a Faker-generated value based on column type.\"\"\"\n",
        "    if re.search(r'\\b(name)\\b', column_name, re.IGNORECASE):\n",
        "        return faker_mapping[\"name\"]()\n",
        "    if re.search(r'\\b(email)\\b', column_name, re.IGNORECASE):\n",
        "        return faker_mapping[\"email\"]()\n",
        "    if re.search(r'\\b(phone|mobile|contact)\\b', column_name, re.IGNORECASE):\n",
        "        return faker_mapping[\"phone\"]()\n",
        "    if re.search(r'\\b(address|location)\\b', column_name, re.IGNORECASE):\n",
        "        return faker_mapping[\"address\"]()\n",
        "    if re.search(r'\\b(id)\\b', column_name, re.IGNORECASE):\n",
        "        return faker_mapping[\"uuid\"]()\n",
        "    if re.search(r'\\b(credit_card|card_number|credit card)\\b', column_name, re.IGNORECASE):\n",
        "        return faker_mapping[\"credit_card\"]()\n",
        "    return fake.word()  # Default fallback\n",
        "\n",
        "def anonymize_data(df):\n",
        "    \"\"\"Anonymize sensitive columns using Faker.\"\"\"\n",
        "    sensitive = detect_noun(df)\n",
        "    id_cols = detect_id_columns(df)\n",
        "    desc = descriptive_columns(df)\n",
        "    pii_cols = detect_pii_columns(df)\n",
        "    all_sensitive_cols = set(sensitive + id_cols + pii_cols)\n",
        "\n",
        "    column_mappings = {col: {} for col in all_sensitive_cols}\n",
        "\n",
        "    for col in all_sensitive_cols:\n",
        "        new_values = []\n",
        "        for val in df[col].astype(str):\n",
        "            if pd.notna(val):\n",
        "                if val in column_mappings[col]:\n",
        "                    anonymized_value = column_mappings[col][val]\n",
        "                else:\n",
        "                    anonymized_value = get_faker_value(col)\n",
        "                    column_mappings[col][val] = anonymized_value\n",
        "                mapping[anonymized_value] = val\n",
        "                new_values.append(anonymized_value)\n",
        "            else:\n",
        "                new_values.append(val)\n",
        "        df[col] = new_values\n",
        "\n",
        "    for col in desc:\n",
        "        df[col] = df[col].astype(str).apply(detect_pii)  # Mask PII in descriptive text\n",
        "\n",
        "    return df\n",
        "\n",
        "def save_file(df, file_type, output_file):\n",
        "    \"\"\"Save the DataFrame to a file.\"\"\"\n",
        "    if file_type == \"csv\":\n",
        "        df.to_csv(output_file, index=False)\n",
        "    else:\n",
        "        df.to_excel(output_file, index=False, sheet_name=\"Anonymized Data\")\n",
        "\n",
        "@time_it\n",
        "def anonymization(input_file):\n",
        "    \"\"\"Perform anonymization on a file.\"\"\"\n",
        "    df, file_type = read_file(input_file)\n",
        "    df = anonymize_data(df)\n",
        "    output_file = f\"anonymized.{file_type}\"\n",
        "    save_file(df, file_type, output_file)\n",
        "\n",
        "    # Save mapping for de-anonymization\n",
        "    with open(\"mappings.json\", \"w\") as f:\n",
        "        json.dump(mapping, f)\n",
        "\n",
        "    print(f\"✅ Anonymized file saved as {output_file}\")\n",
        "\n",
        "@time_it\n",
        "def de_anonymization(input_file, mapping_file):\n",
        "    \"\"\"Revert anonymized data to its original values using mappings.\"\"\"\n",
        "    df, file_type = read_file(input_file)\n",
        "\n",
        "    with open(mapping_file, \"r\") as f:\n",
        "        mapping = json.load(f)\n",
        "\n",
        "    df = df.applymap(lambda x: mapping.get(x, x))\n",
        "    output_file = f\"deanonymized.{file_type}\"\n",
        "    save_file(df, file_type, output_file)\n",
        "\n",
        "    print(f\"✅ De-anonymized file saved as {output_file}\")\n",
        "\n",
        "uploaded = files.upload()\n",
        "file_name = list(uploaded.keys())[0].strip().replace(' ', '-')\n",
        "anonymization(file_name)\n",
        "# anonymization(\"testing-csv-ff.csv\")\n",
        "de_anonymization(\"anonymized.csv\", \"mappings.json\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "NE_3TMPO-Aw2",
        "outputId": "7f25cc51-817a-427b-85fd-9854e7f3fb87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e1328290-0e88-4372-a63e-058990c2e74c\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e1328290-0e88-4372-a63e-058990c2e74c\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving testing-csv-ff.csv to testing-csv-ff.csv\n",
            "\n",
            "\n",
            "Sensitive cols: ['Reporting Month', 'Cluster Name', 'Input Account Name', 'Account Name']\n",
            "\n",
            "\n",
            "PII Columns: ['Mobile Number', 'Credit Card Number']\n",
            "✅ Anonymized file saved as anonymized.csv\n",
            "\n",
            "\n",
            "Execution time anonymization: 2.413634 seconds\n",
            "✅ De-anonymized file saved as deanonymized.csv\n",
            "\n",
            "\n",
            "Execution time de_anonymization: 0.007664 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-322bf1fc318e>:180: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
            "  df = df.applymap(lambda x: mapping.get(x, x))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **TESTING FAKER CODE**"
      ],
      "metadata": {
        "id": "DDOEZvZeo_jm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flair\n",
        "!pip install faker\n",
        "!pip install presidio_analyzer\n",
        "!pip install presidio_anonymizer\n",
        "# !python -m spacy download en_core_web_lg\n",
        "!pip install --upgrade --force-reinstall \"numpy==1.26.4\"\n",
        "!pip install --upgrade --force-reinstall \"Cython\" \"spacy\""
      ],
      "metadata": {
        "id": "IEZcpbeVfnm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Faker code (1)\n",
        "import pandas as pd\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from faker import Faker\n",
        "\n",
        "def time_it(func):\n",
        "    \"\"\"Decorator to measure execution time.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(f'\\n\\nExecution time {func.__name__}: {end-start:.6f} seconds')\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "@time_it\n",
        "def analyze_column(df):\n",
        "  entity_columns = {}  # Initialize as a dictionary\n",
        "  for col in df.columns:\n",
        "    unique_values = df[col].dropna().astype(str).unique()[:25]\n",
        "    entity_counts = {}\n",
        "\n",
        "    for value in unique_values:\n",
        "      results = analyzer.analyze(text=value, language=\"en\")\n",
        "      for result in results:\n",
        "        entity_counts[result.entity_type] = entity_counts.get(result.entity_type, 0) + 1\n",
        "    if entity_counts:\n",
        "      predominant_entity = max(entity_counts, key=entity_counts.get)\n",
        "      if predominant_entity not in entity_columns:\n",
        "        entity_columns[predominant_entity] = []  # Assign a list to the entity key\n",
        "      entity_columns[predominant_entity].append(col)\n",
        "  return entity_columns  # Return the dictionary\n",
        "\n",
        "def load_and_analyze(file_path):\n",
        "  if file_path.endswith(\".csv\"):\n",
        "    df = pd.read_csv(file_path, engine=\"python\")\n",
        "  elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "    df = pd.read_excel(file_path)\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported file format. Provide a CSV or Excel file.\")\n",
        "\n",
        "  df_sample = df.drop_duplicates().head()\n",
        "  classified_columns = analyze_column(df_sample)\n",
        "  return classified_columns  # Return the dictionary"
      ],
      "metadata": {
        "id": "IJZv-UcNpVoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Faker code (2)\n",
        "file=\"smaller_companies.csv\"\n",
        "classified_columns=load_and_analyze(file)\n",
        "df=pd.read_csv(file)\n",
        "print(classified_columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JwyiugG8paCC",
        "outputId": "d59c44c6-36c7-4f79-c16f-9bbeed216ceb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Execution time analyze_column: 0.468736 seconds\n",
            "{'DATE_TIME': ['id', 'size range'], 'LOCATION': ['name', 'locality', 'country'], 'URL': ['domain', 'linkedin url'], 'US_DRIVER_LICENSE': ['current employee estimate', 'total employee estimate']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Faker code (3)\n",
        "import os\n",
        "import json\n",
        "from faker import Faker\n",
        "\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "\n",
        "# data_mapping.json\n",
        "MAPPING_FILE = \"output/mappings.json\"\n",
        "forward_mapping = {}\n",
        "reverse_mapping = {}\n",
        "\n",
        "@time_it\n",
        "def load_mapping():\n",
        "    \"\"\"Loads existing mappings from a file if available.\"\"\"\n",
        "    global forward_mapping, reverse_mapping\n",
        "    if os.path.exists(MAPPING_FILE):\n",
        "        try:\n",
        "            with open(MAPPING_FILE, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "                # Convert keys back to tuples if needed\n",
        "                forward_mapping = {tuple(eval(k)): v for k, v in data.get(\"forward_mapping\", {}).items()}\n",
        "                reverse_mapping = {tuple(eval(k)): v for k, v in data.get(\"reverse_mapping\", {}).items()}\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON file: {e}\")\n",
        "            # If the file is invalid, reset the mappings\n",
        "            forward_mapping = {}\n",
        "            reverse_mapping = {}\n",
        "\n",
        "def save_mapping():\n",
        "    \"\"\"Saves the mapping to a JSON file.\"\"\"\n",
        "    os.makedirs(os.path.dirname(MAPPING_FILE), exist_ok=True)\n",
        "    # Convert tuple keys to strings before saving\n",
        "    modified_forward_mapping = {str(k): v for k, v in forward_mapping.items()}\n",
        "    modified_reverse_mapping = {str(k): v for k, v in reverse_mapping.items()}\n",
        "    with open(MAPPING_FILE, \"w\") as f:\n",
        "        json.dump({\"forward_mapping\": modified_forward_mapping, \"reverse_mapping\": modified_reverse_mapping}, f, indent=4)\n",
        "\n",
        "@time_it\n",
        "def generate_fake_value(original: str, entity_type: str) -> str:\n",
        "    \"\"\"Generates fake values while ensuring uniqueness.\"\"\"\n",
        "    if original in forward_mapping:\n",
        "        return forward_mapping[original]\n",
        "\n",
        "    faker_mapping = {\n",
        "        \"PERSON\": fake.name,\n",
        "        \"FIRST_NAME\": fake.first_name,\n",
        "        \"LAST_NAME\": fake.last_name,\n",
        "        \"EMAIL\": fake.email,\n",
        "        \"URL\": fake.url,\n",
        "        \"PHONE_NUMBER\": fake.phone_number,\n",
        "        \"CREDIT_CARD\": fake.credit_card_number,\n",
        "        \"IBAN\": fake.iban,\n",
        "        \"US_SSN\": fake.ssn,\n",
        "        # \"DATE\": fake.date_of_birth,\n",
        "        \"LOCATION\": fake.company,\n",
        "        \"STREET_ADDRESS\": fake.street_address,\n",
        "        \"CITY\": fake.city,\n",
        "        \"STATE\": fake.state,\n",
        "        \"COUNTRY\": fake.country,\n",
        "        \"ZIP_CODE\": fake.zipcode,\n",
        "        \"ORGANIZATION\": fake.company,\n",
        "        \"JOB_TITLE\": fake.job,\n",
        "        \"USERNAME\": fake.user_name,\n",
        "        \"PASSWORD\": fake.password,\n",
        "        \"IP_ADDRESS\": fake.ipv4,\n",
        "        \"MAC_ADDRESS\": fake.mac_address,\n",
        "        \"LICENSE_PLATE\": fake.license_plate,\n",
        "        \"UUID\": fake.uuid4,\n",
        "        \"BANK_ACCOUNT\": fake.bban,\n",
        "        \"TRANSACTION_ID\": fake.uuid4,\n",
        "        \"DEVICE_ID\": fake.uuid4,\n",
        "    }\n",
        "\n",
        "    fake_value = faker_mapping.get(entity_type, lambda: original)()\n",
        "    forward_mapping[original] = fake_value\n",
        "    reverse_mapping[fake_value] = original\n",
        "    return fake_value\n",
        "\n",
        "@time_it\n",
        "def mask_sensitive_data(df: pd.DataFrame, classified_columns: dict) -> pd.DataFrame:\n",
        "    \"\"\"Masks sensitive columns using Faker based on classified entity types.\"\"\"\n",
        "    for entity_type, columns in classified_columns.items():\n",
        "        for column in columns:\n",
        "            if column in df.columns:\n",
        "                df[column] = df[column].astype(str).apply(lambda x: generate_fake_value(x, entity_type) if x.strip() else x)\n",
        "    return df\n",
        "\n",
        "# testing-csv-ff.csv\n",
        "# smaller_companies.csv\n",
        "if __name__ == \"__main__\":\n",
        "    load_mapping()\n",
        "    input_file = \"smaller_companies.csv\"\n",
        "    output_file = \"output/anonymized.csv\"\n",
        "\n",
        "    df = pd.read_csv(input_file)\n",
        "    classified_columns = load_and_analyze(\"smaller_companies.csv\")\n",
        "\n",
        "    df = mask_sensitive_data(df, classified_columns)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    save_mapping()"
      ],
      "metadata": {
        "id": "1XsgZQFzpday"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hashing file so as to reuse it (FAKER)\n",
        "import os\n",
        "import json\n",
        "import pandas as pd\n",
        "import hashlib\n",
        "from faker import Faker\n",
        "\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "\n",
        "MAPPING_FILE = \"mappings.json\"  # Store mappings directly\n",
        "N_LINES_FOR_HASH = 100  # Number of lines to consider for hashing\n",
        "\n",
        "# Load existing mappings if available\n",
        "if os.path.exists(MAPPING_FILE):\n",
        "    with open(MAPPING_FILE, \"r\") as f:\n",
        "        try:\n",
        "            mappings_data = json.load(f)\n",
        "        except json.JSONDecodeError:\n",
        "            mappings_data = {}\n",
        "else:\n",
        "    mappings_data = {}\n",
        "\n",
        "def compute_file_hash(file_path, num_lines=N_LINES_FOR_HASH):\n",
        "    \"\"\"Compute a quick hash based on the first `num_lines` of the file.\"\"\"\n",
        "    hasher = hashlib.sha256()\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for _ in range(num_lines):\n",
        "            line = f.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            hasher.update(line.encode(\"utf-8\"))\n",
        "    return hasher.hexdigest()\n",
        "\n",
        "def save_mapping(filename, file_hash, forward_mapping, reverse_mapping):\n",
        "    \"\"\"Save mappings along with filename and file hash.\"\"\"\n",
        "    mappings_data[filename] = {\n",
        "        \"file_hash\": file_hash,\n",
        "        \"forward_mapping\": forward_mapping,\n",
        "        \"reverse_mapping\": reverse_mapping\n",
        "    }\n",
        "    with open(MAPPING_FILE, \"w\") as f:\n",
        "        json.dump(mappings_data, f, indent=4)\n",
        "\n",
        "def load_mapping(filename, file_hash):\n",
        "    \"\"\"Load mapping if filename and file hash match.\"\"\"\n",
        "    if filename in mappings_data and mappings_data[filename][\"file_hash\"] == file_hash:\n",
        "        return mappings_data[filename][\"forward_mapping\"], mappings_data[filename][\"reverse_mapping\"]\n",
        "    return {}, {}\n",
        "\n",
        "def generate_fake_value(original: str, entity_type: str, forward_mapping, reverse_mapping):\n",
        "    \"\"\"Generates fake values while ensuring uniqueness.\"\"\"\n",
        "    if original in forward_mapping:\n",
        "        return forward_mapping[original]\n",
        "\n",
        "    faker_mapping = {\n",
        "        \"PERSON\": fake.name,\n",
        "        \"FIRST_NAME\": fake.first_name,\n",
        "        \"LAST_NAME\": fake.last_name,\n",
        "        \"EMAIL\": fake.email,\n",
        "        \"URL\": fake.url,\n",
        "        \"PHONE_NUMBER\": fake.phone_number,\n",
        "        \"CREDIT_CARD\": fake.credit_card_number,\n",
        "        \"IBAN\": fake.iban,\n",
        "        \"US_SSN\": fake.ssn,\n",
        "        \"LOCATION\": fake.company,\n",
        "        \"STREET_ADDRESS\": fake.street_address,\n",
        "        \"CITY\": fake.city,\n",
        "        \"STATE\": fake.state,\n",
        "        \"COUNTRY\": fake.country,\n",
        "        \"ZIP_CODE\": fake.zipcode,\n",
        "        \"ORGANIZATION\": fake.company,\n",
        "        \"JOB_TITLE\": fake.job,\n",
        "        \"USERNAME\": fake.user_name,\n",
        "        \"PASSWORD\": fake.password,\n",
        "        \"IP_ADDRESS\": fake.ipv4,\n",
        "        \"MAC_ADDRESS\": fake.mac_address,\n",
        "        \"LICENSE_PLATE\": fake.license_plate,\n",
        "        \"UUID\": fake.uuid4,\n",
        "        \"BANK_ACCOUNT\": fake.bban,\n",
        "        \"TRANSACTION_ID\": fake.uuid4,\n",
        "        \"DEVICE_ID\": fake.uuid4,\n",
        "    }\n",
        "\n",
        "    fake_value = faker_mapping.get(entity_type, lambda: original)()\n",
        "    forward_mapping[original] = fake_value\n",
        "    reverse_mapping[fake_value] = original\n",
        "    return fake_value\n",
        "\n",
        "def mask_sensitive_data(df: pd.DataFrame, classified_columns: dict, forward_mapping, reverse_mapping):\n",
        "    \"\"\"Masks sensitive columns using Faker based on classified entity types.\"\"\"\n",
        "    for entity_type, columns in classified_columns.items():\n",
        "        for column in columns:\n",
        "            if column in df.columns:\n",
        "                df[column] = df[column].astype(str).apply(lambda x: generate_fake_value(x, entity_type, forward_mapping, reverse_mapping) if x.strip() else x)\n",
        "    return df\n",
        "\n",
        "# Run masking process\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"smaller_companies.csv\"\n",
        "    output_file = \"anonymized.csv\"\n",
        "\n",
        "    # Compute file hash\n",
        "    file_hash = compute_file_hash(input_file)\n",
        "\n",
        "    # Load or initialize mappings\n",
        "    forward_mapping, reverse_mapping = load_mapping(input_file, file_hash)\n",
        "\n",
        "    df = pd.read_csv(input_file)\n",
        "    classified_columns = load_and_analyze(input_file)  # Ensure this function is defined\n",
        "\n",
        "    df = mask_sensitive_data(df, classified_columns, forward_mapping, reverse_mapping)\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "    # Save updated mapping\n",
        "    save_mapping(input_file, file_hash, forward_mapping, reverse_mapping)\n"
      ],
      "metadata": {
        "id": "gmoF0kU225PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tried out finding total unique names\n",
        "from faker import Faker\n",
        "from faker.exceptions import UniquenessException\n",
        "\n",
        "fake = Faker()\n",
        "fake.unique.clear()\n",
        "unique_names = set()\n",
        "\n",
        "try:\n",
        "  while True:\n",
        "    unique_names.add(fake.unique.name())\n",
        "except UniquenessException:\n",
        "  print(f'Unique names length:', len(unique_names))"
      ],
      "metadata": {
        "id": "6Z5jkj4hCHS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Masking ID in column headers\n",
        "\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "\n",
        "# Presidio Analyzer\n",
        "analyzer = AnalyzerEngine()\n",
        "\n",
        "# Faker initalization\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "\n",
        "# mappings.json\n",
        "MAPPING_FILE = \"mappings.json\"\n",
        "forward_mapping = {}\n",
        "reverse_mapping = {}\n",
        "\n",
        "def time_it(func):\n",
        "    \"\"\"Decorator to measure execution time.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(f'\\n\\nExecution time {func.__name__}: {end-start:.6f} seconds')\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "@time_it\n",
        "def analyze_column(df):\n",
        "  entity_columns = {}  # Initialize as a dictionary\n",
        "  for col in df.columns:\n",
        "    unique_values = df[col].dropna().astype(str).unique()[:15]\n",
        "    entity_counts = {}\n",
        "\n",
        "    for value in unique_values:\n",
        "      results = analyzer.analyze(text=value, language=\"en\")\n",
        "      for result in results:\n",
        "        entity_counts[result.entity_type] = entity_counts.get(result.entity_type, 0) + 1\n",
        "    if entity_counts:\n",
        "      predominant_entity = max(entity_counts, key=entity_counts.get)\n",
        "      if predominant_entity not in entity_columns:\n",
        "        entity_columns[predominant_entity] = []  # Assign a list to the entity key\n",
        "      entity_columns[predominant_entity].append(col)\n",
        "  return entity_columns  # Return the dictionary\n",
        "\n",
        "def load_and_analyze(file_path):\n",
        "  if file_path.endswith(\".csv\"):\n",
        "    df = pd.read_csv(file_path, engine=\"python\")\n",
        "  elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "    df = pd.read_excel(file_path)\n",
        "  else:\n",
        "    raise ValueError(\"Unsupported file format. Provide a CSV or Excel file.\")\n",
        "\n",
        "  df_sample = df.drop_duplicates().head()\n",
        "  classified_columns = analyze_column(df_sample)\n",
        "  return classified_columns\n",
        "\n",
        "def load_mapping():\n",
        "    \"\"\"Loads existing mappings from a file if available.\"\"\"\n",
        "    global forward_mapping, reverse_mapping\n",
        "    if os.path.exists(MAPPING_FILE):\n",
        "        try:\n",
        "            with open(MAPPING_FILE, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "                forward_mapping = data.get(\"forward_mapping\", {})\n",
        "                reverse_mapping = data.get(\"reverse_mapping\", {})\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON file: {e}\")\n",
        "            forward_mapping = {}\n",
        "            reverse_mapping = {}\n",
        "\n",
        "def save_mapping():\n",
        "    \"\"\"Saves the mapping to a JSON file.\"\"\"\n",
        "    with open(MAPPING_FILE, \"w\") as f:\n",
        "        json.dump({\"forward_mapping\": forward_mapping, \"reverse_mapping\": reverse_mapping}, f, indent=4)\n",
        "\n",
        "def generate_fake_value(original: str, entity_type: str) -> str:\n",
        "    \"\"\"Generates fake values while ensuring uniqueness.\"\"\"\n",
        "    if original in forward_mapping:\n",
        "        return forward_mapping[original]\n",
        "\n",
        "    faker_mapping = {\n",
        "        \"PERSON\": fake.name,\n",
        "        \"FIRST_NAME\": fake.first_name,\n",
        "        \"LAST_NAME\": fake.last_name,\n",
        "        \"EMAIL\": fake.email,\n",
        "        \"URL\": fake.url,\n",
        "        \"PHONE_NUMBER\": fake.phone_number,\n",
        "        \"CREDIT_CARD\": fake.credit_card_number,\n",
        "        \"IBAN\": fake.iban,\n",
        "        \"US_SSN\": fake.ssn,\n",
        "        \"LOCATION\": fake.company,\n",
        "        \"STREET_ADDRESS\": fake.street_address,\n",
        "        \"CITY\": fake.city,\n",
        "        \"STATE\": fake.state,\n",
        "        \"COUNTRY\": fake.country,\n",
        "        \"ZIP_CODE\": fake.zipcode,\n",
        "        \"ORGANIZATION\": fake.company,\n",
        "        \"JOB_TITLE\": fake.job,\n",
        "        \"USERNAME\": fake.user_name,\n",
        "        \"PASSWORD\": fake.password,\n",
        "        \"IP_ADDRESS\": fake.ipv4,\n",
        "        \"MAC_ADDRESS\": fake.mac_address,\n",
        "        \"LICENSE_PLATE\": fake.license_plate,\n",
        "        \"UUID\": fake.uuid4,\n",
        "        \"BANK_ACCOUNT\": fake.bban,\n",
        "        \"TRANSACTION_ID\": fake.uuid4,\n",
        "        \"DEVICE_ID\": fake.uuid4,\n",
        "        \"ID\": fake.uuid4,  # Generating fake values for ID columns\n",
        "    }\n",
        "\n",
        "    fake_value = faker_mapping.get(entity_type, lambda: original)()\n",
        "    forward_mapping[original] = fake_value\n",
        "    reverse_mapping[fake_value] = original\n",
        "    return fake_value\n",
        "\n",
        "@time_it\n",
        "def mask_sensitive_data(df: pd.DataFrame, classified_columns: dict) -> pd.DataFrame:\n",
        "    \"\"\"Masks sensitive columns using Faker based on classified entity types.\"\"\"\n",
        "    for entity_type, columns in classified_columns.items():\n",
        "        for column in columns:\n",
        "            if column in df.columns:\n",
        "                df[column] = df[column].astype(str).apply(lambda x: generate_fake_value(x, entity_type) if x.strip() else x)\n",
        "\n",
        "    # Find columns with 'id' in the name using regex and mask them\n",
        "    id_columns = [col for col in df.columns if re.search(r'\\bid\\b', col, re.IGNORECASE)]\n",
        "    print('ID cols:', id_columns)\n",
        "    for col in id_columns:\n",
        "        df[col] = df[col].astype(str).apply(lambda x: generate_fake_value(x, \"ID\") if x.strip() else x)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# smaller_companies.csv\n",
        "# testing-csv-ff.csv\n",
        "if __name__ == \"__main__\":\n",
        "    load_mapping()\n",
        "    input_file = \"smaller_companies.csv\"\n",
        "    output_file = \"anonymized.csv\"\n",
        "\n",
        "    df = pd.read_csv(input_file)\n",
        "    classified_columns = load_and_analyze(input_file)\n",
        "\n",
        "    df = mask_sensitive_data(df, classified_columns)\n",
        "    df.to_csv(output_file, index=False)\n",
        "    save_mapping()\n"
      ],
      "metadata": {
        "id": "rLibUWPLPWbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing columnwise replacing using Faker\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "import json\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "\n",
        "# Initialize Faker & Presidio Analyzer\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "analyzer = AnalyzerEngine()\n",
        "\n",
        "# Mapping file for storing forward/reverse mappings\n",
        "MAPPING_FILE = \"mappings.json\"\n",
        "forward_mapping = {}\n",
        "reverse_mapping = {}\n",
        "\n",
        "def time_it(func):\n",
        "    \"\"\"Decorator to measure execution time.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(f'\\nExecution time {func.__name__}: {end-start:.6f} seconds')\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "def load_mapping():\n",
        "    \"\"\"Loads existing mappings from a file if available.\"\"\"\n",
        "    global forward_mapping, reverse_mapping\n",
        "    if os.path.exists(MAPPING_FILE):\n",
        "        try:\n",
        "            with open(MAPPING_FILE, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "                forward_mapping = data.get(\"forward_mapping\", {})\n",
        "                reverse_mapping = data.get(\"reverse_mapping\", {})\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"Error decoding JSON file: {e}\")\n",
        "            forward_mapping = {}\n",
        "            reverse_mapping = {}\n",
        "\n",
        "def save_mapping():\n",
        "    \"\"\"Saves the mapping to a JSON file.\"\"\"\n",
        "    with open(MAPPING_FILE, \"w\") as f:\n",
        "        json.dump({\"forward_mapping\": forward_mapping, \"reverse_mapping\": reverse_mapping}, f, indent=4)\n",
        "\n",
        "@time_it\n",
        "def analyze_column(df):\n",
        "    \"\"\"Detects sensitive columns using Presidio.\"\"\"\n",
        "    entity_columns = {}\n",
        "    for col in df.columns:\n",
        "        unique_values = df[col].dropna().astype(str).unique()[:10]\n",
        "        entity_counts = {}\n",
        "\n",
        "        for value in unique_values:\n",
        "            results = analyzer.analyze(text=value, language=\"en\")\n",
        "            for result in results:\n",
        "                entity_counts[result.entity_type] = entity_counts.get(result.entity_type, 0) + 1\n",
        "\n",
        "        if entity_counts:\n",
        "            predominant_entity = max(entity_counts, key=entity_counts.get)\n",
        "            if predominant_entity not in entity_columns:\n",
        "                entity_columns[predominant_entity] = []\n",
        "            entity_columns[predominant_entity].append(col)\n",
        "\n",
        "    return entity_columns\n",
        "\n",
        "def load_and_analyze(file_path):\n",
        "    \"\"\"Loads CSV/Excel and analyzes sensitive columns.\"\"\"\n",
        "    df = pd.read_csv(file_path) if file_path.endswith(\".csv\") else pd.read_excel(file_path)\n",
        "    return analyze_column(df)\n",
        "\n",
        "def generate_fake_values(df, entity_type, column):\n",
        "    \"\"\"Generates fake values for an entire column while preserving uniqueness.\"\"\"\n",
        "    faker_mapping = {\n",
        "        \"PERSON\": fake.name,\n",
        "        \"FIRST_NAME\": fake.first_name,\n",
        "        \"LAST_NAME\": fake.last_name,\n",
        "        \"EMAIL\": fake.email,\n",
        "        \"URL\": fake.url,\n",
        "        \"PHONE_NUMBER\": fake.phone_number,\n",
        "        \"CREDIT_CARD\": lambda: fake.credit_card_number(card_type=None),\n",
        "        \"IBAN\": fake.iban,\n",
        "        \"US_SSN\": fake.ssn,\n",
        "        \"LOCATION\": fake.company,\n",
        "        \"STREET_ADDRESS\": fake.street_address,\n",
        "        \"CITY\": fake.city,\n",
        "        \"STATE\": fake.state,\n",
        "        \"COUNTRY\": fake.country,\n",
        "        \"ZIP_CODE\": fake.zipcode,\n",
        "        \"ORGANIZATION\": fake.company,\n",
        "        \"JOB_TITLE\": fake.job,\n",
        "        \"USERNAME\": fake.user_name,\n",
        "        \"PASSWORD\": fake.password,\n",
        "        \"IP_ADDRESS\": fake.ipv4,\n",
        "        \"MAC_ADDRESS\": fake.mac_address,\n",
        "        \"LICENSE_PLATE\": fake.license_plate,\n",
        "        \"UUID\": fake.uuid4,\n",
        "        \"BANK_ACCOUNT\": fake.bban,\n",
        "        \"TRANSACTION_ID\": fake.uuid4,\n",
        "        \"DEVICE_ID\": fake.uuid4,\n",
        "        \"ID\": fake.uuid4,\n",
        "        \"US_BANK_NUMBER\": lambda: fake.credit_card_number(card_type=\"visa\")\n",
        "    }\n",
        "\n",
        "    if entity_type not in faker_mapping:\n",
        "        return df  # Skip if no Faker function exists\n",
        "\n",
        "    faker_func = faker_mapping[entity_type]\n",
        "\n",
        "    # Convert numeric columns to strings before applying Faker mapping\n",
        "    if df[column].dtype in [int, float]:\n",
        "        df[column] = df[column].astype(str)\n",
        "\n",
        "    original_values = df[column].dropna().astype(str).unique()\n",
        "    fake_values = [forward_mapping.get(value, faker_func()) for value in original_values]\n",
        "\n",
        "    # Store mappings\n",
        "    for original, fake_val in zip(original_values, fake_values):\n",
        "        forward_mapping[original] = str(fake_val)  # Ensure JSON serializable\n",
        "        reverse_mapping[str(fake_val)] = original\n",
        "\n",
        "    df[column] = df[column].map(forward_mapping).fillna(df[column])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def mask_id_columns(df):\n",
        "    \"\"\"Finds columns with 'id' in the name and replaces them with fake UUIDs.\"\"\"\n",
        "    id_columns = [col for col in df.columns if re.search(r'\\bid\\b', col, re.IGNORECASE)]\n",
        "\n",
        "    for column in id_columns:\n",
        "        unique_ids = df[column].astype(str).dropna().unique()\n",
        "        fake_ids = [forward_mapping.get(value, str(fake.uuid4())) for value in unique_ids]\n",
        "\n",
        "        # Store mappings for ID replacements\n",
        "        for original, fake_id in zip(unique_ids, fake_ids):\n",
        "            forward_mapping[original] = fake_id\n",
        "            reverse_mapping[fake_id] = original\n",
        "\n",
        "        # 🔹 Forcefully update df[column] with mapped values\n",
        "        df[column] = df[column].astype(str).apply(lambda x: forward_mapping.get(x, x))\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "@time_it\n",
        "def mask_sensitive_data(df, classified_columns):\n",
        "    \"\"\"Masks all detected sensitive columns using Faker.\"\"\"\n",
        "    for entity_type, columns in classified_columns.items():\n",
        "        for column in columns:\n",
        "            if column in df.columns:\n",
        "                df = generate_fake_values(df, entity_type, column)\n",
        "\n",
        "    # Mask all ID columns separately\n",
        "    df = mask_id_columns(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "# testing-csv-ff.csv\n",
        "# smaller_companies.csv\n",
        "if __name__ == \"__main__\":\n",
        "    load_mapping()\n",
        "    input_file = \"smaller_companies.csv\"\n",
        "    output_file = \"anonymized.csv\"\n",
        "\n",
        "    df = pd.read_csv(input_file)\n",
        "    classified_columns = load_and_analyze(input_file)\n",
        "\n",
        "    df = mask_sensitive_data(df, classified_columns)\n",
        "    df.to_csv(output_file, index=False)\n",
        "\n",
        "    save_mapping()\n"
      ],
      "metadata": {
        "id": "WAEvxZ-SeEYK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Thejus code fast!\n",
        "output_folder = \"output_files\"\n",
        "class MaskingModule:\n",
        "    def __init__(self, mapping_file: str = f\"{output_folder}/data_mapping.json\", seed: int = 42):\n",
        "        self.fake = Faker()\n",
        "        Faker.seed(seed)\n",
        "        self.mapping_file = mapping_file\n",
        "        self.forward_mapping = {}\n",
        "        self.reverse_mapping = {}\n",
        "        self._load_mapping_if_exists()\n",
        "        print(\"✅ MaskingModule initialized.\")\n",
        "\n",
        "    def _generate_fake_column(self, column: pd.Series, column_name: str) -> pd.Series:\n",
        "        \"\"\"Generate fake data for a whole column at once and update mappings.\"\"\"\n",
        "        column_lower = column_name.lower()\n",
        "        original_values = column.dropna().unique()\n",
        "        fake_values = []\n",
        "\n",
        "        if (\n",
        "            \"employee\" in column_lower\n",
        "            or \"company\" in column_lower\n",
        "            or \"account\" in column_lower\n",
        "        ):\n",
        "            fake_values = [\n",
        "                f\"{self.fake.first_name()}{self.fake.random_number(digits=12)}{self.fake.first_name()}\"\n",
        "                for _ in original_values\n",
        "            ]\n",
        "        elif \"project\" in column_lower:\n",
        "            fake_values = [\n",
        "                f\"PRJ-{self.fake.random_number(digits=12)}\" for _ in original_values\n",
        "            ]\n",
        "        elif \"entity\" in column_lower:\n",
        "            fake_values = [\n",
        "                f\"ENT-{self.fake.random_number(digits=12)}\" for _ in original_values\n",
        "            ]\n",
        "        elif \"program\" in column_lower:\n",
        "            fake_values = [\n",
        "                f\"PROG-{self.fake.random_number(digits=12)}\" for _ in original_values\n",
        "            ]\n",
        "        else:\n",
        "            fake_values = [\n",
        "                f\"{self.fake.first_name()}-{self.fake.random_number(digits=20)}-{self.fake.first_name()}\"\n",
        "                for _ in original_values\n",
        "            ]\n",
        "\n",
        "        mapping = dict(zip(original_values, fake_values))\n",
        "        self.forward_mapping.update(mapping)\n",
        "        self.reverse_mapping.update({v: k for k, v in mapping.items()})\n",
        "\n",
        "        return column.map(mapping).fillna(column)\n",
        "\n",
        "    def _save_mapping(self):\n",
        "        \"\"\"Save mappings to JSON with string keys and serializable values.\"\"\"\n",
        "\n",
        "        def convert_to_serializable(obj):\n",
        "            if isinstance(\n",
        "                obj, (np.integer, np.int64, np.int32)\n",
        "            ):  # Convert numpy int to Python int\n",
        "                return int(obj)\n",
        "            elif isinstance(\n",
        "                obj, (np.floating, np.float64, np.float32)\n",
        "            ):  # Convert numpy float to Python float\n",
        "                return float(obj)\n",
        "            elif isinstance(obj, np.ndarray):  # Convert numpy array to list\n",
        "                return obj.tolist()\n",
        "            else:\n",
        "                return obj  # Return as-is if already serializable\n",
        "\n",
        "        mapping_data = {\n",
        "            \"forward_mapping\": {\n",
        "                str(k): convert_to_serializable(v) for k, v in self.forward_mapping.items()\n",
        "            },\n",
        "            \"reverse_mapping\": {\n",
        "                str(k): convert_to_serializable(v) for k, v in self.reverse_mapping.items()\n",
        "            },\n",
        "            \"metadata\": {\n",
        "                \"updated_at\": datetime.now().isoformat(),\n",
        "                \"record_count\": len(self.forward_mapping),\n",
        "            },\n",
        "        }\n",
        "\n",
        "        with open(self.mapping_file, \"w\") as f:\n",
        "            json.dump(mapping_data, f, indent=4)\n",
        "\n",
        "        print(\"💾 Mapping saved successfully.\")\n",
        "\n",
        "    def _load_mapping_if_exists(self):\n",
        "        \"\"\"Load existing mappings from JSON.\"\"\"\n",
        "        if os.path.exists(self.mapping_file):\n",
        "            with open(self.mapping_file, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "                self.forward_mapping = data[\"forward_mapping\"]\n",
        "                self.reverse_mapping = data[\"reverse_mapping\"]\n",
        "            print(\"📂 Existing mapping loaded.\")\n",
        "\n",
        "    def process_file(\n",
        "        self,\n",
        "        input_file: str,\n",
        "        output_file: str,\n",
        "        mode: str = \"anonymize\",\n",
        "        include_columns: List[str] = None,\n",
        "    ):\n",
        "        \"\"\"Process CSV and Excel files for anonymization and de-anonymization.\"\"\"\n",
        "        if mode not in [\"anonymize\", \"deanonymize\"]:\n",
        "            raise ValueError(\"Mode must be either 'anonymize' or 'deanonymize'\")\n",
        "\n",
        "        file_ext = os.path.splitext(input_file)[-1].lower()\n",
        "        df = (\n",
        "            pd.read_csv(input_file) if file_ext == \".csv\" else pd.read_excel(input_file)\n",
        "        )\n",
        "        print(\"📊 Data loaded successfully.\")\n",
        "\n",
        "        include_set = {\"id\", \"name\", \"domain\", \"year\", \"founded\", \"industry\", \"size\"}\n",
        "        if include_columns:\n",
        "            include_set.update(include_columns)\n",
        "\n",
        "        for column in df.columns:\n",
        "            if column in include_set:\n",
        "                if mode == \"anonymize\":\n",
        "                    df[column] = self._generate_fake_column(df[column], column)\n",
        "                else:\n",
        "                    df[column] = (\n",
        "                        df[column]\n",
        "                        .astype(str)\n",
        "                        .map(self.reverse_mapping)\n",
        "                        .where(df[column].notna(), df[column])\n",
        "                    )\n",
        "\n",
        "        df.to_csv(output_file, index=False) if file_ext == \".csv\" else df.to_excel(\n",
        "            output_file, index=False\n",
        "        )\n",
        "        if mode == \"anonymize\":\n",
        "            self._save_mapping()\n",
        "        print(f\"✅ Data {mode}d and saved to {output_file}\")\n",
        "\n",
        "    def check_if_two_files_are_same(self, input_file: str, restored_file: str):\n",
        "        \"\"\"Check if the original and restored files match.\"\"\"\n",
        "        file_ext = os.path.splitext(input_file)[-1].lower()\n",
        "        original_df = (\n",
        "            pd.read_csv(input_file) if file_ext == \".csv\" else pd.read_excel(input_file)\n",
        "        )\n",
        "        restored_df = (\n",
        "            pd.read_csv(restored_file)\n",
        "            if file_ext == \".csv\"\n",
        "            else pd.read_excel(restored_file)\n",
        "        )\n",
        "\n",
        "        test = original_df.equals(restored_df)\n",
        "        print(f\"📊 Are files identical? {test}\")\n",
        "        if not test:\n",
        "            print(\"⚠️ Files are not identical!\")\n",
        "            print(\"🔍 Sample from original:\")\n",
        "            print(original_df.head(2))\n",
        "            print(\"🔍 Sample from restored:\")\n",
        "            print(restored_df.head(2))\n",
        "        else:\n",
        "            print(\"✅ Files match perfectly!\")\n",
        "\n",
        "    def print_mapping_sample(self, n: int = 5):\n",
        "        \"\"\"Print sample mappings.\"\"\"\n",
        "        print(\"📌 Forward Mapping (Original → Anonymized):\")\n",
        "        for k, v in list(self.forward_mapping.items())[:n]:\n",
        "            print(f\"{k} → {v}\")\n",
        "        print(\"📌 Reverse Mapping (Anonymized → Original):\")\n",
        "        for k, v in list(self.reverse_mapping.items())[:n]:\n",
        "            print(f\"{k} → {v}\")\n",
        "\n",
        "    def mask_text(self, text, replace_dict):\n",
        "        \"\"\"Mask text using forward mapping.\"\"\"\n",
        "        for key, value in replace_dict.items():\n",
        "            text = text.replace(key, value)\n",
        "        return text\n",
        "\n"
      ],
      "metadata": {
        "id": "buRYOpVj6UDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optimizing Thejus's logic\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "\n",
        "# Initialize Faker & Presidio Analyzer\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "analyzer = AnalyzerEngine()\n",
        "\n",
        "# Mapping file for storing forward/reverse mappings\n",
        "MAPPING_FILE = \"mappings.json\"\n",
        "forward_mapping = {}\n",
        "reverse_mapping = {}\n",
        "\n",
        "NUMERIC_PATTERN = re.compile(r\"^[<>]?[\\d,.%]+$\")\n",
        "SENSITIVE_HEADERS = {\"phone\", \"mobile\", \"credit\", \"card\", \"id\"}\n",
        "EXCLUDED_ENTITIES = {\"DATE_TIME\"}  # Exclude unwanted entities\n",
        "\n",
        "\n",
        "def time_it(func):\n",
        "    \"\"\"Decorator to measure execution time of functions.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(f'\\n⏳ Execution time {func.__name__}: {end-start:.6f} seconds')\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "\n",
        "def load_mapping():\n",
        "    \"\"\"Loads existing mapping from file, ensuring proper data handling.\"\"\"\n",
        "    global forward_mapping, reverse_mapping\n",
        "    if os.path.exists(MAPPING_FILE):\n",
        "        try:\n",
        "            with open(MAPPING_FILE, \"r\") as f:\n",
        "                data = json.load(f)\n",
        "                forward_mapping = data.get(\"forward_mapping\", {})\n",
        "                reverse_mapping = data.get(\"reverse_mapping\", {})\n",
        "            print(\"📂 Existing mapping loaded.\")\n",
        "        except json.JSONDecodeError as e:\n",
        "            print(f\"⚠️ Error decoding JSON file: {e}\")\n",
        "\n",
        "\n",
        "def save_mapping():\n",
        "    \"\"\"Saves the mapping to a JSON file with string keys.\"\"\"\n",
        "    mapping_data = {\n",
        "        \"forward_mapping\": {str(k): str(v) for k, v in forward_mapping.items()},\n",
        "        \"reverse_mapping\": {str(k): str(v) for k, v in reverse_mapping.items()},\n",
        "        \"metadata\": {\n",
        "            \"updated_at\": datetime.now().isoformat(),\n",
        "            \"record_count\": len(forward_mapping),\n",
        "        },\n",
        "    }\n",
        "    with open(MAPPING_FILE, \"w\") as f:\n",
        "        json.dump(mapping_data, f, indent=4)\n",
        "    print(\"💾 Mapping saved successfully.\")\n",
        "\n",
        "\n",
        "\n",
        "@time_it\n",
        "def analyze_column(df):\n",
        "    \"\"\"Uses Presidio to classify columns based on detected entity types.\"\"\"\n",
        "    entity_columns = {}\n",
        "    for col in df.columns:\n",
        "        col_lower = col.lower()\n",
        "        if any(keyword in col_lower for keyword in SENSITIVE_HEADERS):\n",
        "            entity_columns.setdefault(\"SENSITIVE\", []).append(col)\n",
        "            continue\n",
        "\n",
        "        unique_values = df[col].dropna().astype(str).unique()[:10]\n",
        "        entity_counts = {}\n",
        "\n",
        "        for value in unique_values:\n",
        "            if NUMERIC_PATTERN.match(value):\n",
        "                continue  # Skip purely numeric values unless it's a phone/credit card\n",
        "\n",
        "            results = analyzer.analyze(text=value, language=\"en\")\n",
        "            for result in results:\n",
        "                if result.entity_type in EXCLUDED_ENTITIES:\n",
        "                    continue  # Skip unwanted entities\n",
        "                entity_counts[result.entity_type] = entity_counts.get(result.entity_type, 0) + 1\n",
        "\n",
        "        if entity_counts:\n",
        "            predominant_entity = max(entity_counts, key=entity_counts.get)\n",
        "            if predominant_entity not in entity_columns:\n",
        "                entity_columns[predominant_entity] = []\n",
        "            entity_columns[predominant_entity].append(col)\n",
        "\n",
        "    return entity_columns\n",
        "\n",
        "\n",
        "def generate_fake_values(column):\n",
        "    \"\"\"Generates consistent fake values for a column while preserving uniqueness.\"\"\"\n",
        "    col_name = column.name.lower()\n",
        "    original_values = column.dropna().unique()\n",
        "\n",
        "    if col_name in forward_mapping:\n",
        "        mapping = forward_mapping[col_name]\n",
        "    else:\n",
        "        mapping = {}\n",
        "\n",
        "    fake_values = []\n",
        "    if re.search(r\"\\b(phone|mobile)\\b\", col_name, re.IGNORECASE):\n",
        "        fake_values = [fake.phone_number() for _ in original_values]\n",
        "    elif re.search(r\"\\b(credit|card)\\b\", col_name, re.IGNORECASE):\n",
        "        fake_values = [fake.credit_card_number() for _ in original_values]\n",
        "    elif re.search(r\"\\b(id|identifier)\\b\", col_name, re.IGNORECASE):\n",
        "        fake_values = [fake.bothify(text=\"ID##########????\") for _ in original_values]\n",
        "    elif re.search(r\"\\b(name|full[_\\s]?name|first[_\\s]?name|last[_\\s]?name)\\b\", col_name, re.IGNORECASE):\n",
        "        fake_values = [f'{fake.name()--{fake.bothify(text=\"??????????\")}}' for _ in original_values]\n",
        "    else:\n",
        "        fake_values = [fake.bothify(text=\"????########\") for _ in original_values]\n",
        "\n",
        "\n",
        "    mapping.update(dict(zip(original_values, fake_values)))\n",
        "    forward_mapping[col_name] = mapping\n",
        "    reverse_mapping[col_name] = {v: k for k, v in mapping.items()}\n",
        "\n",
        "    return column.map(mapping).fillna(column)\n",
        "\n",
        "\n",
        "@time_it\n",
        "def mask_columnwise(df):\n",
        "    \"\"\"Replaces sensitive data only in columns detected as sensitive by Presidio.\"\"\"\n",
        "    sensitive_columns = analyze_column(df)\n",
        "\n",
        "    for entity, cols in sensitive_columns.items():\n",
        "        for column in cols:\n",
        "            print(f\"🔒 Masking sensitive column: {column} (Detected as {entity})\")\n",
        "            df[column] = generate_fake_values(df[column])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def restore_data(df):\n",
        "    \"\"\"Restores original values using stored mappings.\"\"\"\n",
        "    for column in df.columns:\n",
        "        col_name = column.lower()\n",
        "        if col_name in reverse_mapping:\n",
        "            df[column] = df[column].astype(str).map(reverse_mapping[col_name]).fillna(df[column])\n",
        "    return df\n",
        "\n",
        "\n",
        "def check_if_files_match(original_file, restored_file):\n",
        "    \"\"\"Checks if the original and restored files match.\"\"\"\n",
        "    file_ext = os.path.splitext(original_file)[-1].lower()\n",
        "    original_df = pd.read_csv(original_file) if file_ext == \".csv\" else pd.read_excel(original_file)\n",
        "    restored_df = pd.read_csv(restored_file) if file_ext == \".csv\" else pd.read_excel(restored_file)\n",
        "    test = original_df.equals(restored_df)\n",
        "    print(f\"📊 Are files identical? {test}\")\n",
        "    if not test:\n",
        "        print(\"⚠️ Files are not identical!\")\n",
        "    else:\n",
        "        print(\"✅ Files match perfectly!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    load_mapping()\n",
        "    uploaded = files.upload()\n",
        "    input_file = list(uploaded.keys())[0].strip().replace(' ', '-')\n",
        "    # input_file = \"smaller_companies.csv\" if not input_file else input_file\n",
        "\n",
        "    anonymized_file = \"anonymized.csv\"\n",
        "    restored_file = \"restored.csv\"\n",
        "\n",
        "    df = pd.read_csv(input_file)\n",
        "    df = mask_columnwise(df)  # Only masks detected sensitive columns\n",
        "    df.to_csv(anonymized_file, index=False)\n",
        "    save_mapping()\n",
        "\n",
        "    df = restore_data(pd.read_csv(anonymized_file))\n",
        "    df.to_csv(restored_file, index=False)\n",
        "\n",
        "    check_if_files_match(input_file, restored_file)\n"
      ],
      "metadata": {
        "id": "0G10tyBm7ISe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# V2\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "from datetime import datetime\n",
        "from google.colab import files\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "# Initialize Faker & Presidio Analyzer\n",
        "fake = Faker()\n",
        "Faker.seed(42)\n",
        "analyzer = AnalyzerEngine()\n",
        "\n",
        "# Mapping file for storing forward/reverse mappings\n",
        "MAPPING_FILE = \"mappings.pkl\"\n",
        "forward_mapping = {}\n",
        "reverse_mapping = {}\n",
        "\n",
        "NUMERIC_PATTERN = re.compile(r\"^[<>]?[\\d,.%]+$\")\n",
        "SENSITIVE_HEADERS = {\"phone\", \"mobile\", \"credit\", \"card\", \"id\", \"name\"}\n",
        "EXCLUDED_ENTITIES = {\"DATE_TIME\"}\n",
        "\n",
        "def time_it(func):\n",
        "    \"\"\"Decorator to measure execution time of functions.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(f'\\n⏳ Execution time {func.__name__}: {end-start:.6f} seconds')\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "def de_anonymize_paragraph(text):\n",
        "  for category,mapping in reverse_mapping.items():\n",
        "    for fake_value,original_value in mapping.items():\n",
        "      if fake_value in text:\n",
        "        text=text.replace(fake_value,original_value)\n",
        "  return text\n",
        "\n",
        "def load_merged_pickle():\n",
        "    \"\"\"Loads fake data from fake_dataset.pkl\"\"\"\n",
        "    merged_file = \"fake_dataset.pkl\"\n",
        "\n",
        "    if os.path.exists(merged_file):\n",
        "        try:\n",
        "            with open(merged_file, \"rb\") as f:\n",
        "                return pickle.load(f)\n",
        "        except (pickle.UnpicklingError, EOFError) as e:\n",
        "            print(f\"⚠️ Error loading fake_dataset.pkl: {e}\")\n",
        "\n",
        "    return {}\n",
        "\n",
        "def load_mapping():\n",
        "    \"\"\"Loads existing mapping from Pickle file, ensuring proper data handling.\"\"\"\n",
        "    global forward_mapping, reverse_mapping\n",
        "    if os.path.exists(MAPPING_FILE):\n",
        "        try:\n",
        "            with open(MAPPING_FILE, \"rb\") as f:\n",
        "                data = pickle.load(f)\n",
        "                forward_mapping = data.get(\"forward_mapping\", {})\n",
        "                reverse_mapping = data.get(\"reverse_mapping\", {})\n",
        "            print(\"📂 Existing mapping loaded.\")\n",
        "        except (pickle.UnpicklingError, EOFError) as e:\n",
        "            print(f\"⚠️ Error loading Pickle file: {e}\")\n",
        "\n",
        "def save_mapping(filename):\n",
        "    \"\"\"Saves the mapping to a Pickle file with filename tracking.\"\"\"\n",
        "    mapping_data = {\n",
        "        \"file_name\": filename,\n",
        "        \"forward_mapping\": forward_mapping,\n",
        "        \"reverse_mapping\": reverse_mapping,\n",
        "        \"metadata\": {\n",
        "            \"updated_at\": datetime.now().isoformat(),\n",
        "            \"record_count\": len(forward_mapping),\n",
        "        },\n",
        "    }\n",
        "    with open(MAPPING_FILE, \"wb\") as f:\n",
        "        pickle.dump(mapping_data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    print(\"💾 Mapping saved successfully.\")\n",
        "\n",
        "def load_and_analyze(file_path):\n",
        "    \"\"\"Loads the file (CSV or Excel) and returns a DataFrame.\"\"\"\n",
        "    if file_path.endswith(\".csv\"):\n",
        "        return pd.read_csv(file_path, engine=\"python\")\n",
        "    elif file_path.endswith((\".xls\", \".xlsx\")):\n",
        "        return pd.read_excel(file_path)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Provide a CSV or Excel file.\")\n",
        "\n",
        "@time_it\n",
        "def check_existing_mappings(input_file):\n",
        "    \"\"\"Checks if existing mappings match the input file and regenerates anonymized data if needed.\"\"\"\n",
        "    if os.path.exists(MAPPING_FILE):\n",
        "        with open(MAPPING_FILE, \"r\") as f:\n",
        "            data = json.load(f)\n",
        "            stored_filename = data.get(\"file_name\")\n",
        "\n",
        "        if stored_filename and stored_filename == input_file:\n",
        "            if not os.path.exists(\"anonymized.csv\"):\n",
        "                print(\"📌 Regenerating anonymized.csv from existing mappings...\")\n",
        "                df = load_and_analyze(input_file)\n",
        "                df = apply_existing_mappings(df)\n",
        "                write_file(df, \"anonymized.csv\")\n",
        "                return True\n",
        "        else:\n",
        "            print(\"⚠️ Input file does not match stored filename. Overwriting mapping...\")\n",
        "    return False\n",
        "\n",
        "def write_file(df, file_path):\n",
        "    \"\"\"Writes DataFrame to CSV or Excel.\"\"\"\n",
        "    file_ext = os.path.splitext(file_path)[-1].lower()\n",
        "    if file_ext == \".csv\":\n",
        "        df.to_csv(file_path, index=False)\n",
        "    elif file_ext in [\".xls\", \".xlsx\"]:\n",
        "        df.to_excel(file_path, index=False)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file format. Please use CSV or Excel.\")\n",
        "\n",
        "@time_it\n",
        "def analyze_column(df):\n",
        "    \"\"\"Uses Presidio to classify columns based on detected entity types.\"\"\"\n",
        "    entity_columns = {}\n",
        "    for col in df.columns:\n",
        "        col_lower = col.lower()\n",
        "        if any(keyword in col_lower for keyword in SENSITIVE_HEADERS):\n",
        "            entity_columns.setdefault(\"SENSITIVE\", []).append(col)\n",
        "            continue\n",
        "\n",
        "        unique_values = df[col].dropna().astype(str).unique()[:10]\n",
        "        entity_counts = {}\n",
        "\n",
        "        for value in unique_values:\n",
        "            if NUMERIC_PATTERN.match(value):\n",
        "                continue  # Skip purely numeric values unless it's a phone/credit card\n",
        "\n",
        "            results = analyzer.analyze(text=value, language=\"en\")\n",
        "            for result in results:\n",
        "                if result.entity_type in EXCLUDED_ENTITIES:\n",
        "                    continue  # Skip unwanted entities\n",
        "                entity_counts[result.entity_type] = entity_counts.get(result.entity_type, 0) + 1\n",
        "\n",
        "        if entity_counts:\n",
        "            predominant_entity = max(entity_counts, key=entity_counts.get)\n",
        "            if predominant_entity not in entity_columns:\n",
        "                entity_columns[predominant_entity] = []\n",
        "            entity_columns[predominant_entity].append(col)\n",
        "\n",
        "    return entity_columns\n",
        "\n",
        "def generate_fake_values(column):\n",
        "    \"\"\"Generates fake values while preserving uniqueness, using bothify and merged.json data.\"\"\"\n",
        "    col_name = column.name.lower()\n",
        "    original_values = column.dropna().unique()\n",
        "\n",
        "    match_key = None\n",
        "    for key in merged_data.keys():\n",
        "        if re.search(fr\"\\b{key}\\b\", col_name, re.IGNORECASE):\n",
        "            match_key = key\n",
        "            break\n",
        "\n",
        "    if column.dtype.kind in 'biufc':  # Skip numeric types except IDs\n",
        "        if re.search(r'\\bid\\b', col_name, re.IGNORECASE):\n",
        "            fake_values = [f\"ID-{fake.bothify(text='???##########')}\" for _ in original_values]\n",
        "            mapping = dict(zip(original_values, fake_values))\n",
        "            forward_mapping[col_name] = mapping\n",
        "            reverse_mapping[col_name] = {v: k for k, v in mapping.items()}\n",
        "            return column.map(mapping).fillna(column)\n",
        "\n",
        "        return column\n",
        "\n",
        "    fake_values = []\n",
        "\n",
        "    # ✅ Handle different column types\n",
        "    if re.search(r'\\b(phone|mobile)\\b', col_name, re.IGNORECASE):\n",
        "        fake_values = [fake.bothify(text='+## ##########') for _ in original_values]\n",
        "    elif re.search(r'\\b(card|credit)\\b', col_name, re.IGNORECASE):\n",
        "        fake_values = [fake.bothify(text='####-####-####-####') for _ in original_values]\n",
        "    elif re.search(r'\\b(domain|url|link)\\b', col_name, re.IGNORECASE):\n",
        "        fake_values = [\n",
        "            f\"www.{fake.bothify(text='?????')}/{fake.bothify(text='##??#?#?#?#?###')}/{fake.bothify(text='####')}.com\"\n",
        "            for _ in original_values\n",
        "        ]\n",
        "    elif re.search(r'\\bname\\b', col_name, re.IGNORECASE) and match_key:\n",
        "        fake_values_list = merged_data[match_key]\n",
        "        fake_values = [\n",
        "            f\"{name.split()[0]} {name.split()[-1]} {fake.bothify(text='????')}\" if ' ' in name else f\"{name} {fake.bothify(text='????')}\"\n",
        "            for name in fake_values_list[:len(original_values)]\n",
        "        ]\n",
        "    elif match_key:\n",
        "        fake_values_list = merged_data[match_key]\n",
        "        fake_values = [\n",
        "            fake_values_list[i] if i < len(fake_values_list) else fake.bothify(text='??????????')\n",
        "            for i in range(len(original_values))\n",
        "        ]\n",
        "    else: fake_values = [fake.bothify(text=\"?????##########\") for _ in original_values]\n",
        "\n",
        "    mapping = dict(zip(original_values, fake_values))\n",
        "    forward_mapping[col_name] = mapping\n",
        "    reverse_mapping[col_name] = {v: k for k, v in mapping.items()}\n",
        "\n",
        "    return column.map(mapping).fillna(column)\n",
        "\n",
        "@time_it\n",
        "def mask_columnwise(df):\n",
        "    \"\"\"Replaces sensitive data only in columns detected as sensitive by Presidio.\"\"\"\n",
        "    sensitive_columns = analyze_column(df)\n",
        "\n",
        "    for entity, cols in sensitive_columns.items():\n",
        "        for column in cols:\n",
        "            print(f\"🔒 Masking sensitive column: {column} (Detected as {entity})\")\n",
        "            df[column] = generate_fake_values(df[column])\n",
        "\n",
        "    return df\n",
        "\n",
        "def apply_existing_mappings(df):\n",
        "    \"\"\"Applies stored mappings to re-anonymize a DataFrame if mappings.json exists.\"\"\"\n",
        "    for column in df.columns:\n",
        "        col_name = column.lower()\n",
        "        if col_name in forward_mapping:\n",
        "            df[column] = df[column].astype(str).map(forward_mapping[col_name]).fillna(df[column])\n",
        "    return df\n",
        "\n",
        "\n",
        "def restore_data(df):\n",
        "    \"\"\"Restores original values using stored mappings.\"\"\"\n",
        "    for column in df.columns:\n",
        "        col_name = column.lower()\n",
        "        if col_name in reverse_mapping:\n",
        "            df[column] = df[column].astype(str).map(reverse_mapping[col_name]).fillna(df[column])\n",
        "    return df\n",
        "\n",
        "\n",
        "def check_if_files_match(original_file, restored_file):\n",
        "    \"\"\"Checks if the original and restored files match.\"\"\"\n",
        "    file_ext = os.path.splitext(original_file)[-1].lower()\n",
        "    original_df = pd.read_csv(original_file) if file_ext == \".csv\" else pd.read_excel(original_file)\n",
        "    restored_df = pd.read_csv(restored_file) if file_ext == \".csv\" else pd.read_excel(restored_file)\n",
        "\n",
        "    test = original_df.equals(restored_df)\n",
        "    print(f\"📊 Are files identical? {test}\")\n",
        "    if not test:\n",
        "        print(\"⚠️ Files are not identical!\")\n",
        "    else:\n",
        "        print(\"✅ Files match perfectly!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    merged_data = load_merged_pickle()\n",
        "    load_mapping()\n",
        "    # uploaded = files.upload()\n",
        "    # input_file = list(uploaded.keys())[0].strip().replace(' ', '-')\n",
        "    input_file = 'smaller_100k_companies.csv'\n",
        "\n",
        "    if not check_existing_mappings(input_file):\n",
        "        df = load_and_analyze(input_file)\n",
        "        df = mask_columnwise(df)\n",
        "        write_file(df, \"anonymized.csv\")\n",
        "        save_mapping(input_file)\n",
        "\n",
        "    restored_df = restore_data(load_and_analyze(\"anonymized.csv\"))\n",
        "    write_file(restored_df, \"restored.csv\")\n",
        "    check_if_files_match(input_file, \"restored.csv\")\n",
        "\n",
        "    print(\"✅ Processing complete!\")"
      ],
      "metadata": {
        "id": "8_CffsGBAsWE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Converted list of dictionary of lists into dictionary\n",
        "# Converting json to pickle\n",
        "import os\n",
        "import json\n",
        "import pickle\n",
        "\n",
        "def convert_merged_json(input_file=\"merged.json\", output_file=\"converted_merged.json\"):\n",
        "    \"\"\"Converts a list of dictionaries into a single dictionary and saves it.\"\"\"\n",
        "    if not os.path.exists(input_file):\n",
        "        print(f\"⚠️ File {input_file} not found.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        if isinstance(data, list):\n",
        "            merged_dict = {}\n",
        "            for entry in data:\n",
        "                if isinstance(entry, dict):\n",
        "                    merged_dict.update(entry)\n",
        "\n",
        "            with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(merged_dict, f, indent=4)\n",
        "\n",
        "            print(f\"✅ Successfully converted and saved as {output_file}\")\n",
        "        else:\n",
        "            print(\"⚠️ The input JSON is already in the correct format.\")\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"❌ Error parsing JSON: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Unexpected error: {e}\")\n",
        "\n",
        "def converting_json_to_pickle(input_file=\"fake_dataset.json\", output_file=\"fake_dataset.pkl\"):\n",
        "  with open(input_file, 'r') as fp:\n",
        "    data = json.load(fp)\n",
        "\n",
        "  with open(output_file, 'wb') as fp:\n",
        "    pickle.dump(data, fp)\n",
        "\n",
        "# converting_json_to_pickle()"
      ],
      "metadata": {
        "id": "zC8xAa1DXC01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!pip install faker\n",
        "!pip install presidio_analyzer\n",
        "!python -m spacy download en_core_web_lg\n",
        "!pip install --upgrade --force-reinstall \"Cython\" \"spacy\""
      ],
      "metadata": {
        "id": "aEpVkesIulix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing - classify orgainization Main Code\n",
        "import re\n",
        "import os\n",
        "import gzip\n",
        "import time\n",
        "import json\n",
        "import spacy\n",
        "import string\n",
        "import random\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "from collections import defaultdict, deque\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "\n",
        "# Variable defined\n",
        "ID = {}\n",
        "fake_data={}\n",
        "used_urls = set()\n",
        "\n",
        "url_extensions = [\n",
        "    \".com\", \".net\", \".org\", \".edu\", \".gov\", \".co\", \".us\", \".uk\", \".in\", \".ru\",\n",
        "    \".jp\", \".cn\", \".de\", \".fr\", \".it\", \".nl\", \".es\", \".br\", \".au\", \".ca\",\n",
        "    \".ch\", \".se\", \".no\", \".za\", \".mx\", \".ar\", \".be\", \".kr\", \".pl\", \".tr\",\n",
        "    \".ua\", \".ir\", \".sa\", \".ae\", \".my\", \".sg\", \".hk\", \".tw\", \".nz\", \".id\",\n",
        "    \".th\", \".ph\", \".vn\", \".bd\", \".lk\", \".np\", \".pk\", \".cz\", \".gr\", \".hu\",\n",
        "    \".fi\", \".dk\", \".il\", \".ie\", \".pt\", \".sk\", \".si\", \".ro\", \".bg\", \".rs\",\n",
        "    \".lt\", \".lv\", \".ee\", \".hr\", \".ba\", \".md\", \".ge\", \".kz\", \".by\", \".tm\",\n",
        "    \".uz\", \".af\", \".qa\", \".om\", \".kw\", \".bh\", \".ye\", \".jo\", \".lb\", \".sy\",\n",
        "    \".iq\", \".ps\", \".az\", \".am\", \".kg\", \".mn\", \".bt\", \".mv\", \".mm\", \".kh\",\n",
        "    \".la\", \".tl\", \".sb\", \".fj\", \".pg\", \".to\", \".tv\", \".ws\", \".fm\", \".ki\"\n",
        "]\n",
        "with gzip.open(\"faker_data_v2.json.gz\", \"rt\", encoding = 'utf-8') as f: fake_data_list = json.load(f)\n",
        "# Objects\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "fake=Faker()\n",
        "analyzer = AnalyzerEngine()\n",
        "\n",
        "for data in fake_data_list:\n",
        "    for key,value in data.items(): fake_data[key]=deque(value)\n",
        "domain_pool = list(fake_data.get('url', deque()))\n",
        "\n",
        "entity_mapping={\n",
        "    'names':'PERSON',\n",
        "    'emails':'EMAIL_ADDRESS',\n",
        "    'phone':'PHONE_NUMBER',\n",
        "    'location':'LOCATION',\n",
        "    'credit':'CREDIT_CARD',\n",
        "    'url':'URL',\n",
        "    'country':'COUNTRY',\n",
        "    'company':\"ORG\",\n",
        "    'id':'ID',\n",
        "}\n",
        "\n",
        "mapping_file=\"mapping.json\"\n",
        "forward_mapping=defaultdict(dict)\n",
        "reverse_mapping=defaultdict(dict)\n",
        "\n",
        "if os.path.exists(mapping_file):\n",
        "    with open(mapping_file, \"r\") as f:\n",
        "        mapping_data = json.load(f)\n",
        "        forward_mapping.update(mapping_data.get(\"forward_mapping\", {}))\n",
        "        reverse_mapping.update(mapping_data.get(\"reverse_mapping\", {}))\n",
        "def time_it(func):\n",
        "    \"\"\"Decorator to measure execution time of functions.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(f'\\n⏳ Execution time {func.__name__}: {end-start:.6f} seconds')\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "@time_it\n",
        "def analyze_column(df):\n",
        "    entity_columns = {}\n",
        "\n",
        "    # Step 1: Use Presidio to analyze all columns\n",
        "    for col in df.columns:\n",
        "        if 'id' in col.lower():\n",
        "            entity_columns[col] = 'ID'\n",
        "        elif 'country' in col.lower():\n",
        "            entity_columns[col] = 'COUNTRY'\n",
        "        else:\n",
        "            unique_values = df[col].dropna().astype(str).unique()[:25]\n",
        "            entity_counts = {}\n",
        "\n",
        "            for value in unique_values:\n",
        "                results = analyzer.analyze(text=value, language='en')\n",
        "                for result in results:\n",
        "                    entity_counts[result.entity_type] = entity_counts.get(result.entity_type, 0) + 1\n",
        "\n",
        "            if entity_counts:\n",
        "                predominant_entity = max(entity_counts, key=entity_counts.get)\n",
        "                entity_columns[col] = predominant_entity\n",
        "                if predominant_entity==\"LOCATION\":\n",
        "                  org_count=0\n",
        "                  for value in unique_values:\n",
        "                    doc=nlp(value)\n",
        "                    for ent in doc.ents:\n",
        "                      if ent.label_==\"ORG\":\n",
        "                        org_count+=1\n",
        "                  if org_count>5:\n",
        "                    predominant_entity=\"ORG\"\n",
        "                entity_columns[col]=predominant_entity\n",
        "\n",
        "    # Step 2: Use SpaCy to analyze non-numeric and unclassified columns\n",
        "    for col in df.select_dtypes(exclude=['number']).columns:\n",
        "        if col not in entity_columns:\n",
        "            unique_values = df[col].dropna().astype(str).unique()[:25]\n",
        "            org_count = 0\n",
        "\n",
        "            for value in unique_values:\n",
        "                doc = nlp(value)\n",
        "                for ent in doc.ents:\n",
        "                    if ent.label_ == 'ORG':\n",
        "                        org_count += 1\n",
        "\n",
        "            # If more than half of the sample values are ORG, classify as ORG\n",
        "            if org_count > 12:\n",
        "                entity_columns[col] = 'ORG'\n",
        "\n",
        "    return entity_columns\n",
        "\n",
        "\n",
        "def modify_fake_value(category, base_fake_value, counter):\n",
        "    if category == \"names\": return f\"{base_fake_value} {string.ascii_uppercase[counter % 26]}.\"\n",
        "    elif category == \"emails\":\n",
        "        name, domain = base_fake_value.split(\"@\")\n",
        "        return f\"{name}{counter}@{domain}\"\n",
        "    elif category in {\"location\", \"country\"}: return f\"{base_fake_value}, District {counter % 100_000_000 + 1}\"\n",
        "    elif category == \"url\":\n",
        "        # Check if the URL already exists, if so, append a country extension\n",
        "        fake_value = base_fake_value\n",
        "        while fake_value in used_urls:\n",
        "            ext = random.choice(url_extensions)\n",
        "            if not fake_value.endswith(ext):\n",
        "                fake_value += ext\n",
        "        used_urls.add(fake_value)\n",
        "        return fake_value\n",
        "    elif category == \"phone\": return f\"{base_fake_value[:-2]}{counter % 100:02d}\"\n",
        "    elif category == \"company\": return f\"{base_fake_value} Group {counter % 100_000_000 + 1}\"\n",
        "    elif category == \"credit\": return f\"{base_fake_value[:-4]}{counter % 10000:04d}\"\n",
        "    else: return f\"{base_fake_value}-{counter}\"\n",
        "\n",
        "\n",
        "def get_fake_value(category, original_value):\n",
        "    global ID\n",
        "    fake_value = None\n",
        "    original_value = original_value.strip()\n",
        "    if original_value in forward_mapping[category]: return forward_mapping[category][original_value]\n",
        "\n",
        "    # Special case for ID\n",
        "    if category == 'id':\n",
        "        length = 6\n",
        "        while True:\n",
        "            fake_value = fake.bothify(text=f'ID-{\"#\"*length}')\n",
        "            if fake_value not in ID:\n",
        "                ID[fake_value] = True\n",
        "                break\n",
        "            if len(ID) >= 10 ** length: length += 1\n",
        "    elif category == 'url':\n",
        "        domain1, domain2 = random.sample(domain_pool, 2)\n",
        "        base_fake_value = f\"https://{domain1.lower()}/{domain2.lower()}.co\"\n",
        "\n",
        "        if base_fake_value not in reverse_mapping[\"url\"]:\n",
        "            fake_value = base_fake_value\n",
        "        else:\n",
        "            counter = len(reverse_mapping[\"url\"])\n",
        "            fake_value = modify_fake_value(\"url\", base_fake_value, counter)\n",
        "\n",
        "    elif fake_data.get(category):\n",
        "      for _ in range(len(fake_data[category])):\n",
        "          candidate = fake_data[category].popleft()\n",
        "          fake_data[category].append(candidate)  # Reinsert at end (rotation)\n",
        "          if candidate not in reverse_mapping[category]:\n",
        "              fake_value = candidate\n",
        "              break\n",
        "      else:\n",
        "          counter = len(reverse_mapping[category])\n",
        "          base_fake_value = random.choice(list(reverse_mapping[category])) if reverse_mapping[category] else f\"{category}_\"\n",
        "          fake_value = modify_fake_value(category, base_fake_value, counter)\n",
        "\n",
        "    if not fake_value or fake_value in reverse_mapping[category]:\n",
        "        counter = len(reverse_mapping[category])\n",
        "        base = fake_value if fake_value else f\"{category}_\"\n",
        "        fake_value = modify_fake_value(category, base, counter)\n",
        "\n",
        "    # Store forward & reverse mappings\n",
        "    forward_mapping[category][original_value] = fake_value\n",
        "    reverse_mapping[category][fake_value] = original_value\n",
        "\n",
        "    return fake_value\n",
        "\n",
        "\n",
        "@time_it\n",
        "def mask_dataframe(df):\n",
        "    for col, entity in entity_columns.items():\n",
        "        matching_keys = [key for key, value in entity_mapping.items() if value == entity]\n",
        "        if matching_keys:\n",
        "            df[col] = df[col].astype(str).apply(lambda x: get_fake_value(matching_keys[0], str(x)) if x else str(x))\n",
        "    return df\n",
        "\n",
        "def restore_original_value(category, fake_value):\n",
        "    return reverse_mapping[category].get(fake_value, fake_value)\n",
        "\n",
        "@time_it\n",
        "def unmask_dataframe(df):\n",
        "    for col, entity in entity_columns.items():\n",
        "        matching_keys = [key for key, value in entity_mapping.items() if value == entity]\n",
        "        if matching_keys:\n",
        "            category = matching_keys[0]\n",
        "            df[col] = df[col].astype(str).apply(lambda x: restore_original_value(category, str(x)) if x else str(x))\n",
        "\n",
        "    return df\n",
        "def compare_files(original_file, restored_file):\n",
        "    \"\"\"Check if the original and restored files are identical.\"\"\"\n",
        "    file_ext = os.path.splitext(original_file)[-1].lower()\n",
        "    original_df = pd.read_excel(original_file) if file_ext == \".xlsx\" else pd.read_csv(original_file)\n",
        "    restored_df = pd.read_excel(restored_file) if file_ext == \".xlsx\" else pd.read_csv(restored_file)\n",
        "\n",
        "    is_identical = original_df.equals(restored_df)\n",
        "    print(f\"📊 Are files identical? {'✅ Yes' if is_identical else '❌ No'}\")\n",
        "    if not is_identical:\n",
        "        print(\"⚠️ The restored file does not match the original. There may be an issue with the mapping.\")\n",
        "\n",
        "    return is_identical\n",
        "\n",
        "def de_anonymize_paragraph(text):\n",
        "  for category,mapping in reverse_mapping.items():\n",
        "    for fake_value,original_value in mapping.items():\n",
        "      if fake_value in text:\n",
        "        text=text.replace(fake_value,original_value)\n",
        "  return text\n",
        "\n",
        "def save_mapping(filename):\n",
        "    mapping_data={\n",
        "        \"filename\":filename,\n",
        "        \"updated_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
        "        \"forward_mapping\":forward_mapping,\n",
        "        \"reverse_mapping\":reverse_mapping\n",
        "    }\n",
        "    with open(mapping_file, \"w\") as f:\n",
        "        json.dump(mapping_data, f, indent=4)\n",
        "\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    input_file=\"smaller_100k_companies.csv\"\n",
        "    file_ext=os.path.splitext(input_file)[-1].lower()\n",
        "\n",
        "    df = pd.read_excel(input_file, dtype=str) if file_ext == \".xlsx\" else pd.read_csv(input_file, dtype=str, low_memory=False)\n",
        "    entity_columns=analyze_column(df)\n",
        "    print(entity_columns)\n",
        "\n",
        "    anonymized_df=mask_dataframe(df)\n",
        "    output_file=\"anonymized.xlsx\" if file_ext==\".xlsx\" else \"anonymized.csv\"\n",
        "    anonymized_df.to_excel(output_file,index=False) if file_ext==\".xlsx\" else anonymized_df.to_csv(output_file,index=False)\n",
        "    print(f\"✅ Anonymized data saved as {output_file}\")\n",
        "\n",
        "    save_mapping(input_file)\n",
        "    restored_df=unmask_dataframe(pd.read_excel(output_file) if file_ext==\".xlsx\" else pd.read_csv(output_file))\n",
        "    restored_file=\"restored.xlsx\" if file_ext==\".xlsx\" else \"restored.csv\"\n",
        "    restored_df.to_excel(restored_file,index=False) if file_ext==\".xlsx\" else restored_df.to_csv(restored_file,index=False)\n",
        "    print(f\"✅ Restored data saved as {restored_file}\")\n",
        "    compare_files(input_file, restored_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUrTvwkGF8AB",
        "outputId": "a93c6af4-ee2c-4e4d-d3be-3bd8560fb497"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⏳ Execution time analyze_column: 2.538862 seconds\n",
            "{'id': 'ID', 'name': 'ORG', 'domain': 'URL', 'size range': 'DATE_TIME', 'locality': 'LOCATION', 'country': 'COUNTRY', 'linkedin url': 'URL', 'current employee estimate': 'US_DRIVER_LICENSE', 'total employee estimate': 'US_DRIVER_LICENSE'}\n",
            "\n",
            "⏳ Execution time mask_dataframe: 0.433332 seconds\n",
            "✅ Anonymized data saved as anonymized.csv\n",
            "\n",
            "⏳ Execution time unmask_dataframe: 0.338441 seconds\n",
            "✅ Restored data saved as restored.csv\n",
            "📊 Are files identical? ✅ Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Main Code - Optimizing for excel\n",
        "import re\n",
        "import os\n",
        "import gzip\n",
        "import time\n",
        "import json\n",
        "import spacy\n",
        "import string\n",
        "import random\n",
        "import tempfile\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "from pathlib import Path\n",
        "from collections import defaultdict, deque\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "\n",
        "# Variable defined\n",
        "ID, fake_data, used_urls = {}, {}, set()\n",
        "\n",
        "url_extensions = [\n",
        "    \".com\", \".net\", \".org\", \".edu\", \".gov\", \".co\", \".us\", \".uk\", \".in\", \".ru\",\n",
        "    \".jp\", \".cn\", \".de\", \".fr\", \".it\", \".nl\", \".es\", \".br\", \".au\", \".ca\",\n",
        "    \".ch\", \".se\", \".no\", \".za\", \".mx\", \".ar\", \".be\", \".kr\", \".pl\", \".tr\",\n",
        "    \".ua\", \".ir\", \".sa\", \".ae\", \".my\", \".sg\", \".hk\", \".tw\", \".nz\", \".id\",\n",
        "    \".th\", \".ph\", \".vn\", \".bd\", \".lk\", \".np\", \".pk\", \".cz\", \".gr\", \".hu\",\n",
        "    \".fi\", \".dk\", \".il\", \".ie\", \".pt\", \".sk\", \".si\", \".ro\", \".bg\", \".rs\",\n",
        "    \".lt\", \".lv\", \".ee\", \".hr\", \".ba\", \".md\", \".ge\", \".kz\", \".by\", \".tm\",\n",
        "    \".uz\", \".af\", \".qa\", \".om\", \".kw\", \".bh\", \".ye\", \".jo\", \".lb\", \".sy\",\n",
        "    \".iq\", \".ps\", \".az\", \".am\", \".kg\", \".mn\", \".bt\", \".mv\", \".mm\", \".kh\",\n",
        "    \".la\", \".tl\", \".sb\", \".fj\", \".pg\", \".to\", \".tv\", \".ws\", \".fm\", \".ki\"\n",
        "]\n",
        "with gzip.open(\"faker_data_v2.json.gz\", \"rt\", encoding = 'utf-8') as f: fake_data_list = json.load(f)\n",
        "# Objects\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "fake=Faker()\n",
        "analyzer = AnalyzerEngine()\n",
        "\n",
        "for data in fake_data_list:\n",
        "    for key,value in data.items(): fake_data[key]=deque(value)\n",
        "domain_pool = list(fake_data.get('url', deque()))\n",
        "\n",
        "entity_mapping={\n",
        "    'names':'PERSON',\n",
        "    'emails':'EMAIL_ADDRESS',\n",
        "    'phone':'PHONE_NUMBER',\n",
        "    'location':'LOCATION',\n",
        "    'credit':'CREDIT_CARD',\n",
        "    'url':'URL',\n",
        "    'country':'COUNTRY',\n",
        "    'company':\"ORG\",\n",
        "    'id':'ID',\n",
        "}\n",
        "\n",
        "mapping_file=\"mapping.json\"\n",
        "forward_mapping=defaultdict(dict)\n",
        "reverse_mapping=defaultdict(dict)\n",
        "\n",
        "if os.path.exists(mapping_file):\n",
        "    with open(mapping_file, \"r\") as f:\n",
        "        mapping_data = json.load(f)\n",
        "        forward_mapping.update(mapping_data.get(\"forward_mapping\", {}))\n",
        "        reverse_mapping.update(mapping_data.get(\"reverse_mapping\", {}))\n",
        "def time_it(func):\n",
        "    \"\"\"Decorator to measure execution time of functions.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(f'\\n⏳ Execution time {func.__name__}: {end-start:.6f} seconds')\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "@time_it\n",
        "def analyze_column(df):\n",
        "    entity_columns = {}\n",
        "\n",
        "    # Step 1: Use Presidio to analyze all columns\n",
        "    for col in df.columns:\n",
        "        if 'id' in col.lower():\n",
        "            entity_columns[col] = 'ID'\n",
        "        elif 'country' in col.lower():\n",
        "            entity_columns[col] = 'COUNTRY'\n",
        "        else:\n",
        "            unique_values = df[col].dropna().astype(str).unique()[:25]\n",
        "            entity_counts = {}\n",
        "\n",
        "            for value in unique_values:\n",
        "                results = analyzer.analyze(text=value, language='en')\n",
        "                for result in results:\n",
        "                    entity_counts[result.entity_type] = entity_counts.get(result.entity_type, 0) + 1\n",
        "\n",
        "            if entity_counts:\n",
        "                predominant_entity = max(entity_counts, key=entity_counts.get)\n",
        "                entity_columns[col] = predominant_entity\n",
        "                if predominant_entity==\"LOCATION\":\n",
        "                  org_count=0\n",
        "                  for value in unique_values:\n",
        "                    doc=nlp(value)\n",
        "                    for ent in doc.ents:\n",
        "                      if ent.label_==\"ORG\":\n",
        "                        org_count+=1\n",
        "                  if org_count>5:\n",
        "                    predominant_entity=\"ORG\"\n",
        "                entity_columns[col]=predominant_entity\n",
        "\n",
        "    # Step 2: Use SpaCy to analyze non-numeric and unclassified columns\n",
        "    for col in df.select_dtypes(exclude=['number']).columns:\n",
        "        if col not in entity_columns:\n",
        "            unique_values = df[col].dropna().astype(str).unique()[:25]\n",
        "            org_count = 0\n",
        "\n",
        "            for value in unique_values:\n",
        "                doc = nlp(value)\n",
        "                for ent in doc.ents:\n",
        "                    if ent.label_ == 'ORG':\n",
        "                        org_count += 1\n",
        "\n",
        "            # If more than half of the sample values are ORG, classify as ORG\n",
        "            if org_count > 12:\n",
        "                entity_columns[col] = 'ORG'\n",
        "\n",
        "    return entity_columns\n",
        "\n",
        "\n",
        "def modify_fake_value(category, base_fake_value, counter):\n",
        "    if category == \"names\": return f\"{base_fake_value} {string.ascii_uppercase[counter % 26]}.\"\n",
        "    elif category == \"emails\":\n",
        "        name, domain = base_fake_value.split(\"@\")\n",
        "        return f\"{name}{counter}@{domain}\"\n",
        "    elif category in {\"location\", \"country\"}: return f\"{base_fake_value}, District {counter % 100_000_000 + 1}\"\n",
        "    elif category == \"url\":\n",
        "        # Check if the URL already exists, if so, append a country extension\n",
        "        fake_value = base_fake_value\n",
        "        while fake_value in used_urls:\n",
        "            ext = random.choice(url_extensions)\n",
        "            if not fake_value.endswith(ext):\n",
        "                fake_value += ext\n",
        "        used_urls.add(fake_value)\n",
        "        return fake_value\n",
        "    elif category == \"phone\": return f\"{base_fake_value[:-2]}{counter % 100:02d}\"\n",
        "    elif category == \"company\": return f\"{base_fake_value} Group {counter % 100_000_000 + 1}\"\n",
        "    elif category == \"credit\": return f\"{base_fake_value[:-4]}{counter % 10000:04d}\"\n",
        "    else: return f\"{base_fake_value}-{counter}\"\n",
        "\n",
        "\n",
        "def get_fake_value(category, original_value):\n",
        "    global ID\n",
        "    fake_value = None\n",
        "    original_value = original_value.strip()\n",
        "    if original_value in forward_mapping[category]: return forward_mapping[category][original_value]\n",
        "\n",
        "    # Special case for ID\n",
        "    if category == 'id':\n",
        "        length = 6\n",
        "        while True:\n",
        "            fake_value = fake.bothify(text=f'ID-{\"#\"*length}')\n",
        "            if fake_value not in ID:\n",
        "                ID[fake_value] = True\n",
        "                break\n",
        "            if len(ID) >= 10 ** length: length += 1\n",
        "    elif category == 'url':\n",
        "        domain1, domain2 = random.sample(domain_pool, 2)\n",
        "        base_fake_value = f\"https://{domain1.lower()}/{domain2.lower()}.co\"\n",
        "\n",
        "        if base_fake_value not in reverse_mapping[\"url\"]:\n",
        "            fake_value = base_fake_value\n",
        "        else:\n",
        "            counter = len(reverse_mapping[\"url\"])\n",
        "            fake_value = modify_fake_value(\"url\", base_fake_value, counter)\n",
        "\n",
        "    elif fake_data.get(category):\n",
        "      for _ in range(len(fake_data[category])):\n",
        "          candidate = fake_data[category].popleft()\n",
        "          fake_data[category].append(candidate)  # Reinsert at end (rotation)\n",
        "          if candidate not in reverse_mapping[category]:\n",
        "              fake_value = candidate\n",
        "              break\n",
        "      else:\n",
        "          counter = len(reverse_mapping[category])\n",
        "          base_fake_value = random.choice(list(reverse_mapping[category])) if reverse_mapping[category] else f\"{category}_\"\n",
        "          fake_value = modify_fake_value(category, base_fake_value, counter)\n",
        "\n",
        "    if not fake_value or fake_value in reverse_mapping[category]:\n",
        "        counter = len(reverse_mapping[category])\n",
        "        base = fake_value if fake_value else f\"{category}_\"\n",
        "        fake_value = modify_fake_value(category, base, counter)\n",
        "\n",
        "    # Store forward & reverse mappings\n",
        "    forward_mapping[category][original_value] = fake_value\n",
        "    reverse_mapping[category][fake_value] = original_value\n",
        "\n",
        "    return fake_value\n",
        "\n",
        "\n",
        "@time_it\n",
        "def mask_dataframe(df):\n",
        "    for col, entity in entity_columns.items():\n",
        "        matching_keys = [key for key, value in entity_mapping.items() if value == entity]\n",
        "        if matching_keys:\n",
        "            df[col] = df[col].astype(str).apply(lambda x: get_fake_value(matching_keys[0], str(x)) if x else str(x))\n",
        "    return df\n",
        "\n",
        "def restore_original_value(category, fake_value):\n",
        "    return reverse_mapping[category].get(fake_value, fake_value)\n",
        "\n",
        "@time_it\n",
        "def unmask_dataframe(df):\n",
        "    for col, entity in entity_columns.items():\n",
        "        matching_keys = [key for key, value in entity_mapping.items() if value == entity]\n",
        "        if matching_keys:\n",
        "            category = matching_keys[0]\n",
        "            df[col] = df[col].astype(str).apply(lambda x: restore_original_value(category, str(x)) if x else str(x))\n",
        "\n",
        "    return df\n",
        "def compare_files(original_file, restored_file):\n",
        "    \"\"\"Check if the original and restored files are identical.\"\"\"\n",
        "    file_ext = os.path.splitext(original_file)[-1].lower()\n",
        "    original_df = pd.read_excel(original_file) if file_ext == \".xlsx\" else pd.read_csv(original_file)\n",
        "    restored_df = pd.read_excel(restored_file) if file_ext == \".xlsx\" else pd.read_csv(restored_file)\n",
        "\n",
        "    is_identical = original_df.equals(restored_df)\n",
        "    print(f\"📊 Are files identical? {'✅ Yes' if is_identical else '❌ No'}\")\n",
        "    if not is_identical:\n",
        "        print(\"⚠️ The restored file does not match the original. There may be an issue with the mapping.\")\n",
        "\n",
        "    return is_identical\n",
        "\n",
        "def de_anonymize_paragraph(text):\n",
        "  for category,mapping in reverse_mapping.items():\n",
        "    for fake_value,original_value in mapping.items():\n",
        "      if fake_value in text:\n",
        "        text=text.replace(fake_value,original_value)\n",
        "  return text\n",
        "\n",
        "def save_mapping(filename):\n",
        "    mapping_data={\n",
        "        \"filename\":filename,\n",
        "        \"updated_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
        "        \"forward_mapping\":forward_mapping,\n",
        "        \"reverse_mapping\":reverse_mapping\n",
        "    }\n",
        "    with open(mapping_file, \"w\") as f:\n",
        "        json.dump(mapping_data, f, indent=4)\n",
        "\n",
        "def process_file(input_file):\n",
        "    file_ext = os.path.splitext(input_file)[-1].lower()\n",
        "\n",
        "    if file_ext == \".xlsx\":\n",
        "        # Create temp dir for CSVs\n",
        "        temp_dir = tempfile.mkdtemp(prefix=\"temp_csv_\")\n",
        "\n",
        "        # Load Excel\n",
        "        xl = pd.read_excel(input_file, sheet_name=None, dtype=str)\n",
        "        masked_sheets = {}\n",
        "        restored_sheets = {}\n",
        "\n",
        "        for sheet_name, df in xl.items():\n",
        "            print(f\"\\n🔍 Processing sheet: {sheet_name}\")\n",
        "\n",
        "            # Save sheet as temp CSV\n",
        "            csv_path = os.path.join(temp_dir, f\"{sheet_name}.csv\")\n",
        "            df.to_csv(csv_path, index=False)\n",
        "\n",
        "            # Process CSV\n",
        "            df_csv = pd.read_csv(csv_path, dtype=str)\n",
        "            entity_columns = analyze_column(df_csv)\n",
        "            print(f\"Detected entities: {entity_columns}\")\n",
        "\n",
        "            masked_df = mask_dataframe(df_csv.copy())\n",
        "            restored_df = unmask_dataframe(masked_df.copy())\n",
        "\n",
        "            masked_sheets[sheet_name] = masked_df\n",
        "            restored_sheets[sheet_name] = restored_df\n",
        "\n",
        "        # Write back to Excel\n",
        "        anonymized_file = \"anonymized.xlsx\"\n",
        "        with pd.ExcelWriter(anonymized_file) as writer:\n",
        "            for sheet, df in masked_sheets.items():\n",
        "                df.to_excel(writer, sheet_name=sheet, index=False)\n",
        "        print(f\"\\n✅ Anonymized Excel saved: {anonymized_file}\")\n",
        "\n",
        "        restored_file = \"restored.xlsx\"\n",
        "        with pd.ExcelWriter(restored_file) as writer:\n",
        "            for sheet, df in restored_sheets.items():\n",
        "                df.to_excel(writer, sheet_name=sheet, index=False)\n",
        "        print(f\"✅ Restored Excel saved: {restored_file}\")\n",
        "\n",
        "        compare_files(input_file, restored_file)\n",
        "\n",
        "        # Clean up\n",
        "        for f in Path(temp_dir).glob(\"*.csv\"):\n",
        "            os.remove(f)\n",
        "        os.rmdir(temp_dir)\n",
        "\n",
        "    elif file_ext == \".csv\":\n",
        "        df = pd.read_csv(input_file, dtype=str, low_memory=False)\n",
        "        print(f\"\\n🔍 Processing CSV: {input_file}\")\n",
        "\n",
        "        entity_columns = analyze_column(df)\n",
        "        print(f\"Detected entities: {entity_columns}\")\n",
        "\n",
        "        masked_df = mask_dataframe(df.copy())\n",
        "        masked_df.to_csv(\"anonymized.csv\", index=False)\n",
        "        print(\"✅ Anonymized CSV saved: anonymized.csv\")\n",
        "\n",
        "        save_mapping(input_file)\n",
        "\n",
        "        restored_df = unmask_dataframe(masked_df.copy())\n",
        "        restored_df.to_csv(\"restored.csv\", index=False)\n",
        "        print(\"✅ Restored CSV saved: restored.csv\")\n",
        "\n",
        "        compare_files(input_file, \"restored.csv\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported file type. Use .csv or .xlsx only.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"smaller_100k_companies.xlsx\"\n",
        "    process_file(input_file)\n"
      ],
      "metadata": {
        "id": "yKsxxp6Q7lJh",
        "outputId": "f4faa96d-9e3e-4726-fd8d-6403fc5450d9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔍 Processing sheet: Sheet1\n",
            "\n",
            "⏳ Execution time analyze_column: 2.405910 seconds\n",
            "Detected entities: {'id': 'ID', 'name': 'ORG', 'domain': 'URL', 'year founded': 'DATE_TIME', 'size range': 'DATE_TIME', 'locality': 'LOCATION', 'country': 'COUNTRY', 'linkedin url': 'URL', 'current employee estimate': 'US_DRIVER_LICENSE', 'total employee estimate': 'US_DRIVER_LICENSE'}\n",
            "\n",
            "⏳ Execution time mask_dataframe: 3.278816 seconds\n",
            "\n",
            "⏳ Execution time unmask_dataframe: 0.328220 seconds\n",
            "\n",
            "✅ Anonymized Excel saved: anonymized.xlsx\n",
            "✅ Restored Excel saved: restored.xlsx\n",
            "📊 Are files identical? ✅ Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import psutil\n",
        "\n",
        "process = psutil.Process(os.getpid())\n",
        "mem_info = process.memory_info()\n",
        "\n",
        "print(\"Your script is using:\", round(mem_info.rss / 1e6, 2), \"MB RAM\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnPSjUplFl0U",
        "outputId": "d9ff7efd-9932-46bb-f14b-23f812871815"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your script is using: 5919.22 MB RAM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Seeing the differences b/w codes\n",
        "import pandas as pd\n",
        "\n",
        "original_df = pd.read_csv('test.csv')\n",
        "restored_df = pd.read_csv('restored.csv')\n",
        "\n",
        "diff_df = original_df.compare(restored_df, keep_equal=False)\n",
        "print(diff_df.head(10))\n",
        "\n",
        "diff_df.to_csv('differences.csv')\n"
      ],
      "metadata": {
        "id": "VSqjIhh14nsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing different mapping technique\n",
        "# Testing - classify orgainization\n",
        "# Problematic -> can't use to demask paragraph\n",
        "import re\n",
        "import os\n",
        "import gzip\n",
        "import time\n",
        "import json\n",
        "import spacy\n",
        "import string\n",
        "import random\n",
        "import pandas as pd\n",
        "from faker import Faker\n",
        "from collections import defaultdict, deque\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "\n",
        "\n",
        "ID = {}\n",
        "fake_data={}\n",
        "nlp = spacy.load(\"en_core_web_lg\")\n",
        "fake=Faker()\n",
        "analyzer = AnalyzerEngine()\n",
        "\n",
        "with gzip.open(\"faker_data.json.gz\", \"rt\", encoding='utf-8') as f:\n",
        "    fake_data_list = json.load(f)\n",
        "for data in fake_data_list:\n",
        "    for key,value in data.items():\n",
        "        fake_data[key]=deque(value)\n",
        "\n",
        "entity_mapping={\n",
        "    'names':'PERSON',\n",
        "    'emails':'EMAIL_ADDRESS',\n",
        "    'phone':'PHONE_NUMBER',\n",
        "    'location':'LOCATION',\n",
        "    'credit':'CREDIT_CARD',\n",
        "    'url':'URL',\n",
        "    'country':'COUNTRY',\n",
        "    'company':\"ORG\",\n",
        "    'id':'ID',\n",
        "}\n",
        "\n",
        "mapping_file=\"mapping.json\"\n",
        "forward_mapping=defaultdict(dict)\n",
        "reverse_mapping=defaultdict(dict)\n",
        "\n",
        "if os.path.exists(mapping_file):\n",
        "    with open(mapping_file, \"r\") as f:\n",
        "        mapping_data = json.load(f)\n",
        "        forward_mapping.update(mapping_data.get(\"forward_mapping\", {}))\n",
        "        reverse_mapping.update(mapping_data.get(\"reverse_mapping\", {}))\n",
        "def time_it(func):\n",
        "    \"\"\"Decorator to measure execution time of functions.\"\"\"\n",
        "    def wrapper(*args, **kwargs):\n",
        "        start = time.time()\n",
        "        result = func(*args, **kwargs)\n",
        "        end = time.time()\n",
        "        print(f'\\n⏳ Execution time {func.__name__}: {end-start:.6f} seconds')\n",
        "        return result\n",
        "    return wrapper\n",
        "\n",
        "@time_it\n",
        "def analyze_column(df):\n",
        "    entity_columns = {}\n",
        "\n",
        "    # Step 1: Use Presidio to analyze all columns\n",
        "    for col in df.columns:\n",
        "        if 'id' in col.lower():\n",
        "            entity_columns[col] = 'ID'\n",
        "        elif 'country' in col.lower():\n",
        "            entity_columns[col] = 'COUNTRY'\n",
        "        else:\n",
        "            unique_values = df[col].dropna().astype(str).unique()[:25]\n",
        "            entity_counts = {}\n",
        "\n",
        "            for value in unique_values:\n",
        "                results = analyzer.analyze(text=value, language='en')\n",
        "                for result in results:\n",
        "                    entity_counts[result.entity_type] = entity_counts.get(result.entity_type, 0) + 1\n",
        "\n",
        "            if entity_counts:\n",
        "                predominant_entity = max(entity_counts, key=entity_counts.get)\n",
        "                entity_columns[col] = predominant_entity\n",
        "                if predominant_entity==\"LOCATION\":\n",
        "                  org_count=0\n",
        "                  for value in unique_values:\n",
        "                    doc=nlp(value)\n",
        "                    for ent in doc.ents:\n",
        "                      if ent.label_==\"ORG\":\n",
        "                        org_count+=1\n",
        "                  if org_count>5:\n",
        "                    predominant_entity=\"ORG\"\n",
        "                entity_columns[col]=predominant_entity\n",
        "\n",
        "    # Step 2: Use SpaCy to analyze non-numeric and unclassified columns\n",
        "    for col in df.select_dtypes(exclude=['number']).columns:\n",
        "        if col not in entity_columns:\n",
        "            unique_values = df[col].dropna().astype(str).unique()[:25]\n",
        "            org_count = 0\n",
        "\n",
        "            for value in unique_values:\n",
        "                doc = nlp(value)\n",
        "                for ent in doc.ents:\n",
        "                    if ent.label_ == 'ORG':\n",
        "                        org_count += 1\n",
        "\n",
        "            # If more than half of the sample values are ORG, classify as ORG\n",
        "            if org_count > 12:\n",
        "                entity_columns[col] = 'ORG'\n",
        "\n",
        "    return entity_columns\n",
        "\n",
        "\n",
        "def modify_fake_value(category, base_fake_value, counter):\n",
        "    if category == \"names\": return f\"{base_fake_value} {string.ascii_uppercase[counter % 26]}.\"\n",
        "    elif category == \"emails\":\n",
        "        name, domain = base_fake_value.split(\"@\")\n",
        "        return f\"{name}{counter}@{domain}\"\n",
        "    elif category == \"location\": return f\"{base_fake_value}, District {counter % 100_000_000 + 1}\"\n",
        "    elif category == \"url\": return base_fake_value.replace(\"://\", f\"://sub{counter}.\", 1)\n",
        "    elif category == \"phone\": return f\"{base_fake_value[:-2]}{counter % 100:02d}\"\n",
        "    elif category == \"company\": return f\"{base_fake_value} Group {counter % 50}\"\n",
        "    elif category == \"credit\": return f\"{base_fake_value[:-4]}{counter % 10000:04d}\"\n",
        "    else: return f\"{base_fake_value}-{counter}\"\n",
        "\n",
        "\n",
        "def get_fake_value(category, original_value, colname):\n",
        "    global ID\n",
        "    # Added\n",
        "    if colname not in forward_mapping[category]: forward_mapping[category][colname] = {}\n",
        "    if colname not in reverse_mapping[category]: reverse_mapping[category][colname] = {}\n",
        "    if colname in forward_mapping[category] and original_value in forward_mapping[category][colname]:\n",
        "      return forward_mapping[category][colname][original_value]\n",
        "\n",
        "    if category == 'id':\n",
        "      length = 6\n",
        "      while True:\n",
        "          fake_value = fake.bothify(text=f'ID-{\"#\"*length}')\n",
        "          if fake_value not in ID:\n",
        "              ID[fake_value] = True\n",
        "              break\n",
        "          # If exhausted all possibilities for the current length, increase the length\n",
        "          if len(ID) >= 10 ** length:\n",
        "              length += 1\n",
        "    elif fake_data.get(category):\n",
        "        fake_value = fake_data[category].pop() if fake_data[category] else None\n",
        "    else:\n",
        "        base_fake=random.choice(list(forward_mapping[category][colname].values()))\n",
        "        if base_fake:\n",
        "          counter=len(forward_mapping[category][colname])\n",
        "          fake_value=modify_fake_value(category,base_fake,counter)\n",
        "\n",
        "    forward_mapping[category][colname][original_value] = fake_value\n",
        "    reverse_mapping[category][colname][fake_value] = original_value\n",
        "\n",
        "\n",
        "    return fake_value\n",
        "\n",
        "@time_it\n",
        "def mask_dataframe(df):\n",
        "    for col, entity in entity_columns.items():\n",
        "        matching_keys = [key for key, value in entity_mapping.items() if value == entity]\n",
        "        if matching_keys:\n",
        "            df[col] = df[col].astype(str).apply(lambda x: get_fake_value(matching_keys[0], str(x), col) if x else str(x))\n",
        "    return df\n",
        "\n",
        "def restore_original_value(category, fake_value, colname):\n",
        "    if colname in reverse_mapping[category]:\n",
        "      return reverse_mapping[category][colname].get(fake_value, fake_value)\n",
        "    return fake_value\n",
        "\n",
        "@time_it\n",
        "def unmask_dataframe(df):\n",
        "    for col, entity in entity_columns.items():\n",
        "        matching_keys = [key for key, value in entity_mapping.items() if value == entity]\n",
        "\n",
        "        if matching_keys:\n",
        "            category = matching_keys[0]\n",
        "            df[col] = df[col].astype(str).apply(lambda x: restore_original_value(category, str(x), col) if x else str(x))\n",
        "\n",
        "    return df\n",
        "def compare_files(original_file, restored_file):\n",
        "    \"\"\"Check if the original and restored files are identical.\"\"\"\n",
        "    file_ext = os.path.splitext(original_file)[-1].lower()\n",
        "    original_df = pd.read_excel(original_file) if file_ext == \".xlsx\" else pd.read_csv(original_file)\n",
        "    restored_df = pd.read_excel(restored_file) if file_ext == \".xlsx\" else pd.read_csv(restored_file)\n",
        "\n",
        "    is_identical = original_df.equals(restored_df)\n",
        "    print(f\"📊 Are files identical? {'✅ Yes' if is_identical else '❌ No'}\")\n",
        "    if not is_identical:\n",
        "        print(\"⚠️ The restored file does not match the original. There may be an issue with the mapping.\")\n",
        "\n",
        "    return is_identical\n",
        "\n",
        "def de_anonymize_paragraph(text):\n",
        "  for category, col_map in reverse_mapping.items():\n",
        "    for colname, mapping in col_map.items():\n",
        "        for fake_value, original_value in mapping.items():\n",
        "            if fake_value in text:\n",
        "                text = text.replace(fake_value, original_value)\n",
        "  return text\n",
        "\n",
        "def save_mapping(filename):\n",
        "    mapping_data={\n",
        "        \"filename\":filename,\n",
        "        \"updated_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime()),\n",
        "        \"forward_mapping\":dict(forward_mapping),\n",
        "        \"reverse_mapping\":dict(reverse_mapping)\n",
        "    }\n",
        "    with open(mapping_file, \"w\") as f:\n",
        "        json.dump(mapping_data, f, indent=4)\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    input_file=\"smaller_100k_companies.csv\"\n",
        "    file_ext=os.path.splitext(input_file)[-1].lower()\n",
        "\n",
        "    df = pd.read_excel(input_file, dtype=str) if file_ext == \".xlsx\" else pd.read_csv(input_file, dtype=str, low_memory=False)\n",
        "    entity_columns=analyze_column(df)\n",
        "    print(entity_columns)\n",
        "\n",
        "    anonymized_df=mask_dataframe(df)\n",
        "    output_file=\"anonymized.xlsx\" if file_ext==\".xlsx\" else \"anonymized.csv\"\n",
        "    anonymized_df.to_excel(output_file,index=False) if file_ext==\".xlsx\" else anonymized_df.to_csv(output_file,index=False)\n",
        "    print(f\"✅ Anonymized data saved as {output_file}\")\n",
        "\n",
        "    save_mapping(input_file)\n",
        "    restored_df=unmask_dataframe(pd.read_excel(output_file) if file_ext==\".xlsx\" else pd.read_csv(output_file))\n",
        "    restored_file=\"restored.xlsx\" if file_ext==\".xlsx\" else \"restored.csv\"\n",
        "    restored_df.to_excel(restored_file,index=False) if file_ext==\".xlsx\" else restored_df.to_csv(restored_file,index=False)\n",
        "    print(f\"✅ Restored data saved as {restored_file}\")\n",
        "    compare_files(input_file, restored_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iLSBTwDt3ndb",
        "outputId": "747f1373-034f-4ab6-ed00-3edd20f8f0ff"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⏳ Execution time analyze_column: 3.097933 seconds\n",
            "{'id': 'ID', 'name': 'ORG', 'domain': 'URL', 'size range': 'DATE_TIME', 'locality': 'LOCATION', 'country': 'COUNTRY', 'linkedin url': 'URL', 'current employee estimate': 'US_DRIVER_LICENSE', 'total employee estimate': 'US_DRIVER_LICENSE'}\n",
            "\n",
            "⏳ Execution time mask_dataframe: 2.705842 seconds\n",
            "✅ Anonymized data saved as anonymized.csv\n",
            "\n",
            "⏳ Execution time unmask_dataframe: 0.398932 seconds\n",
            "✅ Restored data saved as restored.csv\n",
            "📊 Are files identical? ✅ Yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dummy\n",
        "json_data = {\n",
        "    \"name\": \"James Bat Bond\",\n",
        "    \"email\":[ \"jamesbatbond@ust.com\", 'kirant700@gmail.com', '12312@ust.com'],\n",
        "    \"backup_email\": \"b20cs110@mace.ac.in\",\n",
        "    \"phone\": \"+973 30982167\",\n",
        "    \"location\": \"Wakanda, Africa\",\n",
        "    \"description\": \"Abram is a software engineer from Wakanda.\",\n",
        "    \"metadata\": {\n",
        "        \"emergency_contact\": \"+91 9678785654\",\n",
        "        \"company\": \"UST Global\",\n",
        "        \"address\": {\n",
        "            \"city\": \"New York\",\n",
        "            \"country\": \"USA\"\n",
        "        }\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "id": "eTCcJL5ba6oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Uploading json file and doing NER on attributes\n",
        "import json\n",
        "import time\n",
        "from google.colab import files\n",
        "from presidio_analyzer import AnalyzerEngine\n",
        "from presidio_anonymizer import AnonymizerEngine\n",
        "\n",
        "analyzer = AnalyzerEngine()\n",
        "anonymizer = AnonymizerEngine()\n",
        "\n",
        "def time_it(func):\n",
        "  def wrapper(*args, **kwargs):\n",
        "    start = time.time()\n",
        "    result = func(*args, **kwargs)\n",
        "    end = time.time()\n",
        "    print(f'\\n⏳ Execution time {func.__name__}: {end-start:.6f} seconds')\n",
        "    return result\n",
        "  return wrapper\n",
        "\n",
        "# Processing JSON Spacey\n",
        "def processing_json(data):\n",
        "  if isinstance(data, dict):\n",
        "    return {key: processing_json(value) for key, value in data.items()}\n",
        "  elif isinstance(data, list):\n",
        "    return [processing_json(item) for item in data]\n",
        "  elif isinstance(data, str):\n",
        "    result = analyzer.analyze(\n",
        "        text = data,\n",
        "        entities = ['PHONE_NUMBER', 'ADDRESS', 'LOCATION', 'PERSON', 'EMAIL_ADDRESS', 'STREET_ADDRESS', 'CREDIT_CARD'],\n",
        "        language = 'en'\n",
        "    )\n",
        "    return anonymizer.anonymize(text=data, analyzer_results=result).text if result else data\n",
        "  else: return data\n",
        "\n",
        "@time_it\n",
        "def json_format_anonymized(json_data):\n",
        "  processed_data = processing_json(json_data)\n",
        "  print(json.dumps(processed_data, indent = 4))\n",
        "\n",
        "def json_file_anonymized():\n",
        "  file_upload = files.upload()\n",
        "  for filename in file_upload.keys():\n",
        "    if filename.endswith('.json'):\n",
        "      with open(filename, 'r') as fp:\n",
        "        json_data = json.load(fp)\n",
        "        processing_json(json_data)\n",
        "\n",
        "json_format_anonymized(json_data)"
      ],
      "metadata": {
        "id": "zBbEaJWOan9E",
        "outputId": "3c9c032d-86a6-4ec1-e461-c0b289d8a663",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - CreditCardRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNifRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - EsNieRecognizer supported languages: es, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItDriverLicenseRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItFiscalCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItVatCodeRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItIdentityCardRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - ItPassportRecognizer supported languages: it, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Recognizer not added to registry because language is not supported by registry - PlPeselRecognizer supported languages: pl, registry supported languages: en\n",
            "WARNING:presidio-analyzer:Entity ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity STREET_ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity STREET_ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity STREET_ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity STREET_ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity STREET_ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity STREET_ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity STREET_ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity STREET_ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity STREET_ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity STREET_ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity STREET_ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity ADDRESS doesn't have the corresponding recognizer in language : en\n",
            "WARNING:presidio-analyzer:Entity STREET_ADDRESS doesn't have the corresponding recognizer in language : en\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"name\": \"<PERSON>\",\n",
            "    \"email\": [\n",
            "        \"<EMAIL_ADDRESS>\",\n",
            "        \"<EMAIL_ADDRESS>\",\n",
            "        \"<EMAIL_ADDRESS>\"\n",
            "    ],\n",
            "    \"backup_email\": \"<EMAIL_ADDRESS>\",\n",
            "    \"phone\": \"<PHONE_NUMBER>\",\n",
            "    \"location\": \"<LOCATION>, <LOCATION>\",\n",
            "    \"description\": \"<PERSON> is a software engineer from <LOCATION>.\",\n",
            "    \"metadata\": {\n",
            "        \"emergency_contact\": \"<PHONE_NUMBER>\",\n",
            "        \"company\": \"UST Global\",\n",
            "        \"address\": {\n",
            "            \"city\": \"<LOCATION>\",\n",
            "            \"country\": \"<LOCATION>\"\n",
            "        }\n",
            "    }\n",
            "}\n",
            "\n",
            "⏳ Execution time json_format_anonymized: 0.152774 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Generating fake names and emails and comparing them\"\"\"\n",
        "from itertools import count\n",
        "import json\n",
        "import faker\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "fake = faker.Faker()\n",
        "\n",
        "def generate_fake_names():\n",
        "    unique_names = set()\n",
        "\n",
        "    for _ in range(500_000):\n",
        "        fake_name = fake.name()\n",
        "        if fake_name in unique_names: fake_name += f' {fake.name()}'\n",
        "        unique_names.add(fake_name)\n",
        "\n",
        "    with open('fake_names.json', 'w') as fp:\n",
        "        json.dump(\n",
        "            {\n",
        "                'names': list(unique_names)\n",
        "            },\n",
        "            fp,\n",
        "            indent=4\n",
        "        )\n",
        "    print('📈Done generating fake names!')\n",
        "\n",
        "def generate_fake_email():\n",
        "    unique_emails = set()\n",
        "    mail_endings = [\n",
        "                    'gmail.com', 'hotmail.com',\n",
        "                    'yahoo.com', 'outlook.com',\n",
        "                    'live.com', 'yandex.com',\n",
        "                    'mail.com', 'aol.com', 'edu.com'\n",
        "                ]\n",
        "    for _ in range(500_000):\n",
        "        fake_email = fake.email()\n",
        "        while fake_email in unique_emails:\n",
        "            body, domain = fake_email.split('@')\n",
        "            fake_email = f'{body}.{fake.name().replace(\" \", \".\")}.{fake.bothify(text=\"#####\")}@{random.choice(mail_endings)}'\n",
        "        unique_emails.add(fake_email)\n",
        "\n",
        "    with open('fake_emails.json', 'w') as fp:\n",
        "        json.dump(\n",
        "            {\n",
        "                'emails': list(unique_emails),\n",
        "            },\n",
        "            fp,\n",
        "            indent=4\n",
        "        )\n",
        "\n",
        "def find_repeating_values():\n",
        "    file_name = 'fake_emails.json'\n",
        "    with open(file_name, 'r') as fp:\n",
        "        data = json.load(fp)\n",
        "\n",
        "    values = data.get('emails', []) or data.get('names', [])\n",
        "\n",
        "    counts = Counter(values)\n",
        "    repeating_values = {item:count for item, count in counts.items() if count > 1}\n",
        "\n",
        "    print(f'Repeating values:{len(repeating_values)}')\n",
        "\n",
        "# find_repeating_values()\n",
        "# generate_fake_email()\n",
        "# generate_fake_names()"
      ],
      "metadata": {
        "id": "cQL6pAoW6SlT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}